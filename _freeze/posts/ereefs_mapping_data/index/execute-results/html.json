{
  "hash": "0bd2ca3ac3bf45ee600dd5c3c24f66cd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Creating Beautiful Maps Using eReefs Data\"\ndate: \"07-24-2025\"\nabstract-title: \"ABSTRACT\"\nabstract: \"In this blog I demonstrate how you can make beautiful maps in R using example data extracted from the eReefs platform. You can also follow along with any of your own data.\"\nimage: \"image.png\"\nformat: html\ntitle-block-banner: true #This is our banner\ninclude-after-body: \"../../html/html_footer.html\" #This is our footer\n---\n\n\n::: {.cell}\n\n:::\n\n\n# Introduction\n\nThis is part three of a series of blog that focus on eReefs and the data it provides. To follow along with this blog you will need to be able to download data from eReefs, you can learn how to do that in my first blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd). I would also recommend that you check my blog about [Plotting eReefs Data](../ereefs_plotting_data/index.qmd), however it is not essential reading for this post.\n\nIn this post I would like to explore the spatial aspect of eReefs data. Together we will learn some of the most important steps when working with this type of data as we:\n\n 1. Manipulate and transform spatial data,\n 2. Understand how to visualise the data, and\n 3. Build a series of informative maps\n\n# Read in Data\n\n\n::: {.cell}\n\n:::\n\n\nThanks to our previous post about extracting the data, it is saved in our file system ready to open no stress, and we know exactly the preprocessing that we need to do:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#load in the example data\nexample_data <- read_stars(\"example_data.nc\")\n\n#load vector of time values\ntime_vals <- readRDS(\"time_vector.rds\")\n\n#merge \"attributes\" (time) back together\nexample_data <- merge(example_data)\n\n#update time dimension values and names\nexample_data <- example_data |> \n  st_set_dimensions(3, time_vals,\n                    names = c(\"x\", \"y\", \"time\"))\n  \n#then update the attribute name\nexample_data <- setNames(example_data, \"Chla\")\n```\n:::\n\n\n# Analyse Data\n\nThere are few goals I have for analysing the data, I would like to be able to:\n\n 1. Manipulate data using the time dimension, i.e. extract certain dates\n 2. Manipulate data using spatial dimensions, i.e. extract certain areas\n 3. Aggregate data using the time dimension, e.g. getting monthly average values\n 4. Aggregate data using spatial dimensions, .e.g. getting average values per area\n \n## Layer Manipulation\n\nBefore we can get right into analysis or mapping we need to get a better understanding of the data structure and what we are looking at. Simply by calling the object we can already get a pretty good breakdown:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get a summary of the data\nexample_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nChla  0.04715918 0.2284472 0.3236666 0.3413846 0.4579611 0.8344838 57762\ndimension(s):\n     from  to                  offset  delta                refsys x/y\nx       1 161                  408823   1426 GDA2020 / MGA zone 55 [x]\ny       1 179                 8051625  -1288 GDA2020 / MGA zone 55 [y]\ntime    1 365 2020-07-01 02:00:00 UTC 1 days               POSIXct    \n```\n\n\n:::\n:::\n\n\nA couple of things to point out here.\n\n 1. The object is a \"stars object\", this is a class introduced by the `stars` package\n    1.1 stars objects hold attribute(s) and dimensions\n        1.1.1 attributes are our values (e.g. chlorophyll a), and there can be more than one\n        1.1.2 dimensions are our lat, long, depth, time, spectral band, etc.\n    1.2 This means stars objects can be n-dimensional **and** hold multiple attributes - which is a lot to think about\n 2. We can see the summary statistics for our attribute (chlorophyll a)\n 3. We can also see some information about our dimensions\n \nTo see the names of our dimensions we can use `dim()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#view the dimension names and lengths\ndim(example_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x    y time \n 161  179  365 \n```\n\n\n:::\n:::\n\n\nand to see the names of our attributes we can use `names()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#view the names of the attributes\nnames(example_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Chla\"\n```\n\n\n:::\n:::\n\n\nWhen we look to analyse our data we are going to have to think about all of our dimensions and any attributes. The simplest way to interact with each of these is using the `[]` square brackets. The first element in the brackets corresponds to the attributes, and each element following this is one of the dimensions (in the order in which you see them using `dim()`): `stars_object[att, i, j, k, time]`.\n\n - If we wanted to get just the first time step, we would write `stars_object[], , , , 1]` where the blank entry just means give me all of it.\n - If we wanted the 2nd i, the 1st-5th j, and the 3rd time step, we would write `stars_object[, 2, 1:5, ,3]`\n - If we have more than one attribute we can call that by name in the first argument `stars_object[att,,,,]`\n \nIn this way we have a cursory method of manipulating the data, and can use this to squeeze out our first map:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make a simple palette using our website colours\nmy_pal <- c(\"#A7C3C7\", \"#7EA0A7\", \"#55807D\", \"#2D6056\", \"#00402F\", \"#00252A\")\n\n#create a simple map of the data\ntm_shape(example_data[\"Chla\",,,1]) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nNot bad, the shape might seem a bit odd but this is a result of the boundaries we originally used to extract the data. (Check the [eReefs Extraction](../ereefs_extracting_data/index.qmd) blog if you are curious). And just to be clear - if we don't select just 1 time step we would get several maps:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple map of the data\ntm_shape(example_data[\"Chla\",,,1:4]) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nHowever trying to do all of our analysis and manipulation this way would be very painful. Thankfully stars objects work with most tidyverse functions.\n\nWhen we use the tidyverse method, knowing the exact names of our dimensions and attributes is the key for layer manipulation rather than their specific order. For example, if we wanted to once again extract one time layer of data, it is as easy as specifying the dimension we want to slice (\"time\"), and the slice number:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#slice to get a single time step\nsingle_timestep <- example_data |> \n  slice(time, 1)\n\n#slice to get multiple time steps\nmulti_timestep <- example_data |> \n  slice(time, 1:10)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(single_timestep) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nAlthough one downside here is that if you want to slice on multiple dimensions the calls must be run separately:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#slice by latitude and time, not the \nslice_of_lat_and_time <- example_data |> \n  slice(x, 1:30) |> \n  slice(time, 1)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#visualise the slice of lat and time \ntm_shape(slice_of_lat_and_time) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nAs we can seem using `slice()` just about covers all of our layer manipulation needs without much work... Layer aggregation is not going to be so easy...\n\n:::{.callout-note}\nThere are a few other functions from the tidyverse that can be used such as `filter()`, `pull()`, `mutate()`, and `select()`, but we won't worry about those here, we will just focus on `slice()`.\n:::\n\n## Layer Aggregation\n\nAs we explored above we have quite a few time steps, too many to plot all of them. Our initial solution to be able to create a map was to simply slice out a few layers, but obviously this is not a good solution if we are trying to learn something about the entire dataset. Instead, a common method to deal with this kind of problem (too much data) is to aggregate the data into a workable size. \n\nThere are two main ways to do this, the first method is to use `st_apply()` - this method is more general purpose and gives you greater control, it can apply all sorts of function and is not just limited to reducing dimensions. The second method is to use `aggregate()` - this method is easier to use, but has limits on what it can achieved. We will cover both as the more complicated method gives a very helpful conceptual grasp of the data.\n\n### st_apply()\n\nThe `st_apply()` function has three main arguments we are going to focus on:\n\n 1. X (the stars object),\n 2. MARGIN, and\n 3. FUN (the function to apply)\n \nArguments 1 and three are pretty self explanatory, but MARGIN is a bit more confusing so I have drawn up some diagrams to help the explanation. Lets first look at a conceptual diagram of our data.\n\n![](image_1.png)\n\nIn this diagram we can see each of our dimensions represented (latitude, longitude, depth, and time), and our attribute would be the value in the cell. Also note that for this diagram we have included multiple depth layers, but our actual data only has the one depth at the moment.\n\nWhat MARGIN does, is ask \"where do you want to apply the function?\" As in what dimension. The dimension that you supply is the dimension that is preserved. For our data there are four margins to choose from:\n\n 1 = Latitude\n 2 = Longitude\n 3 = Depth\n 4 = Time\n \nIf we say MARGIN = 1, we are applying our function over latitude, and the resulting data will **only retain the latitude dimension**. It would look like this:\n\n![](image_2.png)\nSee how all of the cells that share the same latitude, but have different longitudes, times, or depths, are all combined into the same group.\n\nIf we say MARGIN = 2, we are applying our function over longitude, and the resulting data will **only retain the longitude dimension**. It would look like this:\n\n![](image_3.png)\n\nThis time note that all cells that share the same longitude, but have different latitudes times, or depths, are all combined into the same group.\n\nMARGIN = 3 (depth) would look like this:\n\n![](image_4.png)\n\nand MARGIN = 4 (time) like this:\n\n![](image_5.png)\n\nReasonably straight forward so far, but also largely unhelpful - none of these outputs retain data that is viable to be mapped. This is where things get a bit more intense, because you can actually supply multiple dimensions to the MARGIN argument, which allows for the preservation of multiple dimensions. For example, if we wanted to learn how our attribute changed as it moved offshore and how it changed over time we could say MARGIN = c(2,4) which would look like this:\n\n![](image_6.png)\n\nSee how both the time and the longitude dimensions are maintained, and only the latitude and depth dimensions are grouped up.\n\nBut probably the one we are most interested in is if we set MARGIN = c(1,2) (Latitude and Longitude) which would collapse the time and depth variables leaving us with one raster:\n\n![](image_7.png)\n\nNote this time the depth and time dimensions are aggregated.\n\nOne final thing to note with the MARGIN argument is that while it can take numeric inputs, it can also take the names of the dimensions. So instead of saying `MARGIN = c(1,2)` we could instead say `MARGIN = c(\"x\",\"y\")` to be a bit more clear about what we are doing.\n\nIn fact, this is what we will do right now. Note the dimensions of our dataset before the function is run:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#look at dimensions\nexample_data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nChla  0.04715918 0.2284472 0.3236666 0.3413846 0.4579611 0.8344838 57762\ndimension(s):\n     from  to                  offset  delta                refsys x/y\nx       1 161                  408823   1426 GDA2020 / MGA zone 55 [x]\ny       1 179                 8051625  -1288 GDA2020 / MGA zone 55 [y]\ntime    1 365 2020-07-01 02:00:00 UTC 1 days               POSIXct    \n```\n\n\n:::\n:::\n\n\nversus after:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#take the mean over time\nchl_a_mean <- st_apply(example_data, \n                       c(\"x\",\"y\"),\n                       mean)\n\n#look at dimensions\nchl_a_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n           Min.    1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nmean  0.0678172 0.09142314 0.1768633 0.1596316 0.1922283 0.3919046 17086\ndimension(s):\n  from  to  offset delta                refsys x/y\nx    1 161  408823  1426 GDA2020 / MGA zone 55 [x]\ny    1 179 8051625 -1288 GDA2020 / MGA zone 55 [y]\n```\n\n\n:::\n:::\n\n\nAs we explained above, only the latitude and longitude dimensions remain. What we did was apply the mean function to the data, where the data is grouped by latitude and longitude (collapsing depth and time) to form pools of data to get the mean from. There is then one mean value for each latitude * longitude pair and we are left with a map that looks like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(chl_a_mean) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nCongratulations, using this method we now have a way of aggregating our data - i.e. by getting the mean of all the data into a single spatial layer. But more importantly we now have a very good conceptual understanding of our data, and we also know how we would apply some really complicated functions across different dimensions. This is extremely useful when you move on to more in depth spatial analysis.\n\nUnfortunately the single layer we aggregated to above doesn't cut it. It returns an annual overview that doesn't really tell us too much other than what locations have consistently higher chlorophyll a. No, instead we want to learn something about seasonal or monthly trends. To do this we need to provide some kind of indication to `st_apply()` that we want multiple groups.\n\nThis is achieved using the following steps:\n\n 1. Extract a table that contains the date and time of each layer\n 2. Group the individual layers by month and find the first and last layer per month:\n \n\n::: {.cell}\n\n```{.r .cell-code}\n#extract a table that contains the date and time of each layer\ntime_table <- data.frame(DateTime = st_get_dimension_values(example_data, \"time\"))\n\n#extract the year and month into their own columns, add a column that counts the row number\ntime_table <- time_table |> \n  mutate(Year = year(DateTime),\n         Month = month(DateTime),\n         RowId = row_number()) \n\n#combine the year and month columns\ntime_table <- time_table |> \n  unite(YearMonth, \"Year\", \"Month\", sep = \"_\")\n  \n#group by the YearMonth column and get the min and max row index (layer number) for each month, order by index number\ntime_table <- time_table |> \n    group_by(YearMonth) |> \n    summarise(MinIndex = min(RowId),\n              MaxIndex = max(RowId)) |> \n    arrange(MinIndex)\n\n#visualise the data\nhead(time_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 3\n  YearMonth MinIndex MaxIndex\n  <chr>        <int>    <int>\n1 2020_7           1       31\n2 2020_8          32       62\n3 2020_9          63       92\n4 2020_10         93      123\n5 2020_11        124      153\n6 2020_12        154      184\n```\n\n\n:::\n:::\n\n \n 3. Use `slice()` to extract all the layers per month\n 4. Use `st_apply()` to apply the mean function to all the layers in the month\n 5. Put this inside a `map2()` function to run the code for each month at the same time:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#use map to work through each start-end index and use st_apply to apply the mean\nmonthly_chla <- map2(time_table$MinIndex, time_table$MaxIndex, function(a,b) {\n  \n  #apply mean to the data slice\n  st_apply(slice(example_data, time, a:b),\n           MARGIN = c(\"x\",\"y\"), #using margin x and y to keep lat and long information\n           FUN = mean,\n           na.rm = T,\n           keep = T)\n       \n})\n```\n:::\n\n\n 6. Combine the list output back into a single stars object:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#bind the output into a single stars object. Note there are two \"c\"s here. The first (left) one binds the args. The second (right one) provides the args (list of stars object) plus the final argument (along = \"time\") which tells the first c to bind along a new dimension.\nmonthly_chla <- do.call(c, c(monthly_chla, along = \"time\"))\n```\n:::\n\n\nDone! We can then visualise the data to confirm it worked:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(monthly_chla) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\nSeems good to me! Although we would likely have to fix up those layer names/dates.\n\n:::{.callout-note}\nThe `st_apply()` function is not limited to just the `mean()` function, or even just simple functions at all. It can take in any custom function that you write - provided it has been written to work with matrices. For example, you could run the `min()` function to get a map that shows the min value at each cell, or if your data has spectral bands you could write a function to calculate the NDVI value for a vegetation assessment. The possibilities are endless!\n:::\n\n### aggregate()\n\nAs noted earlier, the `aggregate()` function is a much simpler method for aggregating a stars object and returning data with a lower spatial or temporal resolution. This function works in a similar way, it also has three main arguments:\n\n 1. X (the stars object),\n 2. by, and\n 3. FUN (the function to apply)\n 4. ... (additional arguments such as na.rm = T)\n \nAgain, arguments 1 and 3 are self explanatory, but the second argument is not. The \"by\" argument takes either an sf object (a spatial object) to do spatial aggregation, or a vector of grouping values. The sf object is fairly simple, it acts similar to a mask - anything inside the object is part of the group, anything outside is not. The vector is a bit more flexible, it could be a vector of time values - for temporal aggregation, or it could be a vector of latitude values for spatial aggregation, or a vector of longitude values for spatial aggregation. What it **can't** be is more than one of those things, if you want a combination you must use the `st_apply()` method. To be fair, I cannot think of a single reason why you would want to supply a lat/long value for aggregation this way when st_apply is so much better, so we will effectively treat the \"by\" argument as either an sf object, or a time vector.\n\nLets first demonstrate this with a spatial object, for this we are going to need to load in an sf object, so lets just use the one we originally used to extract the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in the dry tropics region dataset and update crs to projected cords\ndt_region <- st_read(\"dt_region.gpkg\") |> \n  st_transform(\"EPSG:7855\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `dt_region' from data source \n  `C:\\Users\\adams\\GitHub\\website\\posts\\ereefs_mapping_data\\dt_region.gpkg' \n  using driver `GPKG'\nSimple feature collection with 32 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 146.1444 ymin: -19.70039 xmax: 148.2985 ymax: -17.62597\nGeodetic CRS:  GDA2020\n```\n\n\n:::\n\n```{.r .cell-code}\n#demonstrate the aggregate function with an sf object\nagg_example <- aggregate(example_data,\n                         dt_region,\n                         mean,\n                         na.rm = T)\n\n#create a simple plot of the data\ntm_shape(agg_example[,,1]) +\n  tm_polygons(fill = \"Chla\", \n              fill.scale = tm_scale_intervals(n = 6,\n                                              values = my_pal,\n                                              label.format = list(digits = 2)),\n              fill.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nWhich is kind of interesting as we can see that there must be a slight overlap between land and marine for those land polygons to contain values. However, generally I find I don't use this method all that often - despite really wanting to find reasons too.\n\nOf course, the other options is the temporal vector. This actually has some handy short cuts where you can supply a vector of time values, or just a simple string like \"months\", or \"5-days\", etc. For our purposes we will use the string \"months\" which seems to work just fine:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#this aggregates data by month\nagg_example_2 <- aggregate(example_data,\n                           by = \"months\",\n                           mean)\n```\n:::\n\n\nHowever due to weirdness inside the function before we can visualise the output we need to now fix the dimension values as they are out of order. Specifically, after the aggregation they are:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#look at the dimensions of the object\ndim(agg_example_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntime    x    y \n  12  161  179 \n```\n\n\n:::\n:::\n\n\nWhile we need them to be:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#reorder dimensions\nagg_example_2 <- aperm(agg_example_2, c(\"x\", \"y\", \"time\"))\n\n#look at dimensions\ndim(agg_example_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x    y time \n 161  179   12 \n```\n\n\n:::\n:::\n\n\nOnce reordered, we can then visualise just fine:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(agg_example_2) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\nAnd look at that, we now have 12 layers with monthly mean concentration values, which much less effort than `st_apply()`, cool! However it should be noted that we also have much less control over this method, for example if we had ver specific date ranges, or lat and long values it might be a better idea to use the `st_apply()` function.\n\n# Map The Data\n\nOkay so I know we have already mapped the data a bunch of times above, but I would like to explore the visuals just a little further before we round out this blog. Specifically, I would like to add some visual cues to provide a point of reference. These include:\n\n - The sf object that was initially used to extract the data\n - An sf object for the main land\n - An sf object for the coral reefs in the region\n\nLets load in each of these in from file as I have prepared them earlier:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in the dry tropics region dataset and update crs to projected cords\ndt_region <- st_read(\"dt_region.gpkg\") |> \n  st_transform(\"EPSG:7855\")\n\n#read in the reefs dataset\nreefs <- st_read(\"reefs.gpkg\")\n\n#read in the queensland border dataset\nqld <- st_read(\"qld.gpkg\")\n```\n:::\n\n\nFollowing this, lets slap each of those onto the data.\n\n:::{.callout-note}\nBy the way, if you wanted to learn more about mapping using these `tmap` functions, you can check out my blog dedicated to the functions [here](../making_beautiful_maps_in_r/index.qmd)\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(qld) +\n  tm_polygons(fill = \"#99B5B1\",\n                col = \"#7bba9d\") +\n  tm_shape(slice(agg_example_2, time, 1), is.main = T) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(reefs) +\n  tm_borders(fill = \"grey60\",\n             fill_alpha = 0.2,\n             col = \"grey60\",\n             col_alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nLooking much better, we can see exactly where the coastline and the continental shelf is, where the reefs are, and have a good understanding of the overall region in which we are looking at.\n\nI've created this map as a single layer so we can see the change a bit better, but now I will roll these changes out to the facet map as well.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\ntm_shape(qld) +\n  tm_polygons(fill = \"#99B5B1\",\n                col = \"#7bba9d\") +\n  tm_shape(agg_example_2, is.main = T) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(reefs) +\n  tm_borders(fill = \"grey60\",\n             fill_alpha = 0.2,\n             col = \"grey60\",\n             col_alpha = 0.4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\nSaving maps is no problem either, simply pass the mapping code into an object, and then use `tmap_save()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot of the data\nour_final_map <- tm_shape(qld) +\n  tm_polygons(fill = \"#99B5B1\",\n                col = \"#7bba9d\") +\n  tm_shape(agg_example_2) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(reefs) +\n  tm_borders(fill = \"grey60\",\n             fill_alpha = 0.2,\n             col = \"grey60\",\n             col_alpha = 0.4)\n\n#save the map\ntmap_save(our_final_map, \"our_final_map.png\")\n```\n:::\n\n\n# Caveats\n\nAs always I would like to remind you to thoughtfully consider everything you read on the internet. This blog is my own work based on my own research into the topic. There may be practices I use that aren't considered \"best practice\" that I am not aware of, and I highly recommend that you do further exploration into the topic if it is something that interests you.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}