{
  "hash": "973f2a0f9a20f934e736730d8e46138b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear Modelling in R\"\ndate: \"07/16/2025\"\nabstract-title: \"ABSTRACT\"\nabstract: \"Naming your dataframe columns doesn't have to be hard, does it? Here I demonstrate the benefits of implimenting an opionated dataframe cleaner to help keep your columns organised.\"\nimage: \"image.png\"\nformat: html\ntitle-block-banner: true #This is our banner\ninclude-after-body: \"../../html/html_footer.html\" #This is our footer\n---\n\n# Introduction\n\nIn this blog I would like to explore the basics of data modelling using the tidymodels set of packages in R. If you're anything like me the phrase \"modelling\" has probably come to fill you with doubt. The term is often thrown around loosely and could apply to everything from simple regression, all the way to some crazy AI implimentation that only 2 people in the entire world understand. It can be hard to differentiate between these extremes, and to the lay person sometimes these are basically the same thing. I have imagined several scenarios in which I say \"model\" to a manager and they picture the next AI revolution has come to fix all of their problems (what I actually mean is I did some linear regression).\n\nTo combat this, for my own peace of mind, and hopefully yours, I have decided to write a blog (or two) about learning how to use the tidymodels packages in R. Ideally, by the end of this series we are both able to explain in more detail exactly what we are doing to our colleagues.\n\n# Linear Regression\n\nTo start off with I'd like to explore linear regression. Lets create a hypothetical dataset of tree height and elevation. In this dataset we are going to say that this species of tree likes to grow at high elevations. Therefore as elevation increases, so does tree height. Why do these tree like high ground? I don't know. What makes them grow taller? Who cares. Is this scenario realistic? No, but it makes for a very clear story telling experience. Let's go:\n\n## Building A Dataset\n\nThe first thing we need to do is build a dataset that explores this relationship between elevation and tree_height. I am going to manually add 11 elevations values evenly spaced between 0m and 1000m, and then assign tree heights to each elevation. Note that we want to show a relationship so I will deliberately increase tree heights as elevation increases, however to make things interesting I will make sure it is not a completely perfect reationship.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\n#create an example dataset, note that we are being very delibrate to make tree height increase with elevation\ntree_height_x_elevation_df <- data.frame(elevation = seq(0, 1000, 100),\n                                         tree_height = c(0,5,23,32,24,33,40,48,40,50,55))\n```\n:::\n\n\nOn a plot, this is how our make believe data looks, obviously the trend is pretty easy to spot since we specifically created the data for this purpose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a simple plot to show the data pointsw\nggplot(tree_height_x_elevation_df, aes(x = elevation, y = tree_height)) +\n    geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Building A Linear Regression Model\n\nTo make a linear model using this dataset we are going to leverage the tidymodels() package, which is actually a wrapper for a collection of packages that do a wide range of things within the modelling ecosystem such as preparing data, creating models, and evaluating model performance. Making a linear model is actually pretty straight forward when we use these packages, there are only really three steps:\n\n 1. Define the model to be used (for us this is a linear regression)\n 2. Define the \"engine\" that runs the model (for us there is only one option \"linear model\", but for more complex models there are multiple methods of implementation)\n 3. Fit the model, i.e. input the required variables (for us our independant variable is elevation, and our dependent variable is tree height)\n\n:::{.callout-note}\nWhen you are using \"real\" data that wasn't made up for educational purposes there are extra steps in the model building stage focused on properly preparing the data such as removing colinearity and normalsing numeric data to have a standard deviation of one. But we are not going to cover those in this example.\n:::\n\n:::{.callout-note}\nInterestingly, for a lot of models (not just linear regression) made using tidymodels packages, these general steps are almost the same. You can swap out a different model and engine while keeping the same inputs if you wanted.\n:::\n\nThe code to create our linear model is as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a linear model based on the example data\nmy_lin_mod <- linear_reg() |> #set up the framework\n    set_engine(\"lm\") |> #choose the linear model method as the \"engine\"\n    fit(tree_height ~ elevation, data = tree_height_x_elevation_df) #dependent ~ independent, define the dataset\n```\n:::\n\n\nNext, to view the model that we just created we can use the ` tidy()` function to return a nice layout:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#get a nicer view of the information inside the object\ntidy(my_lin_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   6.64     3.49         1.90 0.0901   \n2 elevation     0.0504   0.00591      8.53 0.0000133\n```\n\n\n:::\n:::\n\n\nWhich, on its own, it is not really anything special - just a table. But intepreting the table can allow us to understand a bit about the linear regression model that we just created.\n\n - The intercept estimate is the y-intercept (where the regression line crosses the y axis)\n - The elevation estimate is the slope of the regression line\n - Together, these define the equation of the regression line, which would be: y(pred) = 6.6363636 + 0.0503636x\n\nPutting this line on the plot from earlier demonstrates this nicely:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tree_height_x_elevation_df, aes(x = elevation, y = tree_height)) +\n    geom_point() +\n    geom_abline(slope = 0.0517, intercept = 6.24)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nPretty cool. Note how the regression line seems to split right down the middle of all our datapoints, the close the line is to all the points, the better our model is \"fitted\" to the data. Speaking of model fit, we can now look at the \"fit\" (i.e. the accuracy) of our model by the numbers!\n\n## Evaluating a Linear Regression Model\n\nThe performance of a linear model can be evaluated several ways, and we are going to touch on pretty much all of them. Firstly lets bring up the tabular summary of the model again:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#re print the tabular summary of the model\ntidy(my_lin_mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>\n1 (Intercept)   6.64     3.49         1.90 0.0901   \n2 elevation     0.0504   0.00591      8.53 0.0000133\n```\n\n\n:::\n:::\n\n\nLooking at the table again there are a few additional columns to cover, these speak to the accuracy of the regression line.\n - The std.error column is the standard error of the estimate, with smaller values indicating greater precision. You can think of this as how each of the observations are from the regression line.\n - The statistic is the estimate divided by the std.error. In this case large values are often a case for some kind of statistical significance (that is to say that the std.error is much smaller than what ever the estimate value is). \n - The p.value is the one familar to most introductory statistics students, and represents the likelihood of randomly observing the slope (estimate for dependent variable - e.g. elevation). I.e. if elevation had no real effect on tree height (slope of the regression line = 0), then the chances of getting a slope as large as 0.0503636 just from random noise are about 1.326928\\times 10^{-5}%.\n\nSo why is one row significant and one row not? Well the first row is talking about the intercept. It is saying, is the intercept statistically different from 0? I.e., when elevation is 0, is a tree of 6.6363636m any more or less likely than a tree of 0m? The answer is no (because the p value is high (>0.05) and the statistic is low we don't have any strong evidence to disprove this). Conversely, the second row the table is talking about the slope. It is saying, is the slope significantly different from 0? (zero being no relation) I.e. Does tree height change with elevation? The answer is yes - because the p value is low (<0.05) and the statistic is high we have strong evidence to disprove the null hypothesis. Further more, because the slope is positive, not negative, we can say that tree height increases with elevation.\n\n:::{.callout-note}\nThere are additional methods for evalutation the performance of our model, but we will explore these further into the blog.\n:::\n\n## Using a Linear Regression Model\n\nNow that we have established our linear model is not useless, what is the point of the model, and how do we use it? Well point 1 is simply to be able to confirm \"yes, tree height does change with elevation\", congratulations we can all go home. But that is kind of boring and doesn't have a satisifying conclusion, particularly because we specifically made this data up to have that relationship. Point 2 is that we can use this model to predict the height of trees that we have never observed before.\n\nImagine that the data I just made up is from Hill A, and just over the way, is a second hill; Hill B:\n\n![insert diagram](image_1.png)\n\nUnfortunately there is no road to get to that hill and all you know about the hill is its elevation profile, but your team is particularly interested in the height of trees there. If we assume that the tree species is the same on each hill, we can use our fancy new model to predict the height of the trees on Hill B, without ever going there.\n\nThis is acheived using the predict() function from the tidymodels group of packages. To use predict, obviously I need to create some elevation data for Hill B for us to predict on, so I will also do that here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create another set of fake data, this time its is the elevation of Hill B, it will not contain tree height - we are going to predict that\nhill_b_elevation <- data.frame(elevation = c(317, 842, 569, 74, 926, 458, 13, 731, 287, 652))\n\n#use the linear model to predict values\nhill_b_output <- my_lin_mod |> \n    predict(hill_b_elevation) #note that the column name must match what was used in the model\n\n#view the output\nglimpse(hill_b_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 1\n$ .pred <dbl> 22.601636, 49.042545, 35.293273, 10.363273, 53.273091, 29.702909…\n```\n\n\n:::\n:::\n\n\nThe output of the predict function is provided as a table, rather than a vector, because a common next step with the predicted values is to join them back to the original elevation values. Thus we will do that now:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#add the columns from the original dataset onto the predicted values\nhill_b_output <- hill_b_output |> \n    bind_cols(hill_b_elevation)\n\n#view the data\nglimpse(hill_b_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 2\n$ .pred     <dbl> 22.601636, 49.042545, 35.293273, 10.363273, 53.273091, 29.70…\n$ elevation <dbl> 317, 842, 569, 74, 926, 458, 13, 731, 287, 652\n```\n\n\n:::\n:::\n\n\nAnd now we have predicted tree height values for trees on Hill B, without ever having gone to that hill! Thats fun.\n\nAlso here is a visualisation of the new data combined with the old data. Something that might not be clear until seeing this that each of the predictions land exactly on the regression line:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot the original data, the line from the linear model, and the predicted dat\nggplot() +\n    geom_point(data = tree_height_x_elevation_df, aes(x = elevation, y = tree_height), col = \"blue\") +\n    geom_abline(slope = 0.0517, intercept = 6.24) +\n    geom_point(data = hill_b_output, aes(x = elevation, y = .pred), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWhat probably comes to mind looking at this is \"how accurate is this line?\" Yes we know that the linear model proved there was a significant relationship between elevation and tree height, but how strong is the relationship? How accurate is that line on the graph?\n\nAn easy way to test the accuracy of the model is to have some training data, and some testing data. Training data is data used to train the model. This data is like the blue dots on our graph, for each data point we know both the height of the tree, and the elevation. The training data is shown to the model, and the regression line is created. Testing data is additional data that we withhold from the model. \n\nNote that in the testing dataset we also know both the height of the tree, and the elevation. Generally, training data and testing data come from the same parent dataset, and each group is created randomly. The training dataset normally receives about 80% of the total data, and 20% of the data is withheld for testing, however the split could be whatever you want - if you can justify it.\n\nTo split the dataset into testing and training we can use the `initial_split()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create a split of the data\nt_e_split <- initial_split(tree_height_x_elevation_df)\n```\n:::\n\n\nNote that the output of this is no longer just a df, it is a rplit object. When looking at it you can see the division of rows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#view object\nt_e_split\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n<Training/Testing/Total>\n<8/3/11>\n```\n\n\n:::\n:::\n\n\nTo access specifically the training or testing data from this object you can use the `training()` or `testing()` functions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#to see the training or testing part of the data, use training() or testing()\nt_e_split |> \n    testing() |> \n    glimpse()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3\nColumns: 2\n$ elevation   <dbl> 400, 500, 600\n$ tree_height <dbl> 24, 33, 40\n```\n\n\n:::\n:::\n\n\nWith our dataset split we can the create a new linear model the same way we did before, but this time we are only going to show it 80% of the data (the training data):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#train a new model on just the training data\nnew_lm <- linear_reg() |> \n    set_engine(\"lm\") |> \n    fit(tree_height ~ elevation, data = training(t_e_split))\n\n#view new model\ntidy(new_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   6.72     4.40         1.53 0.178   \n2 elevation     0.0498   0.00709      7.02 0.000415\n```\n\n\n:::\n:::\n\n\nWith the new model trained, we can now use it to predict values based on the testing data. Remember that in this case we know both the elevation value and the true tree height value of our testing data (this varies from the scenario above with Hill B where we only knew the elevation). The goal of predicting on values that we already know the tree height for is to see how close we get to the real answer:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#test new model on just the testing data\ntesting_output <- new_lm |> \n    predict(testing(t_e_split)) |> #use model to predict tree heights based on elevation\n    bind_cols(testing(t_e_split)) #bind the full testing dataset on to the predicted outputs\n\ntesting_output\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .pred elevation tree_height\n  <dbl>     <dbl>       <dbl>\n1  26.6       400          24\n2  31.6       500          33\n3  36.6       600          40\n```\n\n\n:::\n:::\n\n\nLooking at the table, the .pred column is the models predictions based on the elevation, and the tree_height column is the actual height of the tree measured at that elevation. The model does seem to be broadly correct, but how correct? Thankfully the tidymodels package also gives us an easy way to compare the predicted values against the true values using the `metric()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#you can see accuracy metrics using the metrics() function\ntesting_output_metrics <- testing_output |> \n    metrics(truth = tree_height, estimate = .pred)\n\ntesting_output_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       2.61 \n2 rsq     standard       0.995\n3 mae     standard       2.47 \n```\n\n\n:::\n:::\n\n\nOkay cool, but what do these values actually mean?\n\n  - RMSE is Root Mean Square Error, it is the average difference between the predicted values and the actual values. So for us, it is saying our model is on average 2.6073475 meters from the real value.\n  - RSQ is \"R Squared\", it tells us as a proportion how much of the variance in the dependent variable is predictable from the independent variable. In our case it is saying our model can explain 99.4818653% of the variance in tree height using the elevation. Nice!\n  - MAE is Mean Absolute Error, it is also looking at the average distance between the predicted and actual values, but it is not squaring this number. In a nutshell this make RMSE very sensitive to large errors, but MAE treats all errors equally. In our case, the MAE is saying our model is on average 2.470679 meters from the real value. This is a fairly similar value to RMSE and thus also tells us that there are not any particularly large errors distorting the average error.\n\n# Extending the Linear Model\n\nSo far we have only looked at modelling values within the original bounds of our training dataset. By that I mean, in our training data elevation ranged from 0m to 900m, and when we have predicted values, we have only predicted values for elevations from 0m to 900m. So what happens if we look further afield?\n\nLet's pretend there is a third hill; Hill C. This hill is huge, and has a max elevation of 5000m! Could we use our model to try and predict tree height on this hill? In theory yes, but there are important caveats and precautions that need to be taken. Let's take a look.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create hill C data\nhill_c_elevation <- data.frame(elevation = c(3172, 4821, 1398, 2784, 4502, 3567, 1210, 4935, 2123, 3894))\n\n#use the linear model to predict values\nhill_c_output <- my_lin_mod |> \n    predict(hill_c_elevation) |> #note that the column name must match what was used in the model\n    bind_cols(hill_c_elevation)\n\n#view the output\nglimpse(hill_c_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 2\n$ .pred     <dbl> 166.38982, 249.43945, 77.04473, 146.84873, 233.37345, 186.28…\n$ elevation <dbl> 3172, 4821, 1398, 2784, 4502, 3567, 1210, 4935, 2123, 3894\n```\n\n\n:::\n:::\n\n\nSo far so good, and here is the plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot the original data, the line from the linear model, and the predicted dat\nggplot() +\n    geom_point(data = tree_height_x_elevation_df, aes(x = elevation, y = tree_height), col = \"blue\") +\n    geom_abline(slope = 0.0517, intercept = 6.24) +\n    geom_point(data = hill_c_output, aes(x = elevation, y = .pred), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\nIt is at this point that some warning sirens should be going off in your head. Looking at the data used to train our model (blue) and the data we are predicting (red) you can immediately see there is no overlap. Right now we are just crossing our fingers and trusting that the relationship between elevation and tree height holds true. \n\nLet us push this a little further, the tip of Mount Everest is 8849m above sea level, lets use our model to see how tall a tree would be at the top of Mount Everest:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt_everest <- data.frame(elevation = c(8849))\n\nmt_ev_model <- my_lin_mod |> \n    predict(mt_everest) |> \n    bind_cols(mt_everest)\n\nmt_ev_model\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  .pred elevation\n  <dbl>     <dbl>\n1  452.      8849\n```\n\n\n:::\n:::\n\n\nThis says the tree would be 452.3041818m tall. Fun fact, the worlds tallest tree is 115m tall... So not only is this tree ridiculously, unrealistically, stupidly tall. It is also on the top of everest.... where nothing grows.\n\nIf this is not clear enough yet, lets get truely absurb and predict on some negative elevation (i.e) moving below sea level:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#create hill C data\nunder_da_sea <- data.frame(elevation = c(-437, -982, -153, -764, -305, -621, -48, -889, -230, -519))\n\n#use the linear model to predict values\nunder_da_sea_output <- my_lin_mod |> \n    predict(under_da_sea) |> #note that the column name must match what was used in the model\n    bind_cols(under_da_sea)\n\n#view the output\nglimpse(under_da_sea_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 10\nColumns: 2\n$ .pred     <dbl> -15.372545, -42.820727, -1.069273, -31.841455, -8.724545, -2…\n$ elevation <dbl> -437, -982, -153, -764, -305, -621, -48, -889, -230, -519\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#plot the original data, the line from the linear model, and the predicted dat\nggplot() +\n    geom_point(data = tree_height_x_elevation_df, aes(x = elevation, y = tree_height), col = \"blue\") +\n    geom_abline(slope = 0.0517, intercept = 6.24) +\n    geom_point(data = under_da_sea_output, aes(x = elevation, y = .pred), col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nSo now, our model is telling us that when we go below sea level (e.g. under water, although someplaces in the world e.g. the Netherlands are still above water even with <0. elevation) our trees should have negative height values. Does this make any sense at all? No obviously not. \n\nThis is an important lesson in the application of models, and a warning about applying your model in unfamilar scenarios.\n\nLooking at these silly scenario's has hopefully made you ask how you can address the problem/avoid it/model the scenario better. There are plenty of ways to do this, the most obvious of which is to deploy your model with a string of caveats. E.g. \"Model only valid for Hill A and Hill B\", \"Model should not be used beyond 1000m\", etc. etc. However, these caveats don't actually stop anyone from using the model in this way - particularly if they are like D.W from Arthor:\n\n![image_2](image_2.png)\n\nInstead, statisticians will often seek to better model the environment using things like multiple linear regression, or non-linear regression to name a few. Don't worry though, we will explore some of these more advanced methods in another blog. For now, thats it! Thanks for reading.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}