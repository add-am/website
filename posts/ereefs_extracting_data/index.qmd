---
title: "The Extraction of Highly Specialised Modeled Data from eReefs"
date: "05/23/2025"
abstract-title: "ABSTRACT"
abstract: 'In this blog I talk about the skills needed and the steps taken to execute the extraction of modelled environmental data such as ocean currents, nutrient loads, and water clarity from the online platform "eReefs".'
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| label: hidden set up code for our page
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

library(sf)
library(ereefs)
library(tmap)
library(ncmeta)
library(stars)

```

# Introduction

In this blog post I'd like to cover the essentials of extracting data from the eReefs platform. First of all, hands up - who's heard of [eReefs](https://www.ereefs.org.au/)? Fair enough if you haven't, despite its star power in my world it is still a relatively niche topic. To summarise, eReefs is *"a comprehensive view of dynamic marine environment conditions across the Great Barrier Reef from end-of-catchments and estuaries to reef lagoons and the open ocean."* What this means for us is that it has a whole bunch of modelled environmental datasets that are relatively easy to access, backed by top-notch science, and **heavy** (i.e. we really get to flex our coding and spatial "muscles"). 

The goal today is to learn how to:

 1. Extract data from eReefs - a surprisingly hard thing to do
 2. That's it - the data extraction section is already going to make this a long post.
 
:::{.callout-note}
If you read this blog and want to learn what to do once you have the data, make sure to check out these two additional blogs that explore [plotting](ereefs_plotting_data/index.qmd) and [mapping](ereefs_mapping_data/index.qmd) eReefs data.
:::

# Extracting Data

The datasets we are going to look at today are the chlorophyll a dataset, and the nitrites dataset. However there are hundreds of datasets to choose from including water movement variables, water clarity indicators, and a whole host of chemical indicators. Later in this blog I will show you how to obtain a complete list of variables available. 

All data produced by eReefs is stored on the [National Computing Infrastructure's (NCIs) THREDDS server](https://thredds.nci.org.au/thredds/catalog/catalogs/fx3/catalog.html) which is a server we can interact with in an automated fashion. Don't be fooled though, accessing the data can still pose quite the challenge and can sometimes seem like you are feeling your way around in the dark.

To assist in the process of extracting data from eReefs we are going to need an extra dataset that details the specific boundaries of a target region within the scope of the eReefs model. I will be using the boundaries of the Dry Tropics region (near Townsville, QLD), however any boundaries withing the Great Barrier Reef Marine Park will work fine.

Lets get into it.

## Step 1 - Obtain Facilitating Dataset

This is just about the only simple step in this blog. 

Below I load in my boundaries of the Dry Tropics region, unfortunately this is a custom dataset that cannot be made available for download, however any polygon area within the Great Barrier Reef region will work so I encourage you to create your own.

```{r}
#| output: FALSE

#read in the dry tropics region dataset
dt_region <- st_read("dt_region.gpkg") |> 
  st_transform("EPSG:7844")

```

:::{.callout-note}
In a pinch for coordinates? Use these for a simple box: 

```{r}

library(sf)

example_box <- matrix(c(144.227, -24.445,   # bottom-left
                        144.227, -15.195,   # top-left
                        151.329, -15.195,   # top-right
                        151.329, -24.445,   # bottom-right
                        144.227, -24.445),    # close the polygon by repeating the first point
                      ncol = 2, 
                      byrow = TRUE)

#create a polygon geometry and set the CRS
example_box <- st_polygon(list(example_box)) |> 
  st_sfc(crs = "EPSG:7844")

```
:::


## Step 2 - Getting eReefs Data

I would first like to note that there are several resources online that contain useful information about access data from eReefs. These include:

 - [An eReefs R Package](https://github.com/open-AIMS/ereefs)
 - [Multiple eReefs Tutorials](https://open-aims.github.io/ereefs-tutorials/)
 - [And, the National Computing Infrastructure's (NCIs) THREDDS server](https://thredds.nci.org.au/thredds/catalog/catalogs/fx3/catalog.html)
 
However, I personally find that most of these resources either A) gloss over what is really happening behind the scenes, or B) don't provide critical information needed for you to go away and conduct your own analysis (for example, how to see what indicators are available in eReefs). It is these reasons among others that prompted me to write this blog.

Anyway, time to buckle your seatbelts kids.

### Connecting to the Data

The first thing we are going to do today is establish a connection to the database. To do this we are going to need to load the `ereefs()` package as this contains the handy functions `substitute_filename("catalog")` and `get_ereefs_grids()`. First, we will run the `substitute_filename("catalog")` function, which will return a list of all the available datasets that we can choose from. This list is interactive and requires user input - I have picked "5" - "eReefs GBR1 biogeochemistry and sediments v3.2" as this is the most up-to-date model. However, there are older models and models that have been run under different scenarios if you are interested in those instead.

```{r}
#| eval: false

#load in the ereefs package
library(ereefs)

#run the function
substitute_filename("catalog")

```

Once we have made our selection it will return a url. This url is how we are going to connected to the correct database, if you are interested, [this](https://thredds.nci.org.au/thredds/catalog/catalogs/fx3/catalog.html) is a manual view of the range of data that we are choosing from.

We can these use this url as an argument in the `get_ereefs_grids()` function from the `ereefs` package.

```{r}

#manually assign the url from above into a variable (so I don't have to interact with the code)
input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
    
#get all grids
grids <- get_ereefs_grids(input_file)

```

### Gaining Perspective

If the lines of code above worked, congratulations - you can access the data. Now take a look at the object `grids` and you will probably realise that we are only 3 lines of code in and things are already pretty intense - WTF is this object and what does it tell us? 

What we just did is get the dimensions of the dataset. The `grids` object should be a list of length 3, the 3 items in the list should be "x_grid", "y_grid", and "z_grid", each of the these tell us something about one dimension of the data (the longitude, latitude, and depth). Unfortunately, because each of these items are bloody huge manually viewing the object to try and learn about it is essentially useless. Below we use some simple code to explore the grids.

```{r}

#extract just the x grid
x_grid <- grids[["x_grid"]]

#get some basic information about the dataset
xmin <- min(x_grid, na.rm = T)
xmax <- max(x_grid, na.rm = T)
x_dim <- dim(x_grid)

```

The x_grid tells us about longitude The min x value is `r xmin`, the max x value is `r xmax`, and the dimensions of the x_grid are `r x_dim`.

```{r}

#extract just the y grid
y_grid <- grids[["y_grid"]]

#get some basic information about the dataset
ymin <- min(y_grid, na.rm = T)
ymax <- max(y_grid, na.rm = T)
y_dim <- dim(y_grid)

```

The y_grid tells us about latitude. The min y value is `r ymin`, the max x value is `r ymax`, and the dimensions of the y_grid are `r y_dim`. 

By looking at the x and y values we can get an idea of where we are in the world:

```{r}

as.data.frame(head(grids[["y_grid"]], n = c(5,5)))

#create a bbox
ereefs_extent_bbox <- matrix(c(xmin, ymin,   # bottom-left
                               xmin, ymax,   # top-left
                               xmax, ymax,   # top-right
                               xmax, ymin,   # bottom-right
                               xmin, ymin),    # close the polygon by repeating the first point
                      ncol = 2, 
                      byrow = TRUE)

#create a polygon geometry and set the CRS
ereefs_extent_bbox <- st_polygon(list(ereefs_extent_bbox)) |> 
  st_sfc(crs = "EPSG:7844")

tm_shape(World) +
  tm_polygons() +
  tm_shape(ereefs_extent_bbox) +
  tm_polygons(col = "red",
              fill = NULL)

```

```{r}

#extract just the x grid
z_grid <- grids[["z_grid"]]

#get some basic information about the dataset
z_min <- min(grids[["z_grid"]], na.rm = T)
z_max <- 0 #using max returns the "wrong" value for our discussion
z_dim <- dim(grids[["z_grid"]])

```

The z_grid tells us about depth (eReefs models the entire water column). The min z value is `r z_min`m, the max x value is `r z_max`m, and the dimensions of the z_grid are `r z_dim`. These values tell us at what depth each layer of the model is at, and how many layers there are.

In combination these three grids tell us everything we need to know about the data. Lets first look at the x_grid, as we noted above, the dimensions of the x_grid are `r x_dim`, thus picture a table that has `r dim(grids[["x_grid"]])[1]` rows, and `r dim(grids[["x_grid"]])[2]` columns. Once again, here is a snapshot of the first five rows and columns of the grid: 

```{r}

as.data.frame(head(x_grid, n = c(5,5)))

```

In contrast, lets now consider the y_grid, this grid has the exact same dimensions as the x_grid, and we can picture it much the same way: 

```{r}

as.data.frame(head(y_grid, n = c(5,5)))

```

If we combine these two grids together we can get a table in which every cell contains a pair of values, one x_grid value and one y_grid value:

|   |1                        |2                        |3                        |4                        |5                        |
|---|-------------------------|-------------------------|-------------------------|-------------------------|-------------------------|
|1  |151.8048, -28.59505      |151.8046, -28.57945      |151.8044, -28.56385      |151.8042, -28.54808      |151.8039, -28.53231      |
|2  |151.8140, -28.59506      |151.8138, -28.57942      |151.8137, -28.56378      |151.8134, -28.54800      |151.8132, -28.53222      |
|3  |151.8231, -28.59508      |151.8230, -28.57940      |151.8229, -28.56371      |151.8227, -28.54792      |151.8226, -28.53214      |
|4  |151.8324, -28.59510      |151.8323, -28.57938      |151.8322, -28.56367      |151.8321, -28.54787      |151.8319, -28.53206      |
|5  |151.8416, -28.59511      |151.8416, -28.57937      |151.8415, -28.56362      |151.8414, -28.54781      |151.8413, -28.53199      |


What we have now is a table where every single cell corresponds to a cell (value) in the eReefs model. That is to say, that for every cell in this table we just made, there is information about water temperature, turbidity, nutrients, etc., etc. To take things even further, if we include the z dimension depth we would have 45 copies of this table, with each copy of the table corresponding to 1 depth layer in the model.

Add that all up and we have a table that has `r dim(grids[["x_grid"]])[1]*dim(grids[["x_grid"]])[2]` cells, where the table is stacked 45 times in a row (depth), where every cell in every table has more than 200 different environmental variables. Hopefully that makes sense.

OK so sure, that's kind of cool I suppose, but why does this matter? Who cares?

Well, the reason this matters is that we can use this conceptual understanding of the model to be able to sift through all that data to pinpoint the exact thing that we want. You could use this almost like a GPS. For example, If I wanted to figure out the water temperature at 151.4, -23.2, at a depth of -40m, all I would need to do is say "give me the information at row 2, column 4". 

### Specify Our Target Location

To explain how we are going to specify our target I am going to keep the analogy of the table going. The idea is simple, lets once again imagine the table, the table is the exact same dimensions as the table we were talking about above, except the values in this table are all just "FALSE":

| |1    |2    |3    |4    |5    |
|-|-----|-----|-----|-----|-----|
|1|FALSE|FALSE|FALSE|FALSE|FALSE|
|2|FALSE|FALSE|FALSE|FALSE|FALSE|
|3|FALSE|FALSE|FALSE|FALSE|FALSE|
|4|FALSE|FALSE|FALSE|FALSE|FALSE|
|5|FALSE|FALSE|FALSE|FALSE|FALSE|

Lets say that we want to extract all the information within 151.2 to 151.4, and -23.3 to -23.5. What we then do is figure out where those cells are (based on their row and column number) using the table in the previous section, and then change the cells in those positions to TRUE in our current table:

| |1    |2    |3    |4    |5    |
|-|-----|-----|-----|-----|-----|
|1|FALSE|FALSE|FALSE|FALSE|FALSE|
|2|FALSE|FALSE|FALSE|FALSE|FALSE|
|3|FALSE|TRUE |TRUE |TRUE |FALSE|
|4|FALSE|TRUE |TRUE |TRUE |FALSE|
|5|FALSE|TRUE |TRUE |TRUE |FALSE|

We can then use this table to communicate with the database and tell it "only give me data that lines up with my true values, remove the rest". And that's kind of it! If all goes well, the database will return the exact data you requested. Lets see how that looks in code.

:::{.callout-note}
It is important to highlight here that the code we are about to write and the data we are working with does not take the form of an actual table, the above description is just a handy analogy to describe what is happening.
:::

The first thing we are going to do is get the boundaries of our target area.

```{r}

#use the bbox function to get the boundaries
target_bounds <- st_bbox(dt_region)

target_bounds

```

Then we use a series of logical steps that check the xmin, xmax, ymin, and ymax values of our target area and changes cells that fall inside these bounds to TRUE (those outside are given FALSE). There are also some cells that start as NA, so we change those to FALSE.

```{r}

#if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
true_false_array <- x_grid >= target_bounds[1] & 
  x_grid <= target_bounds[3] & 
  y_grid >= target_bounds[2] & 
  y_grid <= target_bounds[4]
  
#if the value is NA, change it to false.
true_false_array[is.na(true_false_array)] <- FALSE

```

### Obtain the "Coordinates" of Our Target Location

So what we did above was create an array that contains TRUE and FALSE values. The dimensions of this array perfectly match the dimensions of the data. Next up, we need to find the exact positions in the array where the values change from FALSE to TRUE (noting that TRUE means inside our area of interest). These positions will then correspond to the positions we need to send to the database. Here is the code to achieve this:

```{r}

#return the row index for every row that contains at least one true value:
true_rows <- which(apply(true_false_array, 1, any))

#find the first row that contains a true value
first_row <- true_rows[1]

#find the number of rows that contains a true value
num_of_rows <- tail(true_rows, n = 1) - first_row

#return the row index for every row that contains at least one true value:
true_cols <- which(apply(true_false_array, 2, any))

#find the first col that contains a true value
first_col <- true_cols[1]

#find the number of cols that contains a true value
num_of_cols <- tail(true_cols, n = 1) - first_col

```

Our values are as follows:

 - First Row = `r first_row`
 - Number of Rows = `r num_of_rows`
 - First Col = `r first_col`
 - Number of Cols = `r num_of_cols`
 
With that done we now have our "coordinates" to send to the database to tell it where to extract data from. 

### Specify Our Target Variable

Almost there, only one more part. In this section we are going to learn how to specify what variable to download. So far all I have told you is that eReefs has hundreds of variables, that's cool and all but what are their names? How do you access them? Thankfully the function `nc_vars()` from the `ncmeta` package can help us. Simply run the function for the input path we figured out earlier and it will return a table with all the variables available:

```{r}

all_variables <- nc_vars(input_file)

```

Be careful though, the dimensions on some of these variables are different, and you might need to provide more (or less) information to make it work. For example, some variables might not have a depth (z) aspect to them and you would need to drop this from the data request.

By looking at this table we can establish that to get the chlorophyll a and the nitrites datasets we need to supply the names "Chl_a_sum" and "NO3".

### Extract the Data

Its finally time, after all that we can start our data extraction!

The function we are going to use to extract the data is the `read_ncdf()` function from the `stars` package. This function takes several inputs, such as the source of the data, the variable we are interested in, and the "coordinates" we are interested in. Thanks to all the work we have done above we have all of the "hard" information, however there are still a few little things to tick off.

When we are talking about the "coordinates" of the data I have previously spoken about the x, y, and z dimensions of the data (the longitude, latitude, and depth). However there is one more dimension I haven't spoken about yet - time. Yes this data actually has 4 dimensions we need to specify. To keep things simple we will start off my just asking for a single point in time, but later we will move on to getting a full time series. So, when we supply the details we are essentially going to tell the database, 

 - On the x dimension; start at this cell, and keep going until this cell
 - On the y dimension; start at this cell, and keep going until this cell
 - On the z dimension; just give us one depth layer
 - On the t dimension; just give us one time step

In code, this is how it looks:

```{r}

#set our dimensions (44 is the surface depth, 100 is the 100th time step)
our_dimensions_request <- cbind(start = c(first_row, first_col, 44, 100),
                                count = c(num_of_rows, num_of_cols, 1, 1))

```

Which we can supply to the `read_ncdf` function (it will take a little while to run, that's fine):

```{r}

#extract the data
extracted_data <- read_ncdf(input_file, 
                            var = "Chl_a_sum", 
                            ncsub = our_dimensions_request)

```

If that code ran, congratulations you have official got the data.

## Step 3 - View and Save eReefs Data

What, there's still more to go? Unfortunately yes.

Lets immediately try and visualise the data:

```{r}

tm_shape(extracted_data) +
  tm_raster() +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Rather ugly yea? Also WTF is up with that scale? And why does the data extend past our area of interest?

 1. Yes it is rather ugly, but it will mostly be fixed by 2.
 2. The scale is because eReefs uses ridiculously high values for land cells (i.e. cells that shouldn't have a chlorophyll a value)
 3. This is because a) we used the maximum bounds of our object, and b) because the data is provided on a curvilinear grid which messes with the boundaries a bit (don't worry about it). This can be fixed with further masking.
 
Removing the land cells is rather easy, simply pick a value that your dataset would never reach (e.g. 1000ug/L) and change all cells with a value greater than that to NA. This fixes both points 1 and 2:

```{r}

extracted_data[(extracted_data > 1000)] <- NA

tm_shape(extracted_data) +
  tm_raster(col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

While masking the values outside our area is a little trickier but also not too much of a stretch:

```{r}

test <- dt_region |> st_make_valid() |> st_union() |> nngeo::st_remove_holes()

tm_shape(test) +
  tm_polygons()

#crop to the actual area of interest
extracted_data <- extracted_data |> 
  st_transform("EPSG:7844") |>
  st_crop(test)

tm_shape(extracted_data) +
  tm_raster(col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main =  T) +
  tm_polygons(fill = NULL,
              col = "black")

```




## eReefs Data

Now we can look to download the eReefs data. We will be using the spatial information for each of the regions that we loaded above to guide us when extracting the eReefs data.

::: {.callout-note}
Please note that this script will download the eReefs data from an online server the first time the script is run. This is a lengthy process given the size of the data (several gb) and will take a significant amount of time to process (approximately 20 minutes). To assist in future runs of the script, the data will be saved to your local computer and reloaded next time (approximate 30 seconds).
:::

Given that this is the 6th script in the series about eReefs data we already have a significant amount of contextual information about the data that makes it a lot easier to understand what data we are downloading and how we can access it. (Not all previous scripts have to be run, but reading them does help). 

Using script 5 we know that to get the data for each year we need to access the following layers:

- 2019-2020 = 1-215 (215 layers total)
- 2020-2021 = 216-580 (365 layers total)
- 2021-2022 = 581-945 (365 layers total)
- 2022-2023 = 946-1310 (365 layers total)
- 2023-2024 = 1311-1510 (200 layers)

For this script we can focus on downloading just the year of data that is closest to our target data year. I say closest because data is only released roughly every 2 years, thus sometimes the target year matches, sometimes it does not. The general method of downloading is as follows:

1. Set up a table containing the layer counter information determined above and then identify the year we are interested in.
2. Establish coordinate boundaries of the area of interest
3. Extract grid cell latitude, longitude information within this area of interest
4. Compare the coordinate boundaries of the area of interest with the grid cell latitude and longitude (this is because the grid cells are on a curvilinear grid and sometimes extend outside our area of interest due to the line "bending")
5. From this comparison, extract the true start and end points of the area of interest, accounting for bending
6. Download each financial year of data individually, using information from steps 4. and 5.

:::{.callout-note}
Use nc_vars(input_file) to get a table that lists all available variables. It will also tell you the dimensions needed to call the data (usually 3 or 4).
:::

```{r}
#| label: load the eReefs data



 
#save(list = glue("{i_lower}_{region_lower}"), 
#     file = glue('{output_path}/datasets/{i_lower}_{region_lower}.RData'))
        
  
  #overwrite erroneous high values (note that a value of even 50 would be very very high)
#  dataset[(dataset > 200)] <- NA
  
  #crop to the actual area of interest
#  dataset <- dataset |> 
#   st_transform(proj_crs)# |>



#map <- tm_shape(qld) +
#    tm_polygons(fill = "#99B5B1",
#                col = "#7bba9d") +
#    tm_shape(target_land_region) +
#    tm_polygons(fill = "grey90", 
#                col = "black") +
#    tm_shape(monthly_mean_df[[i]]) +
#    tm_polygons(fill = "Value",

```














