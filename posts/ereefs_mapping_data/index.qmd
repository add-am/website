---
title: "Creating Beautiful Maps Using eReefs Data"
date: "05/23/2025"
abstract-title: "ABSTRACT"
abstract: "In this blog I demonstrate how you can make beautiful maps in R using example data extracted from the eReefs platform. You can also follow along with any of your own data."
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

library(sf)
library(stars)
library(tmap)
#library(ggplot2)
library(tidyverse)
library(ereefs)
#library(scales)

sf_use_s2(FALSE)

```

# Introduction

This is part three of a series of blog that focus on eReefs and the data it provides. To follow along with this blog you will need to be able to download data from eReefs, you can learn how to do that in my first blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd). I would also recommend that you check my blog about [Plotting eReefs Data](../ereefs_plotting_data/index.qmd), however it is not essential reading for this post.

In this post I would like to explore the spatial aspect of eReefs data. Together we will learn some of the most important steps when working with this type of data as we:

 1. Manipulate and transform spatial data,
 2. Understand how to visualise the data, and
 3. Build a series of informative maps

# Read in Data

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

#if our csv does not exists, create it
if(!file.exists("example_data.nc")){

  #read in the dry tropics region dataset and start with a lat/long crs
  dt_region <- st_read("dt_region.gpkg") |> 
    st_transform("EPSG:7844")
  
  #manually assign the url from above into a variable (so I don't have to interact with the code)
  input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
      
  #get all grids
  grids <- get_ereefs_grids(input_file)
  
  #use the bbox function to get the boundaries of our targeted location
  target_bounds <- st_bbox(dt_region)
  
  #if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
  true_false_array <- grids[["x_grid"]] >= target_bounds[1] & 
    grids[["x_grid"]] <= target_bounds[3] & 
    grids[["y_grid"]] >= target_bounds[2] & 
    grids[["y_grid"]] <= target_bounds[4]
    
  #if the value is NA, change it to false.
  true_false_array[is.na(true_false_array)] <- FALSE
  
  #return the row index for every row that contains at least one true value:
  true_rows <- which(apply(true_false_array, 1, any))
  
  #find the first row that contains a true value
  first_row <- true_rows[1]
  
  #find the number of rows that contains a true value
  num_of_rows <- tail(true_rows, n = 1) - first_row
  
  #return the row index for every row that contains at least one true value:
  true_cols <- which(apply(true_false_array, 2, any))
  
  #find the first col that contains a true value
  first_col <- true_cols[1]
  
  #find the number of cols that contains a true value
  num_of_cols <- tail(true_cols, n = 1) - first_col
  
  #set our dimensions (44 is the surface depth, 100 is the 100th time step)
  our_dimensions_request <- cbind(start = c(first_row, first_col, 44, 216),
                                  count = c(num_of_rows, num_of_cols, 1, 365))
  
  #extract the data
  extracted_data <- read_ncdf(input_file, 
                              var = "Chl_a_sum", 
                              ncsub = our_dimensions_request)
  
  #update the crs
  extracted_data <- st_transform(extracted_data, "EPSG:7855")
  
  #change all land values to NA
  extracted_data[(extracted_data > 1000)] <- NA
  
  #convert our curvilinear object into just a bbox then update the crs on the bbox
  curvilinear_bbox <- extracted_data |> 
    st_bbox() |>
    st_as_sfc()
  
  #get a linear grid target with the same dimensions (number of cells) as our curvilinear grid 
  reg_stars <- st_as_stars(curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., 
                           nx = dim(extracted_data)[[1]], #and the dimensions to provide the x and y count. 
                           ny = dim(extracted_data)[[2]],
                           values = NA_real_) #Fill each cell with NA
  
  #run st warp, it requires a curvilinear object, and a regular object as a target
  warped_data <- st_warp(extracted_data, reg_stars)
  
  #update the crs
  dt_region <- st_transform(dt_region, "EPSG:7855")
  
  #crop to the actual area of interest
  reg_grid_cropped_data <- warped_data |> 
    st_crop(dt_region)
  
  #drop dimensions with only 1 value
  reg_grid_cropped_data <- reg_grid_cropped_data[drop = TRUE]
  
  #save the data 
  write_stars(reg_grid_cropped_data, "example_data.nc", driver = "netCDF")
  
  #get vector of time values and save them for later
  saveRDS(st_get_dimension_values(reg_grid_cropped_data, "time"), file = "time_vector.rds")
  
}

```

Thanks to our previous post about extracting the data, it is saved in our file system ready to open no stress, and we know exactly the preprocessing that we need to do:

```{r}
#| output: FALSE

#load in the example data
example_data <- read_stars("example_data.nc")

#load vector of time values
time_vals <- readRDS("time_vector.rds")

#merge "attributes" (time) back together
example_data <- merge(example_data)

#update time dimension values and names
example_data <- example_data |> 
  st_set_dimensions(3, time_vals,
                    names = c("x", "y", "time"))
  
#then update the attribute name
example_data <- setNames(example_data, "Chla")

#read in the dry tropics region dataset that we will use for spatial context
dt_region <- st_read("dt_region.gpkg") |> 
  st_transform("EPSG:7844")

```

# Analyse Data

There are few goals I have for analysing the data, I would like to be able to:

 1. Manipulate data using the time dimension, i.e. extract certain dates
 2. Manipulate data using spatial dimensions, i.e. extract certain areas
 3. Aggregate data using the time dimension, e.g. getting monthly average values
 4. Aggregate data using spatial dimensions, .e.g. getting average values per area
 
## Layer Manipulation

Before we can get right into analysis or mapping we need to get a better understanding of the data structure and what we are looking at. Simply by calling the object we can already get a pretty good breakdown:

```{r}

#get a summary of the data
example_data

```

A couple of things to point out here.

 1. The object is a "stars object", this is a class introduced by the `stars` package
    1.1 stars objects hold attribute(s) and dimensions
        1.1.1 attributes are our values (e.g. chlorophyll a), and there can be more than one
        1.1.2 dimensions are our lat, long, depth, time, spectral band, etc.
    1.2 This means stars objects can be n-dimensional **and** hold multiple attributes - which is a lot to think about
 2. We can see the summary statistics for our attribute (chlorophyll a)
 3. We can also see some information about our dimensions
 
To see the names of our dimensions we can use `dim()`.

```{r}

#view the dimension names and lengths
dim(example_data)

```

and to see the names of our attributes we can use `names()`.

```{r}

#view the names of the attributes
names(example_data)

```

When we look to analyse our data we are going to have to think about all of our dimensions and any attributes. The simplest way to interact with each of these is using the `[]` square brackets. The first element in the brackets corresponds to the attributes, and each element following this is one of the dimensions (in the order in which you see them using `dim()`): `stars_object[att, i, j, k, time]`.

 - If we wanted to get just the first time step, we would write `stars_object[], , , , 1]` where the blank entry just means give me all of it.
 - If we wanted the 2nd i, the 1st-5th j, and the 3rd time step, we would write `stars_object[, 2, 1:5, ,3]`
 - If we have more than one attribute we can call that by name in the first argument `stars_object[att,,,,]`
 
In this way we have a cursory method of manipulating the data, and can use this to squeeze out our first map:

```{r}

#make a simple palette using our website colours
my_pal <- c("#A7C3C7", "#7EA0A7", "#55807D", "#2D6056", "#00402F", "#00252A")

#create a simple map of the data
tm_shape(example_data["Chla",,,1]) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

Not bad, and just to be clear - if we don't select just 1 time step we would get several maps:

```{r}

#create a simple map of the data
tm_shape(example_data["Chla",,,1:4]) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

However trying to do all of our analysis and manipulation this way would be very painful. Thankfully stars objects work with most tidyverse functions.

When we use the tidyverse method, knowing the exact names of our dimensions and attributes is the key for layer manipulation rather than their specific order. For example, if we wanted to once again extract one time layer of data, it is as easy as specifying the dimension we want to slice ("time"), and the slice number:

```{r}

#slice to get a single time step
single_timestep <- example_data |> 
  slice(time, 1)

#slice to get multiple time steps
multi_timestep <- example_data |> 
  slice(time, 1:10)

```

```{r}

#create a simple plot of the data
tm_shape(single_timestep) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

Although one downside here is that if you want to slice on multiple dimensions the calls must be run separately:

```{r}

#slice by latitude and time, not the 
slice_of_lat_and_time <- example_data |> 
  slice(x, 1:30) |> 
  slice(time, 1)

```

```{r}

#visualise the slice of lat and time 
tm_shape(slice_of_lat_and_time) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

As we can seem using `slice()` just about covers all of our layer manipulation needs without much work... Layer aggregation is not going to be so easy...

:::{.callout-note}
There are a few other functions from the tidyverse that can be used such as `filter()`, `pull()`, `mutate()`, and `select()`, but we won't worry about those here, we will just focus on `slice()`.
:::

## Layer Aggregation

As we explored above we have quite a few time steps, too many to plot all of them. Our initial solution to be able to create a map was to simply slice out a few layers, but obviously this is not a good solution if we are trying to learn something about the entire dataset. Instead, a common method to deal with this kind of problem (too much data) is to aggregate the data into a workable size. 

There are two main ways to do this, the first method is to use `st_apply()` - this method is more general purpose and gives you greater control, it can apply all sorts of function and is not just limited to reducing dimensions. The second method is to use `aggregate()` - this method is easier to use, but has limits on what it can achieved. We will cover both as the more complicated method gives a very helpful conceptual grasp of the data.

### st_apply()

The `st_apply()` function has three main arguments we are going to focus on:

 1. X (the stars object),
 2. MARGIN, and
 3. FUN (the function to apply)
 
Arguments 1 and three are pretty self explanatory, but MARGIN is a bit more confusing so I have drawn up some diagrams to help the explanation. Lets first look at a conceptual diagram of our data.

![](image_1.png)

In this diagram we can see each of our dimensions represented (latitude, longitude, depth, and time), and our attribute would be the value in the cell. Also note that for this diagram we have included multiple depth layers, but our actual data only has the one depth at the moment.

What MARGIN does, is ask "where do you want to apply the function?" As in what dimension. The dimension that you supply is the dimension that is preserved. For our data there are four margins to choose from:

 1 = Latitude
 2 = Longitude
 3 = Depth
 4 = Time
 
If we say MARGIN = 1, we are applying our function over latitude, and the resulting data will **only retain the latitude dimension**. It would look like this:

![](image_2.png)
See how all of the cells that share the same latitude, but have different longitudes, times, or depths, are all combined into the same group.

If we say MARGIN = 2, we are applying our function over longitude, and the resulting data will **only retain the longitude dimension**. It would look like this:

![](image_3.png)

This time note that all cells that share the same longitude, but have different latitudes times, or depths, are all combined into the same group.

MARGIN = 3 (depth) would look like this:

![](image_4.png)

and MARGIN = 4 (time) like this:

![](image_5.png)

Reasonably straight forward so far, but also largely unhelpful - none of these outputs retain data that is viable to be mapped. This is where things get a bit more intense, because you can actually supply multiple dimensions to the MARGIN argument, which allows for the preservation of multiple dimensions. For example, if we wanted to learn how our attribute changed as it moved offshore and how it changed over time we could say MARGIN = c(2,4) which would look like this:

![](image_6.png)

See how both the time and the longitude dimensions are maintained, and only the latitude and depth dimensions are grouped up.

But probably the one we are most interested in is if we set MARGIN = c(1,2) (Latitude and Longitude) which would collapse the time and depth variables leaving us with one raster:

![](image_7.png)

Note this time the depth and time dimensions are aggregated.

One final thing to note with the MARGIN argument is that while it can take numeric inputs, it can also take the names of the dimensions. So instead of saying `MARGIN = c(1,2)` we could instead say `MARGIN = c("x","y")` to be a bit more clear about what we are doing.

In fact, this is what we will do right now. Note the dimensions of our dataset before the function is run:

```{r}

#look at dimensions
example_data

```

versus after:

```{r}

#take the mean over time
chl_a_mean <- st_apply(example_data, 
                       c("x'","y"),
                       mean)

#look at dimensions
chl_a_mean

```

As we explained above, only the latitude and longitude dimensions remain. What we did was apply the mean function to the data, where the data is grouped by latitude and longitude (collapsing depth and time) to form pools of data to get the mean from. There is then one mean value for each latitude * longitude pair and we are left with a map that looks like this:

```{r}

#create a simple plot of the data
tm_shape(chl_a_mean) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Congratulations, using this method we now have a way of aggregating our data - i.e. by getting the mean of all the data into a single spatial layer. But more importantly we now have a very good conceptual understanding of our data, and we also know how we would apply some really complicated functions across different dimensions. This is extremely useful when you move on to more in depth spatial analysis.

:::{.callout-note}
The `st_apply()` function is not limited to just the `mean()` function, or even just simple functions at all. It can take in any custom function that you write - provided it has been written to work with matrices. For example, you could run the `min()` function to get a map that shows the min value at each cell, or if your data has spectral bands you could write a function to calculate the NDVI value for a vegetation assessment. The possibilities are endless!
:::

### aggregate()

As noted earlier, the `aggregate()` function is a much simpler method for aggregating a stars object and returning data with a lower spatial or temporal resolution. This function works in a similar way, it also has three main arguments:

 1. X (the stars object),
 2. by, and
 3. FUN (the function to apply)
 4. ... (additional arguments such as na.rm = T)
 
Again, arguments 1 and 3 are self explanatory, but the second argument is not. The "by" argument takes either an sf object (a spatial object) to do spatial aggregation, or a vector of grouping values. The sf object is fairly simple, it acts similar to a mask - anything inside the object is part of the group, anything outside is not. The vector is a bit more flexible, it could be a vector of time values - for temporal aggregation, or it could be a vector of latitude values for spatial aggregation, or a vector of longitude values for spatial aggregation. What it **can't** be is more than one of those things, if you want a combination you must use the `st_apply()` method. To be fair, I cannot think of a single reason why you would want to supply a lat/long value for aggregation this way when st_apply is so much better, so we will effectively treat the "by" argument as either an sf object, or a time vector.

Lets first demonstrate this with a spatial object:

```{r}

#demonstrate the aggregate function with an sf object
agg_example <- aggregate(st_transform(example_data, "EPSG:7844"),
                         dt_region,
                         mean,
                         na.rm = T)

#create a simple plot of the data
tm_shape(agg_example[,,1]) +
  tm_polygons(fill = "Chla", 
              fill.scale = tm_scale_intervals(n = 6,
                                              values = my_pal,
                                              label.format = list(digits = 2)),
              fill.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Which is kind of interesting as we can see that there must be a slight overlap between land and marine for those land polygons to contain values. However, generally I find I don't use this method all that often - despite really wanting to find reasons too.

Of course, the other options is the temporal vector. What I have done here is; 

 1. Get a vector of all the time layers,
 2. Drop the time and day off each layer (keeping only month and year),
 3. Extracted the unique year-month values,
 3. Provided this as the vector - effectively grouping by month and year

```{r}

#create a vector of times for each layer
all_times <- st_get_dimension_values(example_data, "time")

#change these times to only contain the year and month information, then extract unique values
unique_months <- unique(floor_date(all_times, unit = "month"))

#this give us a vector the same length as the number of data layers. e.g. month information is repeated
agg_example_2 <- aggregate(example_data,
                           by = shifted_dates,
                           mean,
                           na.rm = T)

```

However due to weirdness inside the function before we can visualise the output we need to now fix the dimension values as they are out of order. Specifically, after the aggregation they are:

```{r}

#look at the dimensions of the object
dim(agg_example_2)

```

While we need them to be:

```{r}

#reorder dimensions
agg_example_2 <- aperm(agg_example_2, c("x", "y", "time"))

#look at dimensions
dim(agg_example_2)

```

Once reordered, we can then visualise just fine:

```{r}

#create a simple plot of the data
tm_shape(agg_example_2) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

And look at that, we now have 12 layers with monthly mean concentration values, cool!!


## Layer Aggregation In Practice

Now we know how to aggregate our data we need to aggregate our data in a useful way. Unfortunately the single layer we aggregated to above doesn't cut it. It returns an annual overview that doesn't really tell us too much other than what locations have consistently higher chlorophyll a. No, instead we want to learn something about seasonal or monthly trends. To do this we need to provide some kind of indication to `st_apply()` that we want multiple groups.

```{r}

time_table <- data.frame(DateTime = st_get_dimension_values(example_data, "time"))

time_table <- time_table |> 
  mutate(Year = year(DateTime),
         Month = month(DateTime),
         RowId = row_number()) |> 
  unite(LayerName, "Year", "Month", sep = "_")
  
  #group by LayerName and get the min and max row index (layer number) for each month, then reorder the dataframe by index
  time_table <- time_table |> 
    group_by(LayerName) |> 
    summarise(MinIndex = min(RowId),
              MaxIndex = max(RowId)) |> 
    arrange(MinIndex)
  


map_out_test <- map2(time_table$MinIndex, time_table$MaxIndex, function(a,b) {
  
  st_apply(example_data[,,,a:b],
           MARGIN = 1:2,
           FUN = mean,
           na.rm = T,
           keep = T)
       
})
  
#bind the output into a single stars object. Note there are two "c"s here. The first (left) one binds the args. The second (right one) provides the args (list of stars object) plus the final argument (along = "time") which tells the first c to bind along a new dimension.
full_out <- do.call(c, c(map_out_test, along = "time"))

```

```{r}

#create a simple plot of the data
tm_shape(full_out) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

## Maps

With the plots completed we can move on to creating the monthly mean maps for each of the indicators. First we need to calculate monthly means for each indicator, then stitch the layer back into groups:

```{r}
#| label: map eReefs data p1

#write a custom function that does each of the required steps to calculate the monthly values
monthly_mean_data <- map(data_list, function(dataset){
  
  #get the list of "times" (dates) stored in the dataset (1 date per layer) and convert it into a dataframe
  dates <- data.frame(DateTime = st_get_dimension_values(dataset, which = "time"))
  
  #extract the year and month and combine into a single unique name and add an index column
  dates <- dates |> 
    mutate(Year = year(DateTime),
           Month = month(DateTime, label = T),
           RowId = row_number()) |> 
    unite(LayerName, "Year", "Month", sep = "_")
  
  #group by LayerName and get the min and max row index (layer number) for each month, then reorder the dataframe by index
  dates <- dates |> 
    group_by(LayerName) |> 
    summarise(MinIndex = min(RowId),
              MaxIndex = max(RowId)) |> 
    arrange(MinIndex)
  
  #create a trackers for the objects that will be made
  created_objects <- c()
  
  #loop over each of the unique names
  for (i in unique(dates$LayerName)){
  
    #get the index numbers for the min and max layers from the table
    min_layer <- dates |> filter(LayerName == i) |> select(MinIndex) |> as.numeric()
    max_layer <- dates |> filter(LayerName == i) |> select(MaxIndex) |> as.numeric()
    
    #extract the month of data using the index numbers 
    if (str_detect(names(dataset), "Secchi")){# if secchi: [attribute, i (lat), j (long), time] - no depth
      
      monthly_mean <- dataset[,,,min_layer:max_layer]
    
    } else {#otherwise [attribute, i (lat), j (long), k (depth), time]
      
      monthly_mean <- dataset[,,,,min_layer:max_layer]
    }
    
    #and apply the mean function over the i,j dimensions (lat, long)
    monthly_mean <- st_apply(monthly_mean, 1:2, FUN = mean, keep = TRUE)
    
    #assign the monthly_mean dataset to a new object
    assign(glue("{i}"), monthly_mean)
    
    #create a vector that tracks all the names of the created objects
    created_objects <- c(created_objects, glue("{i}"))
  
  }
  
  #combine each of the monthly layers into one new dataset ("mget" is "multiple get") and this will be the output of the function
  dataset <- do.call(c, mget(created_objects))
  
})

```

Then we convert these stacks of monthly means into sf objects:

```{r}
#| label: map eReefs data p2

monthly_mean_df <- map(monthly_mean_data, function(dataset){
  
  #extract the datasets into tables
  dataset_df <- st_as_sf(dataset, as_points = F, merge = F)
  
  #pivot the tables longer to make them easier to work with, split the layer information into Financial Year and month, and order by Month
  dataset_df <- dataset_df |> 
    pivot_longer(cols = matches("\\d"), names_to = "Layer", values_to = "Value") |> 
    separate(Layer, into = c("FinancialYear", "Month"), sep = "_", remove = F) |> 
    mutate(Month = as.factor(Month),
           Month = fct_relevel(Month, "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", 
                               "Mar", "Apr", "May", "Jun"))
  
  #return the object
  return(dataset_df)
  
})


```

Next we set up the arguments that are going to vary for each map and each region.

```{r}
#| label: map eReefs data p3

#create a list of arguments that we will use to inject the correct arguments per region
args_per_region <- list("Position" = list("Dry Tropics" = c(0.01, 0.99),
                                          "Wet Tropics" = c(0.77, 0.99),
                                          "Mackay Whitsunday Isaac" = c(0.79, 0.99)),
                        "FacetDim" = list("Dry Tropics" = 4,
                                          "Wet Tropics" = 6,
                                          "Mackay Whitsunday Isaac" = 4))

#create a list of palettes to switch between for each indicator
indicator_palettes <- c("brewer.greens", "-brewer.oranges", "-carto.ag_sunset", "brewer.blues")

#create a function that will inject the correct arguments per indicator
args_per_indicator <- function(x){

  if (str_detect(names(monthly_mean_df)[[x]], "Turb|Nitri")){#if indicator is Turbidity or nitrites use a log scale
    
    tm_scale_continuous_log(values = indicator_palettes[x])
    
  } else {#otherwise, just use a linear scale
    
    tm_scale_continuous(values = indicator_palettes[x])
    
  }
  
}

#create an function that produces the correct title
map_name <- function(reg, x){
  
  #get the indicator currently being used
  ind <- names(monthly_mean_df)[[x]]
  
  #detect if it should be a log scale or not
  log <- if(str_detect(ind, "Turb|Nitri")){"(Log Scale)"} else {"(Linear Scale)"}
  
  #glue everything together to make the correct title
  glue("{reg}, {ind}: {log}")
  
}

```

Facet maps of the monthly values can then be made:

```{r}
#| label: map eReefs data p4

#mask reefs to the region we are currently looking at
target_reefs <- st_intersection(reefs, st_union(target_marine_region))

#for each sf dataframe in our list
for (i in 1:length(monthly_mean_df)){
  
  #create the map
  map <- tm_shape(qld) +
    tm_polygons(fill = "#99B5B1",
                col = "#7bba9d") +
    tm_shape(target_land_region) +
    tm_polygons(fill = "grey90", 
                col = "black") +
    tm_shape(monthly_mean_df[[i]]) +
    tm_polygons(fill = "Value", 
                fill.scale = args_per_indicator(i),
                fill.legend = tm_legend(reverse = TRUE,
                                        title = "", 
                                        width = 2,
                                        height = 5,
                                        item.width = 0.2,
                                        na.show = FALSE,
                                        position = args_per_region[["Position"]][[proj_region]]),
                                        
                fill.free = T,
                col_alpha = 0) +
    tm_facets(by = "Month", 
              ncol = args_per_region[["FacetDim"]][[proj_region]]) +
    tm_shape(reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "grey60",
               col_alpha = 0.4) +
    tm_shape(target_reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "black",
               col_alpha = 0.5) +
    tm_shape(target_marine_region) +
    tm_borders(col = "black") +
    tm_shape(st_buffer(target_marine_region, 0.5), is.main = T) +
    tm_borders(col = NULL,
               fill = NULL) +
    tm_layout(bg.color = "#C1DEEA") +
    tm_title(text = map_name(proj_region, i),
             size = 0.8)
  
  #extract a clean name for saving
  indicator_name <- str_extract(names(data_list)[i], "^[^ ]+")

  #and save
  tmap_save(map, glue("{output_path}/plots/{region_lower}_{indicator_name}_map.png"))
  
}

```

Following this, we will make the same facet map except we will use a fixed scale for the maps.

```{r}
#| label: map eReefs data p5

#mask reefs to the region we are currently looking at
target_reefs <- st_intersection(reefs, st_union(target_marine_region))

#for each sf dataframe in our list
for (i in 1:length(monthly_mean_df)){
  
  #create the map
  map <- tm_shape(qld) +
    tm_polygons(fill = "#99B5B1",
                col = "#7bba9d") +
    tm_shape(target_land_region) +
    tm_polygons(fill = "grey90", 
                col = "black") +
    tm_shape(monthly_mean_df[[i]]) +
    tm_polygons(fill = "Value", 
                fill.scale = if (i %in% c(3,4)){tm_scale_continuous_log(values = indicator_palettes[i])} #if Turbidity or nitrites use log scale
                             else {tm_scale_continuous(values = indicator_palettes[i])}, #otherwise use linear scale
                fill.legend = tm_legend(reverse = TRUE,
                                        title = if (i == 3){glue("{names(data_list)[i]} (Log)")} #if log, write log
                                                else {names(data_list)[i]}, #otherwise just write the indicator
                                        na.show = FALSE), 
                col_alpha = 0) +
    tm_facets(by = "Month", ncol = if(proj_region == "Wet Tropics"){6}else{4}) +
    tm_shape(reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "grey60",
               col_alpha = 0.4) +
    tm_shape(target_reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "black",
               col_alpha = 0.5) +
    tm_shape(target_marine_region) +
    tm_borders(col = "black") +
    tm_shape(st_buffer(target_marine_region, 0.5), is.main = T) +
    tm_borders(col = NULL,
               fill = NULL) +
    tm_layout(bg.color = "#C1DEEA",
              legend.position = tm_pos_out("right", "center"))
  
  #extract a clean name for saving
  indicator_name <- str_extract(names(data_list)[i], "^[^ ]+")
  #and save
  tmap_save(map, glue("{output_path}/plots/{region_lower}_{indicator_name}_map_fixed_scale.png"))
  
}

```

## Tables

Finally, we will create a table summarizing the key statistics we have found throughout this analysis. These tables will contain:

 - monthly mean values
 - annual mean values

```{r}
#| label: create summary table

#create a folder to store all the data outputs
dir.create(glue("{output_path}/tables/"))

#read in the custom function to style tables
source(here("functions/cond_form_tables.R"))

#extract monthly and annual means
summary_table <- raw_data_df |> 
  group_by(Indicator) |> 
  mutate(AnnualMeanValue = mean(Values, na.rm = T)) |> 
  group_by(Indicator, Month, AnnualMeanValue) |> 
  summarise(MonthlyMeanValue = mean(Values, na.rm = T)) |> 
  ungroup() |> 
  mutate(Month = month(Month, label = T),
         Month = fct_relevel(Month, "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun")) |> 
  arrange(Month)

#pivot data wider into a reader friendly format
summary_table_wide <- summary_table |> 
  pivot_wider(names_from = Month, values_from = MonthlyMeanValue) |> 
  relocate("AnnualMeanValue", .after = last_col()) |> 
  mutate(across(where(is.numeric), ~round(.x, 2)))

#save the table
write_csv(summary_table_wide, glue("{output_path}/tables/{region_lower}_summary_table.csv"))

#print the table in a visually appealing way
cond_form_tables(summary_table_wide, header_rows = 1, landscape = F, score_colour = F) |> 
  set_align(everywhere, everywhere, "center") |> 
  set_valign(everywhere, everywhere, "middle")

```

















