---
title: "Creating Beautiful Maps Using eReefs Data"
date: "05/23/2025"
abstract-title: "ABSTRACT"
abstract: "In this blog I demonstrate how you can make beautiful maps in R using example data extracted from the eReefs platform. You can also follow along with any of your own data."
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

library(sf)
library(stars)
library(tmap)
#library(ggplot2)
library(tidyverse)
library(ereefs)
#library(scales)

sf_use_s2(FALSE)

```

# Introduction

This is part three of a series of blog that focus on eReefs and the data it provides. To follow along with this blog you will need to be able to download data from eReefs, you can learn how to do that in my first blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd). I would also recommend that you check my blog about [Plotting eReefs Data](../ereefs_plotting_data/index.qmd), however it is not essential reading for this post.

In this post I would like to explore the spatial aspect of eReefs data. Together we will learn some of the most important steps when working with this type of data as we:

 1. Manipulate and transform spatial data,
 2. Understand how to visualise the data, and
 3. Build a series of informative maps

# Read in Data

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

#if our csv does not exists, create it
if(!file.exists("example_data.nc")){

  #read in the dry tropics region dataset and start with a lat/long crs
  dt_region <- st_read("dt_region.gpkg") |> 
    st_transform("EPSG:7844")
  
  #manually assign the url from above into a variable (so I don't have to interact with the code)
  input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
      
  #get all grids
  grids <- get_ereefs_grids(input_file)
  
  #use the bbox function to get the boundaries of our targeted location
  target_bounds <- st_bbox(dt_region)
  
  #if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
  true_false_array <- grids[["x_grid"]] >= target_bounds[1] & 
    grids[["x_grid"]] <= target_bounds[3] & 
    grids[["y_grid"]] >= target_bounds[2] & 
    grids[["y_grid"]] <= target_bounds[4]
    
  #if the value is NA, change it to false.
  true_false_array[is.na(true_false_array)] <- FALSE
  
  #return the row index for every row that contains at least one true value:
  true_rows <- which(apply(true_false_array, 1, any))
  
  #find the first row that contains a true value
  first_row <- true_rows[1]
  
  #find the number of rows that contains a true value
  num_of_rows <- tail(true_rows, n = 1) - first_row
  
  #return the row index for every row that contains at least one true value:
  true_cols <- which(apply(true_false_array, 2, any))
  
  #find the first col that contains a true value
  first_col <- true_cols[1]
  
  #find the number of cols that contains a true value
  num_of_cols <- tail(true_cols, n = 1) - first_col
  
  #set our dimensions (44 is the surface depth, 100 is the 100th time step)
  our_dimensions_request <- cbind(start = c(first_row, first_col, 44, 216),
                                  count = c(num_of_rows, num_of_cols, 1, 365))
  
  #extract the data
  extracted_data <- read_ncdf(input_file, 
                              var = "Chl_a_sum", 
                              ncsub = our_dimensions_request)
  
  #update the crs
  extracted_data <- st_transform(extracted_data, "EPSG:7855")
  
  #change all land values to NA
  extracted_data[(extracted_data > 1000)] <- NA
  
  #convert our curvilinear object into just a bbox then update the crs on the bbox
  curvilinear_bbox <- extracted_data |> 
    st_bbox() |>
    st_as_sfc()
  
  #get a linear grid target with the same dimensions (number of cells) as our curvilinear grid 
  reg_stars <- st_as_stars(curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., 
                           nx = dim(extracted_data)[[1]], #and the dimensions to provide the x and y count. 
                           ny = dim(extracted_data)[[2]],
                           values = NA_real_) #Fill each cell with NA
  
  #run st warp, it requires a curvilinear object, and a regular object as a target
  warped_data <- st_warp(extracted_data, reg_stars)
  
  #update the crs
  dt_region <- st_transform(dt_region, "EPSG:7855")
  
  #crop to the actual area of interest
  reg_grid_cropped_data <- warped_data |> 
    st_crop(dt_region)
  
  #drop dimensions with only 1 value
  reg_grid_cropped_data <- reg_grid_cropped_data[drop = TRUE]
  
  #save the data 
  write_stars(reg_grid_cropped_data, "example_data.nc", driver = "netCDF")
  
  #get vector of time values and save them for later
  saveRDS(st_get_dimension_values(reg_grid_cropped_data, "time"), file = "time_vector.rds")
  
}

```

Thanks to our previous post about extracting the data, it is saved in our file system ready to open no stress, and we know exactly the preprocessing that we need to do:

```{r}
#| output: FALSE

#load in the example data
example_data <- read_stars("example_data.nc")

#load vector of time values
time_vals <- readRDS("time_vector.rds")

#merge "attributes" (time) back together
example_data <- merge(example_data)

#update time dimension values and names
example_data <- example_data |> 
  st_set_dimensions(3, time_vals,
                    names = c("x", "y", "time"))
  
#then update the attribute name
example_data <- setNames(example_data, "Chla")

#read in the dry tropics region dataset that we will use for spatial context
dt_region <- st_read("dt_region.gpkg") |> 
  st_transform("EPSG:7844")

```

# Analyse Data

There are few goals I have for analysing the data, I would like to be able to:

 1. Manipulate data using the time dimension, i.e. extract certain dates
 2. Manipulate data using spatial dimensions, i.e. extract certain areas
 3. Aggregate data using the time dimension, e.g. getting monthly average values
 4. Aggregate data using spatial dimensions, .e.g. getting average values per area
 
## Layer Manipulation

Before we can get right into analysis or mapping we need to get a better understanding of the data structure and what we are looking at. Simply by calling the object we can already get a pretty good breakdown:

```{r}

#get a summary of the data
example_data

```

A couple of things to point out here.

 1. The object is a "stars object", this is a class introduced by the `stars` package
    1.1 stars objects hold attribute(s) and dimensions
        1.1.1 attributes are our values (e.g. chlorophyll a), and there can be more than one
        1.1.2 dimensions are our lat, long, depth, time, spectral band, etc.
    1.2 This means stars objects can be n-dimensional **and** hold multiple attributes - which is a lot to think about
 2. We can see the summary statistics for our attribute (chlorophyll a)
 3. We can also see some information about our dimensions
 
To see the names of our dimensions we can use `dim()`.

```{r}

#view the dimension names and lengths
dim(example_data)

```

and to see the names of our attributes we can use `names()`.

```{r}

#view the names of the attributes
names(example_data)

```

When we look to analyse our data we are going to have to think about all of our dimensions and any attributes. The simplest way to interact with each of these is using the `[]` square brackets. The first element in the brackets corresponds to the attributes, and each element following this is one of the dimensions (in the order in which you see them using `dim()`): `stars_object[att, i, j, k, time]`.

 - If we wanted to get just the first time step, we would write `stars_object[], , , , 1]` where the blank entry just means give me all of it.
 - If we wanted the 2nd i, the 1st-5th j, and the 3rd time step, we would write `stars_object[, 2, 1:5, ,3]`
 - If we have more than one attribute we can call that by name in the first argument `stars_object[att,,,,]`
 
In this way we have a cursory method of manipulating the data, and can use this to squeeze out our first map:

```{r}

#make a simple palette using our website colours
my_pal <- c("#A7C3C7", "#7EA0A7", "#55807D", "#2D6056", "#00402F", "#00252A")

#create a simple map of the data
tm_shape(example_data["Chla",,,1]) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

Not bad, and just to be clear - if we don't select just 1 time step we would get several maps:

```{r}

#create a simple map of the data
tm_shape(example_data["Chla",,,1:4]) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

However trying to do all of our analysis and manipulation this way would be very painful. Thankfully stars objects work with most tidyverse functions.

When we use the tidyverse method, knowing the exact names of our dimensions and attributes is the key for layer manipulation rather than their specific order. For example, if we wanted to once again extract one time layer of data, it is as easy as specifying the dimension we want to slice ("time"), and the slice number:

```{r}

#slice to get a single time step
single_timestep <- example_data |> 
  slice(time, 1)

#slice to get multiple time steps
multi_timestep <- example_data |> 
  slice(time, 1:10)

```

```{r}

#create a simple plot of the data
tm_shape(single_timestep) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

Although one downside here is that if you want to slice on multiple dimensions the calls must be run separately:

```{r}

#slice by latitude and time, not the 
slice_of_lat_and_time <- example_data |> 
  slice(x, 1:30) |> 
  slice(time, 1)

```

```{r}

#visualise the slice of lat and time 
tm_shape(slice_of_lat_and_time) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region, is.main = T) +
  tm_polygons(fill = NULL,
              col = "black")

```

As we can seem using `slice()` just about covers all of our layer manipulation needs without much work... Layer aggregation is not going to be so easy...

:::{.callout-note}
There are a few other functions from the tidyverse that can be used such as `filter()`, `pull()`, `mutate()`, and `select()`, but we won't worry about those here, we will just focus on `slice()`.
:::

## Layer Aggregation

As we explored above we have quite a few time steps, too many to plot all of them. Our initial solution to be able to create a map was to simply slice out a few layers, but obviously this is not a good solution if we are trying to learn something about the entire dataset. Instead, a common method to deal with this kind of problem (too much data) is to aggregate the data into a workable size. 

There are two main ways to do this, the first method is to use `st_apply()` - this method is more general purpose and gives you greater control, it can apply all sorts of function and is not just limited to reducing dimensions. The second method is to use `aggregate()` - this method is easier to use, but has limits on what it can achieved. We will cover both as the more complicated method gives a very helpful conceptual grasp of the data.

### st_apply()

The `st_apply()` function has three main arguments we are going to focus on:

 1. X (the stars object),
 2. MARGIN, and
 3. FUN (the function to apply)
 
Arguments 1 and three are pretty self explanatory, but MARGIN is a bit more confusing so I have drawn up some diagrams to help the explanation. Lets first look at a conceptual diagram of our data.

![](image_1.png)

In this diagram we can see each of our dimensions represented (latitude, longitude, depth, and time), and our attribute would be the value in the cell. Also note that for this diagram we have included multiple depth layers, but our actual data only has the one depth at the moment.

What MARGIN does, is ask "where do you want to apply the function?" As in what dimension. The dimension that you supply is the dimension that is preserved. For our data there are four margins to choose from:

 1 = Latitude
 2 = Longitude
 3 = Depth
 4 = Time
 
If we say MARGIN = 1, we are applying our function over latitude, and the resulting data will **only retain the latitude dimension**. It would look like this:

![](image_2.png)
See how all of the cells that share the same latitude, but have different longitudes, times, or depths, are all combined into the same group.

If we say MARGIN = 2, we are applying our function over longitude, and the resulting data will **only retain the longitude dimension**. It would look like this:

![](image_3.png)

This time note that all cells that share the same longitude, but have different latitudes times, or depths, are all combined into the same group.

MARGIN = 3 (depth) would look like this:

![](image_4.png)

and MARGIN = 4 (time) like this:

![](image_5.png)

Reasonably straight forward so far, but also largely unhelpful - none of these outputs retain data that is viable to be mapped. This is where things get a bit more intense, because you can actually supply multiple dimensions to the MARGIN argument, which allows for the preservation of multiple dimensions. For example, if we wanted to learn how our attribute changed as it moved offshore and how it changed over time we could say MARGIN = c(2,4) which would look like this:

![](image_6.png)

See how both the time and the longitude dimensions are maintained, and only the latitude and depth dimensions are grouped up.

But probably the one we are most interested in is if we set MARGIN = c(1,2) (Latitude and Longitude) which would collapse the time and depth variables leaving us with one raster:

![](image_7.png)

Note this time the depth and time dimensions are aggregated.

One final thing to note with the MARGIN argument is that while it can take numeric inputs, it can also take the names of the dimensions. So instead of saying `MARGIN = c(1,2)` we could instead say `MARGIN = c("x","y")` to be a bit more clear about what we are doing.

In fact, this is what we will do right now. Note the dimensions of our dataset before the function is run:

```{r}

#look at dimensions
example_data

```

versus after:

```{r}

#take the mean over time
chl_a_mean <- st_apply(example_data, 
                       c("x","y"),
                       mean)

#look at dimensions
chl_a_mean

```

As we explained above, only the latitude and longitude dimensions remain. What we did was apply the mean function to the data, where the data is grouped by latitude and longitude (collapsing depth and time) to form pools of data to get the mean from. There is then one mean value for each latitude * longitude pair and we are left with a map that looks like this:

```{r}

#create a simple plot of the data
tm_shape(chl_a_mean) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Congratulations, using this method we now have a way of aggregating our data - i.e. by getting the mean of all the data into a single spatial layer. But more importantly we now have a very good conceptual understanding of our data, and we also know how we would apply some really complicated functions across different dimensions. This is extremely useful when you move on to more in depth spatial analysis.

Unfortunately the single layer we aggregated to above doesn't cut it. It returns an annual overview that doesn't really tell us too much other than what locations have consistently higher chlorophyll a. No, instead we want to learn something about seasonal or monthly trends. To do this we need to provide some kind of indication to `st_apply()` that we want multiple groups.

This is achieved using the following steps:

 1. Extract a table that contains the date and time of each layer
 2. Group the individual layers by month and find the first and last layer per month:
 
```{r}

#extract a table that contains the date and time of each layer
time_table <- data.frame(DateTime = st_get_dimension_values(example_data, "time"))

#extract the year and month into their own columns, add a column that counts the row number
time_table <- time_table |> 
  mutate(Year = year(DateTime),
         Month = month(DateTime),
         RowId = row_number()) 

#combine the year and month columns
time_table <- time_table |> 
  unite(YearMonth, "Year", "Month", sep = "_")
  
#group by the YearMonth column and get the min and max row index (layer number) for each month, order by index number
time_table <- time_table |> 
    group_by(YearMonth) |> 
    summarise(MinIndex = min(RowId),
              MaxIndex = max(RowId)) |> 
    arrange(MinIndex)

#visualise the data
head(time_table)

``` 
 
 3. Use `slice()` to extract all the layers per month
 4. Use `st_apply()` to apply the mean function to all the layers in the month
 5. Put this inside a `map2()` function to run the code for each month at the same time:

```{r}

#use map to work through each start-end index and use st_apply to apply the mean
monthly_chla <- map2(time_table$MinIndex, time_table$MaxIndex, function(a,b) {
  
  #apply mean to the data slice
  st_apply(slice(example_data, time, a:b),
           MARGIN = c("x","y"), #using margin x and y to keep lat and long information
           FUN = mean,
           na.rm = T,
           keep = T)
       
})

```

 6. Combine the list output back into a single stars object:

```{r}

#bind the output into a single stars object. Note there are two "c"s here. The first (left) one binds the args. The second (right one) provides the args (list of stars object) plus the final argument (along = "time") which tells the first c to bind along a new dimension.
monthly_chla <- do.call(c, c(monthly_chla, along = "time"))

```

Done! We can then visualise the data to confirm it worked:

```{r}

#create a simple plot of the data
tm_shape(monthly_chla) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Seems good to me! Although we would likely have to fix up those layer names/dates.

:::{.callout-note}
The `st_apply()` function is not limited to just the `mean()` function, or even just simple functions at all. It can take in any custom function that you write - provided it has been written to work with matrices. For example, you could run the `min()` function to get a map that shows the min value at each cell, or if your data has spectral bands you could write a function to calculate the NDVI value for a vegetation assessment. The possibilities are endless!
:::

### aggregate()

As noted earlier, the `aggregate()` function is a much simpler method for aggregating a stars object and returning data with a lower spatial or temporal resolution. This function works in a similar way, it also has three main arguments:

 1. X (the stars object),
 2. by, and
 3. FUN (the function to apply)
 4. ... (additional arguments such as na.rm = T)
 
Again, arguments 1 and 3 are self explanatory, but the second argument is not. The "by" argument takes either an sf object (a spatial object) to do spatial aggregation, or a vector of grouping values. The sf object is fairly simple, it acts similar to a mask - anything inside the object is part of the group, anything outside is not. The vector is a bit more flexible, it could be a vector of time values - for temporal aggregation, or it could be a vector of latitude values for spatial aggregation, or a vector of longitude values for spatial aggregation. What it **can't** be is more than one of those things, if you want a combination you must use the `st_apply()` method. To be fair, I cannot think of a single reason why you would want to supply a lat/long value for aggregation this way when st_apply is so much better, so we will effectively treat the "by" argument as either an sf object, or a time vector.

Lets first demonstrate this with a spatial object:

```{r}

#demonstrate the aggregate function with an sf object
agg_example <- aggregate(st_transform(example_data, "EPSG:7844"),
                         dt_region,
                         mean,
                         na.rm = T)

#create a simple plot of the data
tm_shape(agg_example[,,1]) +
  tm_polygons(fill = "Chla", 
              fill.scale = tm_scale_intervals(n = 6,
                                              values = my_pal,
                                              label.format = list(digits = 2)),
              fill.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

Which is kind of interesting as we can see that there must be a slight overlap between land and marine for those land polygons to contain values. However, generally I find I don't use this method all that often - despite really wanting to find reasons too.

Of course, the other options is the temporal vector. This actually has some handy short cuts where you can supply a vector of time values, or just a simple string like "months", or "5-days", etc. For our purposes we will use the string "months" which seems to work just fine:

```{r}

#this aggregates data by month
agg_example_2 <- aggregate(example_data,
                           by = "months",
                           mean)

```

However due to weirdness inside the function before we can visualise the output we need to now fix the dimension values as they are out of order. Specifically, after the aggregation they are:

```{r}

#look at the dimensions of the object
dim(agg_example_2)

```

While we need them to be:

```{r}

#reorder dimensions
agg_example_2 <- aperm(agg_example_2, c("x", "y", "time"))

#look at dimensions
dim(agg_example_2)

```

Once reordered, we can then visualise just fine:

```{r}

#create a simple plot of the data
tm_shape(agg_example_2) +
  tm_raster(col.scale = tm_scale_intervals(n = 6,
                                           values = my_pal,
                                           label.format = list(digits = 2)),
            col.legend = tm_legend(reverse = T)) +
  tm_shape(dt_region) +
  tm_polygons(fill = NULL,
              col = "black")

```

And look at that, we now have 12 layers with monthly mean concentration values, which much less effort than `st_apply()`, cool! However do note that we also have much less control over this method.

