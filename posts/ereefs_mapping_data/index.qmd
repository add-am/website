---
title: "Creating Beautiful Maps Using eReefs Data"
date: "05/23/2025"
abstract-title: "ABSTRACT"
abstract: "In this blog I demonstrate how you can make beautiful maps in R using example data extracted from the eReefs platform. You can also follow along with any of your own data."
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

#library(sf)
library(stars)
library(tmap)
#library(ggplot2)
library(tidyverse)
#library(scales)

```

# Introduction

This is part three of a series of blog that focus on eReefs and the data it provides. To follow along with this blog you will need to be able to download data from eReefs, you can learn how to do that in my first blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd). I would also recommend that you check my blog about [Plotting eReefs Data](../ereefs_plotting_data/index.qmd), however it is not essential reading for this post.

In this post I would like to explore the spatial aspect of eReefs data. Together we will learn some of the most important steps when working with this type of data as we:

 1. Manipulate and transform spatial data,
 2. Understand how to visualise the data, and
 3. Build a series of informative maps


# Read in Data


```{r}

#NEED TO SILENTLY RETRIEVE DATA FROM EREEFS AND SAVE IT AS AN RDATA OBJECT IN THE FILE DOES NOT EXISTS

#DONT FORGET TO REMOVE STUPID HIGH VALUES

sf_use_s2(FALSE)

#read in the dry tropics region dataset
dt_region <- st_read("dt_region.gpkg") |> 
  st_transform("EPSG:7844")# |> 
  #st_union() |> 
  #nngeo::st_remove_holes() |> 
  #st_union() |> 
  #st_make_valid()



```


Thanks to our previous posts we have data ready to go.

```{r}

#load in the data
load("chl_a_sum_dry_tropics.RData")

```

# Explore Data

Before we can get right into analysis or mapping we need to get a better understanding of the data structure and what we are looking at. Simply by calling the object we can already get a pretty good breakdown:

```{r}

#get a summary of the data
chl_a_sum_dry_tropics

```

A couple of things to point out here.

 1. The object is a "stars object", this is a class introduced by the `stars` package
    1.1 stars objects hold attribute(s) and dimensions
        1.1.1 attributes are our values (e.g. chlorophyll a), and there can be more than one
        1.1.2 dimensions are our lat, long, depth, time, spectral band, etc.
    1.2 This means stars objects can be n-dimensional **and** hold multiple attributes - which is a lot to think about
 2. We can see the summary statistics for our attribute (chlorophyll a)
 3. We can also see some information about our dimensions
 
To see the names of our dimensions we can use `dim()`.

```{r}

#view the dimension names and lengths
dim(chl_a_sum_dry_tropics)

```

and to see the names of our attributes we can use `names()`.

```{r}

#view the names of the attributes
names(chl_a_sum_dry_tropics)

```

When we look to analyse our data we are going to have to think about all of our dimensions and any attributes. The simplest way to interact with each of these is using the `[]` square brackets. The first element in the brackets corresponds to the attributes, and each element following this is one of the dimensions (in the order in which you see them using `dim()`): `stars_object[att, i, j, k, time]`.

 - If we wanted to get just the first time step, we would write `stars_object[], , , , 1]` where the blank entry just means give me all of it.
 - If we wanted the 2nd i, the 1st-5th j, and the 3rd time step, we would write `stars_object[, 2, 1:5, ,3]`
 - If we have more than one attribute we can call that by name in the first argument `stars_object[att,,,,]`
 
In this way we have a cursory method of manipulating the data, and can use this to squeeze out our first map:

```{r}

#create a simple map
tm_shape(chl_a_sum_dry_tropics["Chl_a_sum",,,,1]) +
  tm_raster(col.legend = tm_legend(reverse = T))

```

Not bad, and just to be clear - if we don't select just 1 time step we would get several maps:

```{r}

#create a map with several facets
tm_shape(chl_a_sum_dry_tropics["Chl_a_sum",,,,1:4]) +
  tm_raster(col.legend = tm_legend(reverse = T))

```

However trying to do all of our analysis and manipulation this way would be very painful. Thankfully stars objects work with most tidyverse functions.

# Analyse Data

When we use the tidyverse method, knowing the exact names of our dimensions and attributes is key rather than their specific order. For example, if we wanted to once again extract one time layer of data, it is as easy as specifying the dimension we want to slice, and the slice number:

```{r}

#slice to get a single time step
single_timestep <- chl_a_sum_dry_tropics |> 
  slice(time, 1)

#slice to get multiple time steps
multi_timestep <- chl_a_sum_dry_tropics |> 
  slice(time, 1:10)

```

```{r}

#visualise the single time slice
tm_shape(single_timestep) +
  tm_raster(col.legend = tm_legend(reverse = T))

```

However one downside here is that if you want to slice on multiple dimensions the calls must be run separately:

```{r}

#slice by latitude and time, not the 
slice_of_lat_and_time <- chl_a_sum_dry_tropics |> 
  slice(j, 1:30) |> 
  slice(time, 1)

```

```{r}

#visualise the slice of lat and time 
tm_shape(slice_of_lat_and_time) +
  tm_raster(col.legend = tm_legend(reverse = T))

```

There are a few other functions from the tidyverse that can be used such as `filter()`, `pull()`, `mutate()`, and `select()`, but we won't worry about those here, we will just focus on `slice()`.

## Aggregation Learning

As we explored above we have quite a few time steps, too many to plot all of them. Our initial solution to be able to create a map was to simply slice out a few layers, but obviously this is not a good solution if we are trying to learn something about the entire dataset. Instead, a common method to deal with this kind of problem (too much data) is to aggregate the data into a workable size. 

There are two main ways to do this, the first method is to use `st_apply()` - this method is more general purpose and gives you greater control, it can apply all sorts of function and is not just limited to reducing dimensions. The second method is to use `aggregate()` - this method is easier to use, but has limits on what it can achieved. We will cover both as the more complicated method gives a very helpful conceptual grasp of the data.

### st_apply()

The `st_apply()` function has three main arguments we are going to focus on:

 1. X (the stars object),
 2. MARGIN, and
 3. FUN (the function to apply)
 
Arguments 1 and three are pretty self explanatory, but MARGIN is a bit more confusing so I have drawn up some diagrams to help the explanation. Lets first look at a conceptual diagram of our data.

![](image_1.png)

In this diagram we can see each of our dimensions represented (latitude, longitude, depth, and time), and our attribute would be the value in the cell. Also note that for this diagram we have included multiple depth layers, but our actual data only has the one depth at the moment.

What MARGIN does, is ask "where do you want to apply the function?" As in what dimension. The dimension that you supply is the dimension that is preserved. For our data there are four margins to choose from:

 1. Latitude
 2. Longitude
 3. Depth
 4. Time
 
If we say MARGIN = 1, we are applying our function over latitude, and the resulting data will only retain the latitude dimension. It would look like this:

![](image_2.png)

If we say MARGIN = 2, we are applying our function over longitude, and the resulting data will only retain the longitude dimension. It would look like this:

![](image_3.png)

MARGIN = 3 (depth) would look like this:

![](image_4.png)

and MARGIN = 4 (time) like this:

![](image_5.png)

Reasonably straight forward so far, but also largely unhelpful - none of these outputs retain data that is viable to be mapped. This is where things get a bit more intense, because you can actually supply multiple dimensions to the MARGIN argument, which allows for the preservation of multiple dimensions. For example, if we wanted to learn how our attribute changed as it moved offshore and how it changed over time we could say MARGIN = c(2,4) which would look like this:

![](image_6.png)

But probably the one we are most interested in is if we set MARGIN = c(1,2) (Latitude and Longitude) which would collapse the time and depth variables leaving us with one raster:

![](image_7.png)

In fact, this is what we will do right now. Note the dimensions of our dataset before the function is run:

```{r}

chl_a_sum_dry_tropics

```

versus after:

```{r}

chl_a_mean <- st_apply(chl_a_sum_dry_tropics, 
                       c(1,2),
                       mean)

chl_a_mean

```

As we explained above, only the latitude and longitude dimensions remain. What we did was apply the mean function to the data, where the data is grouped by latitude and longitude (collapsing depth and time) to form pools of data to get the mean from. There is then one mean value for each latitude * longitude pair and we are left with a map that looks like this:

```{r}

tm_shape(chl_a_mean) +
  tm_raster(col.legend = tm_legend(reverse = T))

```

Congratulations, using this method we now have a way of aggregating our data - i.e. by getting the mean of all the data into a single spatial layer. But more importantly we now have a very good conceptual understanding of our data, and we also know how we would apply some really complicated functions across different dimensions.

:::{.callout-note}
The `st_apply()` function is not limited to just the `mean()` function, or even just simple functions at all. It can take in any custom function that you write - provided it has been written to work with matrices. For example, you could run the `min()` function to get a map that shows the min value at each cell, or if your data has spectral bands you could write a function to calculate the NDVI value for a vegetation assessment.
:::

### aggregate()

As noted earlier, the `aggregate()` function is a much simpler method for aggregating a stars object and returning data with a lower spatial or temporal resolution. This function works in a similar way, it also has three main arguments:

 1. X (the stars object),
 2. by, and
 3. FUN (the function to apply)
 
Again, arguments 1 and 3 are self explanatory, but the second argument is not. The "by" argument takes either an sf object (a spatial object) to do spatial aggregation, or a vector with time values to do temporal aggregation. Note that in this case we have less control over the exact dimension we want to aggregate (e.g. we can specify only longitude), but we also can't combine the space and time dimensions like we did above.

Lets first demonstrate this with a spatial object:

```{r}

test <- aggregate(st_transform(chl_a_sum_dry_tropics, "EPSG:7844"),
                  dt_region,
                  mean)

tm_shape(test[,,,1:3]) + tm_polygons(fill = "Chl_a_sum")

```

Before following up with a temporal vector:

```{r}

#create a vector of times for each layer
all_times <- st_get_dimension_values(chl_a_sum_dry_tropics, "time")

#change these times to only contain the year and month information, then extract unique values
unique_months <- unique(floor_date(all_times, unit = "month"))

#this give us a vector the same length as the number of data layers. e.g. month information is repeated
test2 <- aggregate(chl_a_sum_dry_tropics,
                   by = unique_months,
                   mean)

```

However due to weird bugs we have to use the base R mapping tools to visualise the output of this:

```{r}

plot(test2)

```

## Aggregation Doing

Now we know how to aggregate our data we need to aggregate our data in a useful way. Unfortunately the single layer we aggregated to above doesn't cut it. It returns an annual overview that doesn't really tell us too much other than what locations have consistently higher chlorophyll a. No, instead I want to learn something about seasonal or monthly trends. To do this 


:::{.callout-note}
If you would like to dive deep into stars objects - I recommend checking out the main page for the page [here](https://r-spatial.github.io/stars/index.html)
:::

# Introduction

In this blog post I'd like to cover the essentials of extracting data from the eReefs platform. First of all, hands up - who's heard of [eReefs](https://www.ereefs.org.au/)? Fair enough if you haven't, despite its star power in my world it is still a relatively niche topic. To summarise, eReefs is *"a comprehensive view of dynamic marine environment conditions across the Great Barrier Reef from end-of-catchments and estuaries to reef lagoons and the open ocean."* What this means for us is that it has a whole bunch of modelled environmental datasets that are easily accessible, backed by top-notch science, and **heavy** (i.e. we really get to flex our coding and spatial skill "muscles"). 

The goal today is to learn how to:

 1. Extract data from eReefs - a surprisingly hard thing to do
 2. That's it - the data extraction section is already going to make this a long post.
 
:::{.callout-note}
If you read this blog and want to learn what to do once you have the data, make sure to check out these two additional blogs that explore [plotting]() and [mapping]() eReefs data.
:::

# Extracting Data

The datasets we are going to look at today are the chlorophyll a dataset, and the the nitrites dataset. However there are hundreds of datasets to choose from including water movement variables, water clarity indicators, and a whole host of chemical indicators. Later in this blog I will show you how to obtain a complete list of variables available. All data produced by eReefs is stored on the [National Computing Infrastructure's (NCIs) THREDDS server](https://thredds.nci.org.au/thredds/catalog/catalogs/fx3/catalog.html) which is a server we can interact with in an automated fashion. Don't be fooled though, accessing the data can still pose quite the challenge and can sometimes seem like you are feeling your way around in the dark.

To assist in the process of extracting data from eReefs we are going to need two extra datasets: 1) a dataset of all the reefs in the area, which gives us an exceptional frame of reference for where things are happening and why. And 2) a dataset that details the specific boundaries of a target region within the scope of the eReefs model. I will be using the boundaries of the Dry Tropics region (near Townsville, QLD), however any boundaries withing the Great Barrier Reef Marine Park will work fine.

Lets get into it.

## Step 1 - Obtain Facilitating Datasets

First I will load in reef data, which is available to download online using the [GISAIMSR package](https://open-aims.github.io/gisaimsr/). This package is handy, but can sometimes be a bit of a hassle to get working as you can't install it from the usual package library. I've listed the essentials to get started but please note that if you can't get the package working, the analysis in this script will still work, your maps will just be missing the reef layer and the context that the reefs bring.

```{r}

#check and install the remotes package (this is required to then install the gisaimsr package)
if(!"remotes" %in% installed.packages()[,"Package"]){install.packages("remotes")}

#check and install the gisaimsr package
if(!"gisaimsr" %in% installed.packages()[,"Package"]){remotes::install_github("https://github.com/open-AIMS/gisaimsr")}

#read in reef data form the gisaimsr package and update the crs of the data
reefs <- get(data("gbr_feat", package = "gisaimsr")) |> 
  filter(FEAT_NAME == "Reef") |> 
  st_transform("EPSG:7844")

```

Following this I will load in my boundaries of the Dry Tropics region. Unfortunately this is a custom dataset that cannot be made available for download, however any polygon area within the Great Barrier Reef region will work so I encourage you to create your own.

```{r}

#read in the dry tropics region dataset
dt_region <- st_read("dt_region.gpkg") |> 
  st_transform("EPSG:7844")

```

:::{.callout-note}
In a pinch for coordinates? Use these for a simple box: 

```{r}

example_box <- matrix(c(144.227, -24.445,   # bottom-left
                        144.227, -15.195,   # top-left
                        151.329, -15.195,   # top-right
                        151.329, -24.445,   # bottom-right
                        144.227, -24.445),    # close the polygon by repeating the first point
                      ncol = 2, 
                      byrow = TRUE)

#create a polygon geometry and set the CRS
example_box <- st_polygon(list(example_box)) |> 
  st_sfc(crs = "EPSG:7844")

```

## Step 2 - Getting eReefs Data

I would first like to note that there are several resources online that contain useful information about access data from eReefs. These include:

 - [An eReefs R package](https://github.com/open-AIMS/ereefs)
 - [Multiple eReefs Tutorials](https://open-aims.github.io/ereefs-tutorials/)
 - [And, the National Computing Infrastructure's (NCIs) THREDDS server](https://thredds.nci.org.au/thredds/catalog/catalogs/fx3/catalog.html)
 
However, I personally find that most of these resources either A) gloss over what is really happening behind the scenes, or B) don't provide critical information needed for you to conduct your own analysis (for example, how to see what indicators are available in eReefs). It is these reasons among others that prompted me to write this blog.

Anyway, time to buckle your seatbelts kids.




# To Do - I will likely make this part of my website

 - create more visual explanations, particular around the bbox interaction
 - explain the issue are size of data and pwalk[ing] this script??
 - drop the script set up function
 - drop the name cleaning function



# Script Set Up

This script requires multiple core spatial packages, each of which is loaded below. Key variables and paths are also created.

```{r}
#| label: load packages

#use pacman function to load and install (if required) all other packages
pacman::p_load(tidyverse, glue, here, janitor, sf, tmap, stars, ncmeta, ereefs, ggplot2, RColorBrewer, huxtable, scales)

```





## eReefs Data

Now we can look to download the eReefs data. We will be using the spatial information for each of the regions that we loaded above to guide us when extracting the eReefs data.

::: {.callout-note}
Please note that this script will download the eReefs data from an online server the first time the script is run. This is a lengthy process given the size of the data (several gb) and will take a significant amount of time to process (approximately 20 minutes). To assist in future runs of the script, the data will be saved to your local computer and reloaded next time (approximate 30 seconds).
:::

Given that this is the 6th script in the series about eReefs data we already have a significant amount of contextual information about the data that makes it a lot easier to understand what data we are downloading and how we can access it. (Not all previous scripts have to be run, but reading them does help). 

Using script 5 we know that to get the data for each year we need to access the following layers:

- 2019-2020 = 1-215 (215 layers total)
- 2020-2021 = 216-580 (365 layers total)
- 2021-2022 = 581-945 (365 layers total)
- 2022-2023 = 946-1310 (365 layers total)
- 2023-2024 = 1311-1510 (200 layers)

For this script we can focus on downloading just the year of data that is closest to our target data year. I say closest because data is only released roughly every 2 years, thus sometimes the target year matches, sometimes it does not. The general method of downloading is as follows:

1. Set up a table containing the layer counter information determined above and then identify the year we are interested in.
2. Establish coordinate boundaries of the area of interest
3. Extract grid cell latitude, longitude information within this area of interest
4. Compare the coordinate boundaries of the area of interest with the grid cell latitude and longitude (this is because the grid cells are on a curvilinear grid and sometimes extend outside our area of interest due to the line "bending")
5. From this comparison, extract the true start and end points of the area of interest, accounting for bending
6. Download each financial year of data individually, using information from steps 4. and 5.

:::{.callout-note}
Use nc_vars(input_file) to get a table that lists all available variables. It will also tell you the dimensions needed to call the data (usually 3 or 4).
:::

```{r}
#| label: load the eReefs data

#create a folder to store all the data outputs
dir.create(glue("{output_path}/datasets/"))

#-----------------
#step 1:

#set up our indices reference table
indicies <- data.frame("Fyear" = c(2020, 2021, 2022, 2023, 2024),
                       "Start" = c(1, 216, 581, 946, 1311),
                       "Count" = c(215, 365, 365, 365, 200))

#pick the correct year (the one closest to our target)
target_indicies <- indicies |>
  slice(which.min(abs(Fyear - current_fyear)))

#-----------------
#steps 2 to 6 occur here:

#create a vector of all the variables we want to extract. Note the callout above if you want to get different variables
indicators_vect <- c("Chl_a_sum", "Secchi", "Turbidity", "NO3")

#define the buffer around our area, for MWI, the buffer needs to be smaller otherwise the data requested is to large to be loaded into memory
if(proj_region == "Mackay Whitsunday Isaac"){buff_numb <- 0} else {buff_numb <- 1.2}

#get a bounding box of the marine region
marine_bbox <- st_bbox(st_buffer(target_marine_region, buff_numb))
  
#rearrange to suit how eReefs like to request each point ( order is min lon, max lon, min lat, max lat)
box_bounds <- c(marine_bbox[1], marine_bbox[3], marine_bbox[2], marine_bbox[4])
  
#establish the initial file path using the ereefs package (currently we use the eReefs GBR1 biogeochemistry and sediments v3.2 model)
#input_file <- substitute_filename("catalog")
input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
    
#get all grids
grids <- get_ereefs_grids(input_file)
    
#get x and y specifically
x_grid <- grids[["x_grid"]]
y_grid <- grids[["y_grid"]]
  
#create an array of FALSE values the same dimensions as the x (and y) grids
outOfBox <- array(FALSE, dim = dim(x_grid))
    
#change array value to TRUE if the associated value in the x or y grid at the same position is outside our bounding box
if (!is.na(box_bounds[1])) {outOfBox <- apply(x_grid, 2, function(x) {(x < box_bounds[1] | is.na(x))})}
if (!is.na(box_bounds[2])) {outOfBox <- outOfBox | apply(x_grid, 2, function(x) {(x > box_bounds[2] | is.na(x))})}
if (!is.na(box_bounds[3])) {outOfBox <- outOfBox | apply(y_grid, 2, function(x) {(x < box_bounds[3] | is.na(x))})}
if (!is.na(box_bounds[4])) {outOfBox <- outOfBox | apply(y_grid, 2, function(x) {(x > box_bounds[4] | is.na(x))})}
    
#find the first x position (row) that is inside the bounding box (i.e. the first row with at least one TRUE val)
xmin <- which(apply(!outOfBox, 1, any))[1]
  
#find all (rows) inside the bounding box (i.e. all rows with at least one TRUE val) then take the last using length() as the index
xmax <- which(apply(!outOfBox, 1, any))
xmax <- xmax[length(xmax)]
    
#find the first y position (col) that is inside the bounding box (i.e. the first col with at least one TRUE val)
ymin <- which(apply(!outOfBox, 2, any))[1]
    
#find all (cols) inside the bounding box (i.e. all cols with at least one TRUE val) then take the last using length() as the index
ymax <- which(apply(!outOfBox, 2, any))
ymax <- ymax[length(ymax)]
  
#get a vector that states what region we are looking at
region_lower <- str_replace_all(str_to_lower(unique(target_marine_region$Region)), " ", "_")

for (i in indicators_vect){
    
  #create a all lower case versions for reading and writing
  i_lower <- str_to_lower(i)
  
  #check if the file already exists and can be loaded back in
  if (file.exists(glue('{output_path}/datasets/{i_lower}_{region_lower}.RData'))){
    
    #load the data in
    load(glue('{output_path}/datasets/{i_lower}_{region_lower}.RData'))
    
    #if the file does not exists, fetch the data from online then save it
  } else {
    
    #if the indicator is secchi the dimensions of the data are different (no depth)
    if (i_lower == "secchi"){
        
      #extract secchi data using indices to define layer counts
      assign(glue("{i_lower}_{region_lower}"), 
             read_ncdf(input_file, var = i, 
                       ncsub = cbind(start = c(xmin, ymin, target_indicies[1, "Start"]),
                                     count = c((xmax - xmin), (ymax - ymin), target_indicies[1, "Count"])),
                       downsample = down_sample))
        
    } else {
        
      #extract chla data using indices to define layer counts
      assign(glue("{i_lower}_{region_lower}"), 
             read_ncdf(input_file, var = i, 
                       ncsub = cbind(start = c(xmin, ymin, 44, target_indicies[1, "Start"]),
                                     count = c((xmax - xmin), (ymax - ymin), 1, target_indicies[1, "Count"])),
                       downsample = down_sample))
    }
      
    #save a copy of the data to our output folder
    save(list = glue("{i_lower}_{region_lower}"), 
         file = glue('{output_path}/datasets/{i_lower}_{region_lower}.RData'))
        
  }
}

#clean up
rm(box_bounds, input_file, grids, x_grid, y_grid, outOfBox, xmin, xmax, ymin, ymax, indicies,
   marine_bbox)

```

### Edit eReefs Data

Once the eReefs data is obtained we need to conduct some basic editing and cleaning steps such as changing out "wrong" values. Once again we are relying on the contextual knowledge built up by the previous scripts in this series to understand that these "wrong" values are the values of grid cells that occur over land (and thus should have no value). Currently these cells have values more than 10,000 times greater than any "real" cell and should be changed to a value of NA.

Additional edits conducted here include updating the names of the data, transforming the CRS of the data, and cropping the data to be more precisely within our target area (the cropping above only cuts the data down to a square box around our area, the cropping that occurs here reduces data to the exact polygon outline of our target area).

```{r}
#| label: edit the eReefs data p1

#make a cleaner version of the project region variable
clean_proj_region <- str_replace_all(str_to_lower(proj_region), " ", "_")

data_list <- map(c(glue("chl_a_sum_{clean_proj_region}"), 
                   glue("secchi_{clean_proj_region}"),
                   glue("turbidity_{clean_proj_region}"),
                   glue("no3_{clean_proj_region}")),
                 get)

#run a custom function over the list of lists datasets to crop each correctly
data_list <- map(data_list, function(dataset){
  
  #overwrite erroneous high values (note that a value of even 50 would be very very high)
  dataset[(dataset > 200)] <- NA
  
  #crop to the actual area of interest
  dataset <- dataset |> 
   st_transform(proj_crs)# |>
   #st_crop(target_marine_region)
  
})

#update the name of the each data set
names(data_list) <- c("Chla (ug/L)", "Secchi (m)", "Turbidity (NTU)", "Nitrites (mg[N]/m^3)")

```

# Data Analysis

For our data analysis section we will be focuses on a few key aspects, these are:

 - Calculating daily mean concentrations and plotting them
 - Calculating monthly mean concentrations and creating a facet map
 - Calculating annual values and comparing over time.
 
## Plots

First up are the plots, the main focus of these plots are the daily mean concentration values, however we will also take the time to calculate the annual mean and median and overlay these values on the plot.

Below we extract the data from netcdf format into a sf dataframe to create the plots. Please note that a single day (layer) of data contains upwards of 20,000 cells (i.e. indicator values), that's more than 7 million data points per year per dataset. Thus to save time when conducting this initial exploratory analysis we will take a random sub sample of only 500 cells per day (which is still a few hundred thousand data points) and use that for the dots in the background. We still do use the entire dataset to calculate the daily mean and annual mean and median.

```{r}
#| label: analyse raw eReefs data p1

#create a folder to store all the outputs
dir.create(glue("{output_path}/plots/"))

#create a custom function to convert data from nc to sf and take a sub sample if needed
convert_nc_to_sf <- function(dataset, group, sample_size = NULL){
  
  #crop to the actual area of interest
  dataset <- dataset |> 
    st_transform(proj_crs) |>
    st_crop(target_marine_region)
  
  #convert raster to a polygon dataset
  dataset_df <- st_as_sf(dataset, as_points = F, merge = F)
  
  #drop the geometry column
  dataset_df <- st_drop_geometry(dataset_df)
  
  #add dates, for some reason they get dropped during the conversion to an sf object
  target_dates <- date(st_get_dimension_values(dataset, "time"))
  
  #assign the dates to the new data
  colnames(dataset_df) <- target_dates 
  
  if (!is.null(sample_size)){#if we want a subset
  
    #take a random sample of the dataset (only 500 data points per day rather than >25,000)
    dataset_df <- dataset_df |> 
      slice_sample(n = sample_size)
  }
  
  #pivot the data longer
  dataset_df <- dataset_df |> 
    pivot_longer(cols = everything(), names_to = "Day", values_to = "Values")
  
  #make sure the day column is formatted correctly
  dataset_df$Day <- as.Date(dataset_df$Day)
  
  #add a dataset identifier using the second variable in the function
  dataset_df <- dataset_df |> 
    mutate(Indicator = group)
  
  #remove units from the values column
  dataset_df <- dataset_df |> 
    mutate(Values = as.vector(Values))
  
  #add a wet/dry column to group data by 
  dataset_df <- dataset_df |> 
    mutate(Season = case_when(month(Day) > 4 & month(Day) < 11 ~ "Dry", T ~ "Wet"))
  
  #return the df
  return(dataset_df)
  
}


#run the custom function that does all of the pre-processing of the data 
raw_data_df_subset <- map2(data_list, names(data_list), ~convert_nc_to_sf(.x, .y, 500))

#run again, without taking a subset, note this will take much longer
raw_data_df <- map2(data_list, names(data_list), ~convert_nc_to_sf(.x, .y))

#convert list of df subsets into a single df
raw_data_df_subset <- bind_rows(raw_data_df_subset)
  
#convert list of df into a single df
raw_data_df <- bind_rows(raw_data_df)

```

Once the data is in the sf format, we can then treat the data like a simple dataframe, something that is a much more familiar format to work with. The list of sf objects is combined into one single dataframe, and then additional information such as if the day was during the wet season or dry season can be added.

::: {.callout-note}
A question that may occur to you here is "if the sf format is so much easier to work with, why not use it immediately and throughout the script?". The answer to this is one of size and practicality. Although the sf format is more familiar, and thus easier to work with, it does not offer the same efficiency when storing information, or when plotting. As noted above, we down sampled the original data to roughly 1/5 of the size of the original netcdf file and yet the object still takes up more space in our environment. We do also convert the object 1:1, but this is purely to obtain a true annual mean - plotting with this dataset takes much too long. It is simply not practical to convert the objects to the sf format prematurely. Instead, when completing each of our methods further below, we perform as much of the analysis as we can using the native netcdf format. Often these steps include calculating monthly, annual, or spatial means - which naturally reduce the size of the data to a point at which it is then feasible to convert the data to the sf format.
:::

Once the additional modifications have been made to the data, it can be plotted:

```{r}
#| label: plot raw eReefs data p1

#calculate group mean to use for the yintercept line from the full dataset
group_means <- raw_data_df |>
  group_by(Indicator) |> 
  summarise(MeanValue = mean(Values, na.rm = T)) |> 
  ungroup()
    
#calculate group median to use for the yintercept line from the full dataset
group_medians <- raw_data_df |>  
  group_by(Indicator) |> 
  summarise(MedianValue = median(Values, na.rm = T)) |> 
  ungroup()
    
#summarise data by month from the full dataset
raw_data_df <- raw_data_df |> 
  mutate(Month = month(Day))
    
#create the plot, this contains a point plot, a smoothed line plot, and a volin plot
summary_plot <- ggplot() +
  geom_point(data = raw_data_df_subset,
             aes(x = Day, y = Values, color = Season, group = Indicator), 
             size = 0.1) +
  geom_smooth(data = raw_data_df_subset,
              aes(x = Day, y = Values, group = Indicator), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "blue", 
              se = F) +
  geom_violin(data = raw_data_df_subset,
              aes(x = Day, y = Values, group = Indicator), 
              alpha = 0.5, 
              color = "Black") +
  geom_hline(data = group_means, 
             aes(yintercept = MeanValue, group = Indicator), 
             color = "purple") +
  geom_hline(data = group_medians, 
             aes(yintercept = MedianValue, group = Indicator), 
             color = "green") +
  scale_y_log10() +
  scale_x_date(breaks = pretty_breaks(6)) +
  labs(x = "Date", y = "Concentration") +
  theme(panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          legend.position = "none") +
    facet_wrap(~Indicator, nrow = 1, 
               scales = "free")
  
#save the individual plots
ggsave(glue("{output_path}/plots/{region_lower}_all_indicators_plot.png"), summary_plot, width = 15, height = 5)

```

## Maps

With the plots completed we can move on to creating the monthly mean maps for each of the indicators. First we need to calculate monthly means for each indicator, then stitch the layer back into groups:

```{r}
#| label: map eReefs data p1

#write a custom function that does each of the required steps to calculate the monthly values
monthly_mean_data <- map(data_list, function(dataset){
  
  #get the list of "times" (dates) stored in the dataset (1 date per layer) and convert it into a dataframe
  dates <- data.frame(DateTime = st_get_dimension_values(dataset, which = "time"))
  
  #extract the year and month and combine into a single unique name and add an index column
  dates <- dates |> 
    mutate(Year = year(DateTime),
           Month = month(DateTime, label = T),
           RowId = row_number()) |> 
    unite(LayerName, "Year", "Month", sep = "_")
  
  #group by LayerName and get the min and max row index (layer number) for each month, then reorder the dataframe by index
  dates <- dates |> 
    group_by(LayerName) |> 
    summarise(MinIndex = min(RowId),
              MaxIndex = max(RowId)) |> 
    arrange(MinIndex)
  
  #create a trackers for the objects that will be made
  created_objects <- c()
  
  #loop over each of the unique names
  for (i in unique(dates$LayerName)){
  
    #get the index numbers for the min and max layers from the table
    min_layer <- dates |> filter(LayerName == i) |> select(MinIndex) |> as.numeric()
    max_layer <- dates |> filter(LayerName == i) |> select(MaxIndex) |> as.numeric()
    
    #extract the month of data using the index numbers 
    if (str_detect(names(dataset), "Secchi")){# if secchi: [attribute, i (lat), j (long), time] - no depth
      
      monthly_mean <- dataset[,,,min_layer:max_layer]
    
    } else {#otherwise [attribute, i (lat), j (long), k (depth), time]
      
      monthly_mean <- dataset[,,,,min_layer:max_layer]
    }
    
    #and apply the mean function over the i,j dimensions (lat, long)
    monthly_mean <- st_apply(monthly_mean, 1:2, FUN = mean, keep = TRUE)
    
    #assign the monthly_mean dataset to a new object
    assign(glue("{i}"), monthly_mean)
    
    #create a vector that tracks all the names of the created objects
    created_objects <- c(created_objects, glue("{i}"))
  
  }
  
  #combine each of the monthly layers into one new dataset ("mget" is "multiple get") and this will be the output of the function
  dataset <- do.call(c, mget(created_objects))
  
})

```

Then we convert these stacks of monthly means into sf objects:

```{r}
#| label: map eReefs data p2

monthly_mean_df <- map(monthly_mean_data, function(dataset){
  
  #extract the datasets into tables
  dataset_df <- st_as_sf(dataset, as_points = F, merge = F)
  
  #pivot the tables longer to make them easier to work with, split the layer information into Financial Year and month, and order by Month
  dataset_df <- dataset_df |> 
    pivot_longer(cols = matches("\\d"), names_to = "Layer", values_to = "Value") |> 
    separate(Layer, into = c("FinancialYear", "Month"), sep = "_", remove = F) |> 
    mutate(Month = as.factor(Month),
           Month = fct_relevel(Month, "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", 
                               "Mar", "Apr", "May", "Jun"))
  
  #return the object
  return(dataset_df)
  
})


```

Next we set up the arguments that are going to vary for each map and each region.

```{r}
#| label: map eReefs data p3

#create a list of arguments that we will use to inject the correct arguments per region
args_per_region <- list("Position" = list("Dry Tropics" = c(0.01, 0.99),
                                          "Wet Tropics" = c(0.77, 0.99),
                                          "Mackay Whitsunday Isaac" = c(0.79, 0.99)),
                        "FacetDim" = list("Dry Tropics" = 4,
                                          "Wet Tropics" = 6,
                                          "Mackay Whitsunday Isaac" = 4))

#create a list of palettes to switch between for each indicator
indicator_palettes <- c("brewer.greens", "-brewer.oranges", "-carto.ag_sunset", "brewer.blues")

#create a function that will inject the correct arguments per indicator
args_per_indicator <- function(x){

  if (str_detect(names(monthly_mean_df)[[x]], "Turb|Nitri")){#if indicator is Turbidity or nitrites use a log scale
    
    tm_scale_continuous_log(values = indicator_palettes[x])
    
  } else {#otherwise, just use a linear scale
    
    tm_scale_continuous(values = indicator_palettes[x])
    
  }
  
}

#create an function that produces the correct title
map_name <- function(reg, x){
  
  #get the indicator currently being used
  ind <- names(monthly_mean_df)[[x]]
  
  #detect if it should be a log scale or not
  log <- if(str_detect(ind, "Turb|Nitri")){"(Log Scale)"} else {"(Linear Scale)"}
  
  #glue everything together to make the correct title
  glue("{reg}, {ind}: {log}")
  
}

```

Facet maps of the monthly values can then be made:

```{r}
#| label: map eReefs data p4

#mask reefs to the region we are currently looking at
target_reefs <- st_intersection(reefs, st_union(target_marine_region))

#for each sf dataframe in our list
for (i in 1:length(monthly_mean_df)){
  
  #create the map
  map <- tm_shape(qld) +
    tm_polygons(fill = "#99B5B1",
                col = "#7bba9d") +
    tm_shape(target_land_region) +
    tm_polygons(fill = "grey90", 
                col = "black") +
    tm_shape(monthly_mean_df[[i]]) +
    tm_polygons(fill = "Value", 
                fill.scale = args_per_indicator(i),
                fill.legend = tm_legend(reverse = TRUE,
                                        title = "", 
                                        width = 2,
                                        height = 5,
                                        item.width = 0.2,
                                        na.show = FALSE,
                                        position = args_per_region[["Position"]][[proj_region]]),
                                        
                fill.free = T,
                col_alpha = 0) +
    tm_facets(by = "Month", 
              ncol = args_per_region[["FacetDim"]][[proj_region]]) +
    tm_shape(reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "grey60",
               col_alpha = 0.4) +
    tm_shape(target_reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "black",
               col_alpha = 0.5) +
    tm_shape(target_marine_region) +
    tm_borders(col = "black") +
    tm_shape(st_buffer(target_marine_region, 0.5), is.main = T) +
    tm_borders(col = NULL,
               fill = NULL) +
    tm_layout(bg.color = "#C1DEEA") +
    tm_title(text = map_name(proj_region, i),
             size = 0.8)
  
  #extract a clean name for saving
  indicator_name <- str_extract(names(data_list)[i], "^[^ ]+")

  #and save
  tmap_save(map, glue("{output_path}/plots/{region_lower}_{indicator_name}_map.png"))
  
}

```

Following this, we will make the same facet map except we will use a fixed scale for the maps.

```{r}
#| label: map eReefs data p5

#mask reefs to the region we are currently looking at
target_reefs <- st_intersection(reefs, st_union(target_marine_region))

#for each sf dataframe in our list
for (i in 1:length(monthly_mean_df)){
  
  #create the map
  map <- tm_shape(qld) +
    tm_polygons(fill = "#99B5B1",
                col = "#7bba9d") +
    tm_shape(target_land_region) +
    tm_polygons(fill = "grey90", 
                col = "black") +
    tm_shape(monthly_mean_df[[i]]) +
    tm_polygons(fill = "Value", 
                fill.scale = if (i %in% c(3,4)){tm_scale_continuous_log(values = indicator_palettes[i])} #if Turbidity or nitrites use log scale
                             else {tm_scale_continuous(values = indicator_palettes[i])}, #otherwise use linear scale
                fill.legend = tm_legend(reverse = TRUE,
                                        title = if (i == 3){glue("{names(data_list)[i]} (Log)")} #if log, write log
                                                else {names(data_list)[i]}, #otherwise just write the indicator
                                        na.show = FALSE), 
                col_alpha = 0) +
    tm_facets(by = "Month", ncol = if(proj_region == "Wet Tropics"){6}else{4}) +
    tm_shape(reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "grey60",
               col_alpha = 0.4) +
    tm_shape(target_reefs) +
    tm_borders(fill = "grey60",
               fill_alpha = 0.2,
               col = "black",
               col_alpha = 0.5) +
    tm_shape(target_marine_region) +
    tm_borders(col = "black") +
    tm_shape(st_buffer(target_marine_region, 0.5), is.main = T) +
    tm_borders(col = NULL,
               fill = NULL) +
    tm_layout(bg.color = "#C1DEEA",
              legend.position = tm_pos_out("right", "center"))
  
  #extract a clean name for saving
  indicator_name <- str_extract(names(data_list)[i], "^[^ ]+")
  #and save
  tmap_save(map, glue("{output_path}/plots/{region_lower}_{indicator_name}_map_fixed_scale.png"))
  
}

```

## Tables

Finally, we will create a table summarizing the key statistics we have found throughout this analysis. These tables will contain:

 - monthly mean values
 - annual mean values

```{r}
#| label: create summary table

#create a folder to store all the data outputs
dir.create(glue("{output_path}/tables/"))

#read in the custom function to style tables
source(here("functions/cond_form_tables.R"))

#extract monthly and annual means
summary_table <- raw_data_df |> 
  group_by(Indicator) |> 
  mutate(AnnualMeanValue = mean(Values, na.rm = T)) |> 
  group_by(Indicator, Month, AnnualMeanValue) |> 
  summarise(MonthlyMeanValue = mean(Values, na.rm = T)) |> 
  ungroup() |> 
  mutate(Month = month(Month, label = T),
         Month = fct_relevel(Month, "Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun")) |> 
  arrange(Month)

#pivot data wider into a reader friendly format
summary_table_wide <- summary_table |> 
  pivot_wider(names_from = Month, values_from = MonthlyMeanValue) |> 
  relocate("AnnualMeanValue", .after = last_col()) |> 
  mutate(across(where(is.numeric), ~round(.x, 2)))

#save the table
write_csv(summary_table_wide, glue("{output_path}/tables/{region_lower}_summary_table.csv"))

#print the table in a visually appealing way
cond_form_tables(summary_table_wide, header_rows = 1, landscape = F, score_colour = F) |> 
  set_align(everywhere, everywhere, "center") |> 
  set_valign(everywhere, everywhere, "middle")

```

















