---
title: "Creating Beautiful Plots using eReefs Data"
date: "05/23/2025"
abstract-title: "ABSTRACT"
abstract: "In this blog I demonstrate how you can make beautiful plots in R using example data extracted from the eReefs platform. You can also follow along with any of your own data."
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

library(sf)
library(stars)
library(ggplot2)
library(tidyverse)
library(scales)
library(ereefs)

sf_use_s2(FALSE)

```

# Introduction

In this blog we are going to learn how to create some visually interesting plots in R. The package we are going to be using is ggplot2, and the data we are going to be using is from eReefs. If you are interested in getting an exact copy of the data I recommend you check out my other blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd), however you can still follow along just fine using your own data. Once you have completed this blog, I recommend you check out my blog about [Mapping eReefs Data](../ereefs_mapping_data/index.qmd).

# Data

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

#if our csv does not exists, create it
if(!file.exists("example_data.csv")){

  #read in the dry tropics region dataset and start with a lat/long crs
  dt_region <- st_read("dt_region.gpkg") |> 
    st_transform("EPSG:7844")
  
  #manually assign the url from above into a variable (so I don't have to interact with the code)
  input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
      
  #get all grids
  grids <- get_ereefs_grids(input_file)
  
  #use the bbox function to get the boundaries of our targeted location
  target_bounds <- st_bbox(dt_region)
  
  #if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
  true_false_array <- grids[["x_grid"]] >= target_bounds[1] & 
    grids[["x_grid"]] <= target_bounds[3] & 
    grids[["y_grid"]] >= target_bounds[2] & 
    grids[["y_grid"]] <= target_bounds[4]
    
  #if the value is NA, change it to false.
  true_false_array[is.na(true_false_array)] <- FALSE
  
  #return the row index for every row that contains at least one true value:
  true_rows <- which(apply(true_false_array, 1, any))
  
  #find the first row that contains a true value
  first_row <- true_rows[1]
  
  #find the number of rows that contains a true value
  num_of_rows <- tail(true_rows, n = 1) - first_row
  
  #return the row index for every row that contains at least one true value:
  true_cols <- which(apply(true_false_array, 2, any))
  
  #find the first col that contains a true value
  first_col <- true_cols[1]
  
  #find the number of cols that contains a true value
  num_of_cols <- tail(true_cols, n = 1) - first_col
  
  #set our dimensions (44 is the surface depth, 100 is the 100th time step)
  our_dimensions_request <- cbind(start = c(first_row, first_col, 44, 216),
                                  count = c(num_of_rows, num_of_cols, 1, 365))
  
  #extract the data
  extracted_data <- read_ncdf(input_file, 
                              var = "Chl_a_sum", 
                              ncsub = our_dimensions_request)
  
  #change all land values to NA
  extracted_data[(extracted_data > 1000)] <- NA

  #update the crs
  extracted_data <- st_transform(extracted_data, "EPSG:7855")
  
  #convert our curvilinear object into just a bbox then update the crs on the bbox
  curvilinear_bbox <- extracted_data |> 
    st_bbox() |>
    st_as_sfc()
  
  #get a linear grid target with the same dimensions (number of cells) as our curvilinear grid 
  reg_stars <- st_as_stars(curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., 
                           nx = dim(extracted_data)[[1]], #and the dimensions to provide the x and y count. 
                           ny = dim(extracted_data)[[2]], 
                           values = NA_real_) #Fill each cell with NA
  
  #run st warp, it requires a curvilinear object, and a regular object as a target
  warped_data <- st_warp(extracted_data, reg_stars)
  
  #update the crs
  dt_region <- st_transform(dt_region, "EPSG:7855")
  
  #crop to the actual area of interest
  reg_grid_cropped_data <- warped_data |> 
    st_crop(dt_region)
  
  #drop dimensions with only 1 value
  reg_grid_cropped_data <- reg_grid_cropped_data[drop = TRUE]
  
  #convert data to a simple feature object
  sf_data <- st_as_sf(reg_grid_cropped_data, as_points = F, merge = F)
  
  #drop the geometry column from the data
  sf_data <- sf_data |>
    st_drop_geometry()
  
  #add dates, for some reason they get dropped during the conversion to an sf object
  target_dates <- as.character(st_get_dimension_values(reg_grid_cropped_data, "time"))
    
  #assign the dates to the new data
  colnames(sf_data) <- target_dates
    
  #pivot the data longer
  sf_data <- sf_data |> 
    pivot_longer(cols = everything(), 
                 names_to = "Date", 
                 values_to = "Values")
    
  #save the data to file
  write_csv(sf_data, "example_data.csv")
  
}

```

The first thing we want to do is load in the example data that we are going to use for this blog.

```{r}
#| output: FALSE

#read in our example data
example_data <- read_csv("example_data.csv")

```

This data has `r dim(example_data)[1]` rows and `r dim(example_data)[2]` columns. The column names are `r colnames(example_data)[1]` and `r colnames(example_data)[2]`:

```{r}

#view the first few rows of the data
head(example_data)

```

If we ordered this data by date we would see that there are several hundred rows of data that belong to the exact same date and time. This is because the original source of this data was a spatial file - it had a grid of values for each date:

```{r}

#order the first few rows by date
head(arrange(example_data, Date))

```

The value column is the concentration of chlorophyll a in the water column, measured in micrograms per litre. Lets update that.

```{r}

#rename the data column
example_data <- rename(example_data, "Chla (ug/L)" = "Values")

#view the first few rows of data
head(example_data)

```

:::{.callout-note}
Please note that it is generally bad form to include spaces in your column names, but I am doing it to reduce the code needed for the plotting section. You will see I have to refer to the column name using ticks (``) to make the ggplot code work because of the space in the column name.
:::

# Plotting

Due to our cursory exploration of the data we already know a few things:

 - There is a lot of data
 - The data has a time element
 - There are multiple data points per time step
 - Values are the concentration of chlorophyll a, if you are not an environmental scientist this means the data is continuous, and should not contain negative values.
 
## Distribution and Log Transformation

However what we don't know is about the distribution of the data. Below is a histogram of our data.

```{r}

#create a basic histogram plot of the data
ggplot(example_data) +
  geom_histogram(aes(x = `Chla (ug/L)`), 
                 bins = 150,
                 fill = "#00252A") +
  theme_bw()

```

Clearly this data is heavily right skewed. Although we wont be doing any statistical analysis this distribution will still impact how our plot looks. So to make things a bit nice we will look at the data with a log 10 transformation:

```{r}

#create a histogram plot of the data on a log10 scale
ggplot(example_data) +
  geom_histogram(aes(x = `Chla (ug/L)`), 
                 bins = 150,
                 fill = "#8E3B46") +
  scale_x_log10() +
  theme_bw()

```

Yup, using a log10 scale will likely help us get a better visual representation of the data.

Now, knowing a bit more abut the distribution lets consider what kind of plot we want to make. Of course it is up to you, but I know that when I see a time variable and a whole bunch of continuous values, I am thinking lines and/or dot plots.

In its most basic form here is a dot plot:

```{r}

#creat a basic dot plot of the data
ggplot(example_data) +
  geom_point(aes(x = Date, y = `Chla (ug/L)`), 
             color =  "#E6AA04") +
  scale_y_log10() +
  theme_bw()

```

A few things to note:

 - As we have already covered, there is a shit ton of data and actually plotting all the points takes several minutes (boring).
 - It looks like there are some trends but it is a bit hard to tell, particularly because the number of points makes it difficult to identify areas of high, mid, or low density that might affect the trends

For the first point, a simple solution is to take a random subset of data to make plotting more efficient. For the second point, this will in part be fixed by the sub sampling, but we will also be adding extra visuals to this plot as we go along.

## Subsetting Data

To do our sub-setting we will use the `slice_sample()` function. To ensure that we get the same number of randomly sampled points from each time step (day) we will make sure to first group our data by the Date column. In summary we want to randomly select 150 data points from each day - still quite a lot.

```{r}

#get a random subset of data, ensuring an equal sample is taken from each day
example_data_subset <- example_data |> 
  group_by(Date) |> 
  slice_sample(n = 150) |> 
  ungroup()

```

```{r}

#creat a basic dot plot of the data
ggplot(example_data_subset) +
  geom_point(aes(x = Date, y = `Chla (ug/L)`), 
             color =  "#E6AA04") +
  scale_y_log10() +
  theme_bw()

```

Awesome, right away we can see that it is likely that the values dip down towards the end of the graph before suddenly ticking back up. Note that this might be a product of the random sampling, but that is highly unlikely.

## Additional Visuals

Something that will help us determine how the data trends over time would be a nice line that follows the daily mean. We will calculate this line using the full dataset to make sure of the trend we spotted above.

```{r}

#calculate a daily mean value
example_data_daily_mean <- example_data |> 
  group_by(Date) |> 
  summarise(`Chla (ug/L)` = mean(`Chla (ug/L)`))


#create a basic dot plot plus daily mean line
ggplot() +
  geom_point(data = example_data_subset, 
             aes(x = Date, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_line(data = example_data_daily_mean, 
            aes(x = Date, y = `Chla (ug/L)`), 
            color = "#00252A",
            lwd = 1) +
  scale_y_log10() +
  theme_bw()

```

This line confirms that the values do indeed decrease towards the end of the graph before suddenly gaining again, but the line is a bit ugly no? A common replacement in this scenario is to use a Generalized Additive Model (GAM) which creates a smoothing spline that also reveals trends but is not so harsh. Noting that the GAM makes use of the multiple samples per day to acheive the desired results: 

```{r}

#create a basic dot plot plus GAM line
ggplot() +
  geom_point(data = example_data_subset, 
             aes(x = Date, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_smooth(data = example_data,
              aes(x = Date, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

Something else of interest with time series data is how things are doing relative to a long-term mean. This long-term mean might be an annual mean, or a mean of all the available data going several years back, or a mean of some historical reference period. For us, we will just look at the annual mean:

```{r}

#calculate group mean to use for the yintercept line from the full dataset
annual_mean <- example_data |>
  summarise(`Mean Chla (ug/L)` = mean(`Chla (ug/L)`, na.rm = T)) |> 
  as.numeric() |> 
  round(4)

#create a more sophisticated plot
ggplot() +
  geom_point(data = example_data_subset, 
             aes(x = Date, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  geom_smooth(data = example_data,
              aes(x = Date, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

As expected, the smoothed GAM line fluctuates above and below the annual mean. You might initially think that the GAM line goes waaaay below the mean towards the end of the plot and that surely the mean isn't correct, but remember this is all visualised with a log10 y axis.

The next thing I would like to add is some sort of visual cue to signify season. In the Townsville region where we are currently looking at the data there are only two season; "wet" and "dry". This is loosely associated with summer and winter, with hundreds to thousands of millimeters of rain falling in summer and often less than one hundred falling across all of winter. The reason we care about rainfall is that it is one of the most significant drivers of chlorophyll a concentrations in the ocean. The rain on land brings lots of nutrients down the rivers and out onto the reef - nutrients which phytoplankton consume and then produce chlorophyll a (simplified explanation).The exact cut-off dates we will use for the wet season/dry season are March and October.

```{r}

#assign either the wet or dry season to each row of data
example_data_subset <- example_data_subset |> 
  mutate(Season = case_when(month(Date) > 4 & month(Date) < 11 ~ "Dry", T ~ "Wet"))
  
#create a more sophisticated plot
ggplot() +
  geom_point(data = example_data_subset, 
             aes(x = Date, y = `Chla (ug/L)`, 
                 color = Season)) +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  scale_color_manual(values = c("#E6AA04", "#8E3B46")) +
  geom_smooth(data = example_data,
              aes(x = Date, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

There is only one more thing I would like to add to this plot, and it is mainly due to personal preference. But in this case, I would like to overlay a violin plot to further highlight the distribution of the data we are dealing with.

```{r}

#mutate the date column back into an actual date variable
example_data_subset <- example_data_subset |> 
  mutate(Date = as.Date(Date))

#do the same for the full dataset
example_data <- example_data |> 
  mutate(Date = as.Date(Date))

#create a more sophisticated plot
ggplot() +
  geom_point(data = example_data_subset, 
             aes(x = Date, y = `Chla (ug/L)`, 
                 color = Season)) +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  scale_color_manual(values = c("#E6AA04", "#8E3B46")) +
  geom_smooth(data = example_data,
              aes(x = Date, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  geom_violin(data = example_data_subset,
              aes(x = Date, y = `Chla (ug/L)`),
              alpha = 0.4, 
              color = "Black") +
  scale_y_log10() +
  scale_x_date(breaks = pretty_breaks(6)) +
  labs(x = "Date", y = "Chla (ug/L) (Log10 Scale)") +
  theme_bw()

```

And there you have fairly nice looking singular dot plot, it contains a heck of a lot of information without being too crowed (in my opinion). So extensions you could play around with for this plot could be downloading several years of data and faceting by year, comparing different water quality indicators, or even comparing different locations around the reefs.

# Caveats

You may find that the visuals in these plots are not for you, that's okay! Just because I say they look good doesn't mean you have to think that. Play around with colours, do a deep dive on [ggplot2](https://ggplot2.tidyverse.org/) options, experiment with your own ideas and find a style that works for you. There are also plenty of guide around design rules if you are particularly interested, such as this one by [Tableau](https://www.tableau.com/visualization/data-visualization-best-practices)



