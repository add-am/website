---
title: "Creating Beautiful Plots using eReefs Data"
date: "07-30-2025"
abstract-title: "ABSTRACT"
abstract: "In this blog I demonstrate how you can make beautiful plots in R using example data extracted from the eReefs platform. You can also follow along with any of your own data."
image: "image.png"
format: html
title-block-banner: true #This is our banner
include-after-body: "../../html/html_footer.html" #This is our footer
---

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

library(sf)
library(stars)
library(ggplot2)
library(tidyverse)
library(scales)
library(ereefs)

sf_use_s2(FALSE)

```

# Introduction

In this blog we are going to learn how to create some visually interesting plots in R. The package we are going to be using is ggplot2, and the data we are going to be using is from eReefs. If you are interested in getting an exact copy of the data I recommend you check out my other blog; [The Extraction of Highly Specialised Modeled Data from eReefs](../ereefs_extracting_data/index.qmd), however you can still follow along just fine using your own data if it shares a similar format. Once you have completed this blog, I also highly recommend you check out my blog about [Mapping eReefs Data](../ereefs_mapping_data/index.qmd), as this blog uses exactly the same dataset and explores its' spatial aspects.

# Data

```{r}
#| output: FALSE
#| echo: FALSE
#| code-fold: TRUE

#if our csv does not exists, create it
if(!file.exists("example_data.nc")){

  #read in the dry tropics region dataset and start with a lat/long crs
  dt_region <- st_read("dt_region.gpkg") |> 
    st_transform("EPSG:7844")
  
  #manually assign the url from above into a variable (so I don't have to interact with the code)
  input_file <- "https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml"
      
  #get all grids
  grids <- get_ereefs_grids(input_file)
  
  #use the bbox function to get the boundaries of our targeted location
  target_bounds <- st_bbox(dt_region)
  
  #if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false
  true_false_array <- grids[["x_grid"]] >= target_bounds[1] & 
    grids[["x_grid"]] <= target_bounds[3] & 
    grids[["y_grid"]] >= target_bounds[2] & 
    grids[["y_grid"]] <= target_bounds[4]
    
  #if the value is NA, change it to false.
  true_false_array[is.na(true_false_array)] <- FALSE
  
  #return the row index for every row that contains at least one true value:
  true_rows <- which(apply(true_false_array, 1, any))
  
  #find the first row that contains a true value
  first_row <- true_rows[1]
  
  #find the number of rows that contains a true value
  num_of_rows <- tail(true_rows, n = 1) - first_row
  
  #return the row index for every row that contains at least one true value:
  true_cols <- which(apply(true_false_array, 2, any))
  
  #find the first col that contains a true value
  first_col <- true_cols[1]
  
  #find the number of cols that contains a true value
  num_of_cols <- tail(true_cols, n = 1) - first_col
  
  #set our dimensions (44 is the surface depth, 100 is the 100th time step)
  our_dimensions_request <- cbind(start = c(first_row, first_col, 44, 216),
                                  count = c(num_of_rows, num_of_cols, 1, 365))
  
  #extract the data
  extracted_data <- read_ncdf(input_file, 
                              var = "Chl_a_sum", 
                              ncsub = our_dimensions_request)
  
  #change all land values to NA
  extracted_data[(extracted_data > 1000)] <- NA

  #update the crs
  extracted_data <- st_transform(extracted_data, "EPSG:7855")
  
  #convert our curvilinear object into just a bbox then update the crs on the bbox
  curvilinear_bbox <- extracted_data |> 
    st_bbox() |>
    st_as_sfc()
  
  #get a linear grid target with the same dimensions (number of cells) as our curvilinear grid 
  reg_stars <- st_as_stars(curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., 
                           nx = dim(extracted_data)[[1]], #and the dimensions to provide the x and y count. 
                           ny = dim(extracted_data)[[2]], 
                           values = NA_real_) #Fill each cell with NA
  
  #run st warp, it requires a curvilinear object, and a regular object as a target
  warped_data <- st_warp(extracted_data, reg_stars)
  
  #update the crs
  dt_region <- st_transform(dt_region, "EPSG:7855")
  
  #crop to the actual area of interest
  reg_grid_cropped_data <- warped_data |> 
    st_crop(dt_region)
  
  #drop dimensions with only 1 value
  reg_grid_cropped_data <- reg_grid_cropped_data[drop = TRUE]
  
  #save the data 
  write_stars(reg_grid_cropped_data, "example_data.nc", driver = "netCDF")
  
  #get vector of time values and save them for later
  saveRDS(st_get_dimension_values(reg_grid_cropped_data, "time"), file = "time_vector.rds")
  
}

```

The first thing we want to do is load in the example data that we are going to use for this blog. This data comes in a raster format, and was extracted from eReefs. There are few steps to loading it in proper - these are explained in more detail in my other blog.

```{r}
#| output: FALSE

#load in the example data
example_data <- read_stars("example_data.nc")

#load vector of time values
time_vals <- readRDS("time_vector.rds")

#merge "attributes" (time) back together
example_data <- merge(example_data)

#update time dimension values and names
example_data <- example_data |> 
  st_set_dimensions(3, time_vals,
                    names = c("x", "y", "time"))
  
#then update the attribute name
example_data <- setNames(example_data, "Chla")

```

## Converting to Tabular Format

Once we have the raster data we then need to convert it to a tabular format so it can be used by our `ggplot2` functions. There is a very handy function for this called `st_as_sf()` from the `stars` package that converts stars objects into sf objects. Sf objects are essentially tables with an extra column for spatial information, we can then convert the sf object to a "normal" table by simply removing the column with the extra spatial information.

:::{.callout-note}
Sf objects (from the `sf` package), and stars objects (from the `stars` package) are designed to work together as the packages were written by the same author. Thank god for that guy right!
:::

The `st_as_sf()` function has a few key arguments;

 - x (this is the stars object)
 - as_points (should each raster cell become a polygon (F) or a point (T)) - I usually prefer polygons
 - merge (should cells with equal values be merged? Yes (T) or No (F)) - I usually say no, as this can mess with statistics based on cell count
 - long (should the table be wide or long) - ggplot loves long data, so I usually set this to TRUE

```{r}

#convert data to a simple feature object
sf_data <- st_as_sf(example_data, as_points = F, merge = F, long = T)
  
#drop the geometry column from the data
sf_data <- sf_data |>
  st_drop_geometry()

```

## Exploring the Data

Okay, the data has been loaded in, and converted to tabular format. Let's take a quick look at what we are dealing with. This data has `r dim(example_data)[1]` rows and `r dim(example_data)[2]` columns. The column names are `r colnames(sf_data)[1]` and `r colnames(sf_data)[2]`:

```{r}

#view the first few rows of the data
head(sf_data)

```

If we ordered this data by date we would see that there are several hundred rows of data that belong to the exact same date and time. This is because the original source of this data was a spatial file - it had a grid of values for each date and time:

```{r}

#order the first few rows by date
head(arrange(sf_data, time))

```

The value column is the concentration of chlorophyll a in the water column, measured in micrograms per litre. Let's update that.

```{r}

#rename the data column
sf_data <- rename(sf_data, "Chla (ug/L)" = "Chla")

```

And to keep things consistent we will capitalise "Time" for the time column.

```{r}

sf_data <- rename(sf_data, "Time" = "time")

#view the first few rows of data
head(sf_data)

```

:::{.callout-note}
Please note that it is generally bad form to include spaces in your column names, but I am doing it to reduce the code needed for the plotting section. You will see I have to refer to the column name using ticks (``) to make the ggplot code work because of the space in the column name.
:::

# Plotting

Due to our cursory exploration of the data we already know a few things:

 - There is a lot of data
 - The data has a time element
 - There are multiple data points per time step
 - Values are the concentration of chlorophyll a, if you are not an environmental scientist this means the data is continuous, and should not contain negative values.
 
## Distribution and Log Transformation

However what we don't know is about the distribution of the data. Below is a histogram of our data.

```{r}

#create a basic histogram plot of the data
ggplot(sf_data) +
  geom_histogram(aes(x = `Chla (ug/L)`), 
                 bins = 150,
                 fill = "#00252A") +
  theme_bw()

```

Clearly this data is heavily right skewed. Although we wont be doing any statistical analysis this distribution will still impact how our plot looks. So to make things a bit nice we will look at the data with a log 10 transformation:

```{r}

#create a histogram plot of the data on a log10 scale
ggplot(sf_data) +
  geom_histogram(aes(x = `Chla (ug/L)`), 
                 bins = 150,
                 fill = "#8E3B46") +
  scale_x_log10() +
  theme_bw()

```

Yup, using a log10 scale will likely help us get a better visual representation of the data.

Now, knowing a bit more abut the distribution lets consider what kind of plot we want to make. Of course it is up to you, but I know that when I see a time variable and a whole bunch of continuous values, I am thinking lines and/or dot plots.

In its most basic form here is a dot plot:

```{r}

#creat a basic dot plot of the data
ggplot(sf_data) +
  geom_point(aes(x = Time, y = `Chla (ug/L)`), 
             color =  "#E6AA04") +
  scale_y_log10() +
  theme_bw()

```

A few things to note:

 - As we have already covered, there is a shit ton of data and actually plotting all the points takes several minutes (boring).
 - It looks like there are some trends but it is a bit hard to tell, particularly because the number of points makes it difficult to identify areas of high, mid, or low density that might affect the trends

For the first point, a simple solution is to take a random subset of data to make plotting more efficient. For the second point, this will in part be fixed by the sub sampling, but we will also be adding extra visuals to this plot as we go along.

## Subsetting Data

To do our sub-setting we will use the `slice_sample()` function. To ensure that we get the same number of randomly sampled points from each time step (day) we will make sure to first group our data by the Time column. In summary we want to randomly select 150 data points from each day - still quite a lot.

```{r}

#get a random subset of data, ensuring an equal sample is taken from each day
sf_data_subset <- sf_data |> 
  group_by(Time) |> 
  slice_sample(n = 150) |> 
  ungroup()

```

```{r}

#creat a basic dot plot of the data
ggplot(sf_data_subset) +
  geom_point(aes(x = Time, y = `Chla (ug/L)`), 
             color =  "#E6AA04") +
  scale_y_log10() +
  theme_bw()

```

Awesome, right away we can see that there appears to be a downtrend in concentration values around the middle of the time series before the values then increase again towards the end of the graph. It is easier to see this using this plot because we can see that there is a lower density of points in the middle of the plot near the top, where as even with the random sampling there is still a very high density of points in the middle of the plot near the bottom. It should be noted that it is theoretically possible the areas of high and low density are a product of the random sampling, but that is highly unlikely.

## Additional Visuals

Something that will help us determine with greater precision how the data trends over time, would be a nice line that follows the daily mean. We will calculate this line using the full dataset to make 100% sure of the trend we spotted above.

```{r}

#calculate a daily mean value
sf_data_daily_mean <- sf_data |> 
  group_by(Time) |> 
  summarise(`Chla (ug/L)` = mean(`Chla (ug/L)`))


#create a basic dot plot plus daily mean line
ggplot() +
  geom_point(data = sf_data_subset, 
             aes(x = Time, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_line(data = sf_data_daily_mean, 
            aes(x = Time, y = `Chla (ug/L)`), 
            color = "#00252A",
            lwd = 1) +
  scale_y_log10() +
  theme_bw()

```

This line confirms that the values do indeed decrease towards the middle of the time series before increasing again towards the end of the graph, but the line is a bit ugly no? A common replacement in this scenario is to use a Generalized Additive Model (GAM) which creates a smoothing spline that also reveals trends but is not so harsh. Noting that the GAM makes use of the multiple samples per day to achieve the desired results:  

```{r}

#create a basic dot plot plus GAM line
ggplot() +
  geom_point(data = sf_data_subset, 
             aes(x = Time, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_smooth(data = sf_data,
              aes(x = Time, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

Something else of interest with time series data is how things are doing relative to a long-term mean. This long-term mean might be an annual mean, or a mean of all the available data going several years back, or a mean of some historical reference period. For us, we will just look at the annual mean:

```{r}

#calculate group mean to use for the yintercept line from the full dataset
annual_mean <- sf_data |>
  summarise(`Mean Chla (ug/L)` = mean(`Chla (ug/L)`, na.rm = T)) |> 
  as.numeric() |> 
  round(4)

#create a more sophisticated plot
ggplot() +
  geom_point(data = sf_data_subset, 
             aes(x = Time, y = `Chla (ug/L)`), 
             color = "#E6AA04") +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  geom_smooth(data = sf_data,
              aes(x = Time, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

As expected, the smoothed GAM line fluctuates above and below the annual mean. You might initially think that the GAM line goes waaaay below the mean compared to how much it goes above the mean and that surely the mean isn't correct, but remember this is all visualised with a log10 y axis.

The next thing I would like to add is some sort of visual cue to signify season. In the Townsville region (where we are currently looking at the data) there are only two season; "wet" and "dry". This is loosely associated with summer and winter, with hundreds to thousands of millimeters of rain falling in summer and often less than one hundred millimeters falling across all of winter. The reason we care about rainfall is that it is one of the most significant drivers of chlorophyll a concentrations in the ocean. The rain on land brings lots of nutrients down the rivers and out onto the reef - nutrients which phytoplankton consume and then produce chlorophyll a (simplified explanation).The exact cut-off dates we will use for the wet season/dry season are March and October.

```{r}

#assign either the wet or dry season to each row of data
sf_data_subset <- sf_data_subset |> 
  mutate(Season = case_when(month(Time) > 4 & month(Time) < 11 ~ "Dry", T ~ "Wet"))
  
#create a more sophisticated plot
ggplot() +
  geom_point(data = sf_data_subset, 
             aes(x = Time, y = `Chla (ug/L)`, 
                 color = Season)) +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  scale_color_manual(values = c("#E6AA04", "#8E3B46")) +
  geom_smooth(data = sf_data,
              aes(x = Time, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  scale_y_log10() +
  theme_bw()

```

It is interesting to see that there is a clear relationship visible between season and chlorophyll a concentration. This graph suggests there is a sort of "recharge" and "use" cycle occurring. Where chlorophyll a reach a maximum concentration right after the end of the wet season, before being "used up" over the dry season and requiring a "recharge" by the following wet season.

There is only one more thing I would like to add to this plot, and it is mainly due to personal preference. I would like to overlay a violin plot to further highlight the distribution of the data we are dealing with. Specifically, it will highlight any regions of the plot in which large amounts of data are concentrated, as well as any spots that are particularly skewed. 

```{r}

#mutate the date column back into an actual date variable
sf_data_subset <- sf_data_subset |> 
  mutate(Time = as.Date(Time))

#do the same for the full dataset
sf_data <- sf_data |> 
  mutate(Time = as.Date(Time))

#create a more sophisticated plot
ggplot() +
  geom_point(data = sf_data_subset, 
             aes(x = Time, y = `Chla (ug/L)`, 
                 color = Season)) +
  geom_hline(yintercept = annual_mean,
             colour = "#628395",
             lwd = 1.3) +
  scale_color_manual(values = c("#E6AA04", "#8E3B46")) +
  geom_smooth(data = sf_data,
              aes(x = Time, y = `Chla (ug/L)`), 
              method = "gam", 
              formula = y ~ s(x), 
              color = "#00252A",
              se = F) +
  geom_violin(data = sf_data_subset,
              aes(x = Time, y = `Chla (ug/L)`),
              alpha = 0.4, 
              color = "Black") +
  scale_y_log10() +
  scale_x_date(breaks = pretty_breaks(6)) +
  labs(x = "Time", y = "Chla (ug/L) (Log10 Scale)") +
  theme_bw()

```

And there you have it, a fairly nice looking singular dot plot. It contains a heck of a lot of information without being too crowed (in my opinion). Some extensions you could play around with for this plot could be downloading several years of data and faceting by year, comparing different water quality indicators, or even comparing different locations around the reefs.

# Caveats

You may find that the visuals in these plots are not for you, that's okay! Just because I say they look good doesn't mean you have to think that. Play around with colours, do a deep dive on [ggplot2](https://ggplot2.tidyverse.org/) options, experiment with your own ideas and find a style that works for you. There are also plenty of guide around design rules if you are particularly interested, such as this one by [Tableau](https://www.tableau.com/visualization/data-visualization-best-practices).
