[
  {
    "objectID": "posts/tmap_migrating_to_v4/index.html",
    "href": "posts/tmap_migrating_to_v4/index.html",
    "title": "Migrating to Version 4 of the tmap R Package",
    "section": "",
    "text": "1 Introduction\nThe tmap package is one of my all time favourite R packages, and this latest update only solidified this opinion. I highly recommend that you check out the main page here, and take the time to read over some of the documents in each of the tabs.\nAs of the 27th of January, 2025, tmap version 4.0 was released, and with it came some BIG changes. The authors have done a great job making sure that the update is backwards compatible with your current code, however moving forward it is very important to start doing things the new way, as the old way will no longer be receiving updates.\nOne of the most impactful updates in my opinion, is changes to the syntax used within a lot of the core functions. This new syntax makes things easier to understand, cleaner, and provides greater flexibility in the creation of your maps. However it can be a confusing journey to undergo the transition. You will find that for a lot of the old v3 code there are now warnings/pop-up messages to help you with the transition, but there are a few things that slip few the cracks and I think it is sometimes just easier to see some examples.\nBelow I run through how my code for making maps using tmap used to look, and then discuss the changes that have happened and how things look now!\n\n\n\n\n\n\nNote\n\n\n\nNew datasets have also been added for demonstration purposes as well as the ability to extent tmap (to do things like map very unique spatial data types, or creating map overlays in a 3D environment). We won’t be covering those in this blog, but they are very interesting in their own right.\n\n\n\n\n2 How things Used to Look\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this blog I will assume a basic understanding of the tmap package and won’t be explaining in detail what each function is/does.\n\n\nRight, so in version 3.0, how did things look? Well, if I’m honest they looked a little messy. I found that there was only a moderate level of consistency between the arguments in each function, and that arguments within a function that matched together didn’t always make that fact obvious. For example, lets look at the arguments in the tm_polygons() function:\n\n\nCode\ntm_polygons(col,\n            border.col,\n            alpha,\n            palette,\n            legend.show,\n            ...) #There are many more arguments in `tm_polygons()` than what I have listed, but the general idea remains the same for the rest.\n\n\n… seems fine I guess. The arguments make sense, it looks like col would affect the colour of the polygon, and border.col would affect the border colour of the polygon. But what if I now add in the tm_borders() function:\n\n\nCode\ntm_borders(col,\n           ...)\n\n\nThe two functions share the argument col. F tm_polygons() we are pretty sure that col affects the colour of the polygon, however this tm_borders() function sounds like it has been written specifically for the borders of polygons only… So now we need the additional context of knowing that “col” in tm_polygons() changes the colour inside the polygon, but in tm_borders() changes the colour of the border of the polygon. (Technically we could guess this from contextual clues such as how tm_polygons() also has a “border.col” argument but tm_borders() does not). Confusing! Moving on, lets look at the alpha (transparency) argument. Does that change the transparency of col or border.col? You would have to read the documentation to know that. What about the palette argument? Is that the colour palette for the col or border.col argument?… Extra confusing. But it gets worse! There is only one palette argument, so how do you change the palette for the inside of the polygon independently to the palette for the border of the polygons. You can’t, so thats where tm_borders() function comes in. Just making the conflict between col more obvious. Arrgh!\nEnough ranting, lets look at a worked example of a map made using tmap version 3.0:\n\n\nCode\n#extract a subset of dat from the dataset provided with the tmap package\nexample_data &lt;- NLD_muni |&gt; \n  filter(province == \"Fryslan\")\n\n#create a map using the version 3.0 syntax\nv3_map &lt;- tm_shape(example_data) +\n  tm_polygons(col = \"name\", border.col = \"black\", alpha = 0.8, palette = \"Pastel1\", legend.show = T) +\n  tm_text(\"name\", shadow = T, auto.placement = T, size = 0.6) +\n  tm_shape(example_data) +\n  tm_borders(\"name\") +\n  tm_layout(legend.bg.color = \"white\", legend.frame.color = \"black\", asp = 1.1,\n            legend.outside = TRUE)\n\n\nAgain. Overall it doesn’t look too bad, with enough patience we can piece together what things probably do, and by trial and error we can confirm our ideas. However, it is when this code is compared to the new code that the short comings become apparent.\nBy the way, here is what the map produced by this code looks like:\n\n\nCode\n#print the map\nv3_map\n\n\n\n\n\n\n\n\n\n\n\n3 How Things Look Now\nCompared to Version 3.0, the new syntax available with tmap version 4.0 is much more consistent, concise, and also somehow does more with less. For example, lets look at the arguments in the tm_polygons() function again, right now we will just replace 1 to 1 the arguments we looked at before:\n\n\nCode\ntm_polygons(fill, #this was \"col\"\n            col, #this was \"border.col\"\n            fill_alpha, #this was \"alpha\"\n            fill.scale, #this was \"palette\"\n            fill.legend, #this was \"legend.show\"\n            ...)\n\n\nAt first glance this doesn’t seem a whole lot better, there are now just a lot of “fill” arguments. But lets have a look at tm_borders() now:\n\n\nCode\ntm_borders(col, #this was \"col\", and still is \"col\"\n           ...)\n\n\nRight away we can see the first problem has been addressed. col now always refers to the colour of the line/outline/border, whereas fill always refers to the inside/fill of the shape. Secondly, the confusion around alpha has been removed, we can see that alpha is now fill_alpha, but even cooler, there is actually also a col_alpha now, the alphas’ are independent! The same logic has been applied to palette, which is now fill.scale (too detailed to explain that change right now), and the legend arguments, which now have “fill” in front of them. So what we have now is something like this:\n\n\nCode\ntm_polygons(fill, #what variable defines the \"fill\" of the polygons\n            fill.scale, #what palette, breaks, style, should the be used to colour the polygons\n            fill.legend, #do you want a legend? What should it look like for the fill variable\n            fill.free, #should the scale be free for multiples (facets etc.)\n            fill_alpha, #how transparent should the fill colour be\n            col, #what variable defines the \"col\" (border) of the polygons\n            col.scale, #what palette, breaks, style, should the be used to colour the borders\n            col.legend, #do you want a legend? What should it look like for the col variable\n            col.free, #should the scale be free for multiples (facets etc.)\n            col_alpha, #how transparent should the border colour be\n            ...)\n\n\nIt couldn’t be more obvious what each argument does now. Lets now make the same map as above, but this time with the version 4.0 syntax:\n\n\nCode\n#create a map using the version 4.0 syntax\nv4_map &lt;- tm_shape(example_data) +\n  tm_polygons(fill = \"name\", \n              fill.scale = tm_scale_categorical(values = \"brewer.pastel1\"),\n              fill.legend = tm_legend(show = T),\n              fill_alpha = 0.8,\n              col = \"name\",\n              col.scale = tm_scale_categorical(values = \"brewer.set2\"),\n              col.legend = tm_legend(show = T)) +\n  tm_text(\"name\", \n          size = 0.6,\n          options = opt_tm_text(shadow = TRUE, \n                                point.label = TRUE)) +\n  tm_layout(legend.bg.color = \"white\",\n            legend.frame.color = \"black\",\n            legend.outside.position = tm_pos_out(\"right\", \"top\"),\n            asp = 1.1)\n\n\nTwo things are obvious in this code.\n\nWhich arguments should be associated with each other\nThe code looks slightly more verbose, but this is due to less assumptions by the functions, and more explicit directions by me\n\nSide note, you may have also noticed I have changed the structure of my code. This is purely to assist in determining which arguments match together.\nHere is what the map produced by this code looks like:\n\n\nCode\n#print the map\nv4_map\n\n\n\n\n\n\n\n\n\nIgnoring the fact that for both examples these aren’t the most visually appealing maps, it is now very easy to isolate exactly what each argument does and how we can adjust different aspects of the map. Alongside the changes surrounding the fill and col arguments, some other changes include:\n\nThe arguments: shadow = TRUE, and auto.placement = TRUE, have been nested under the new argument options, which has its own helper function opt_tm_text(). (auto.placement has also been changed to point.label). +\n\nThus: options = opt_tm_text(shadow = TRUE, point.label = TRUE)\n\nMore helper functions have been added such as the afor mentioned opt_tm_text(), the various scale helpers tm_scale_..., and functions for positioning things such as tm_pos_out.\n\nI never used to really use the helper functions before this update, but they really do make things easier. For a deeper dive on these, check out my other blog post: Making Beautiful Maps in R\n\n\n\n\n4 A Visually Appealing Map\nOkay lets be honest, I’ve been doing a lot of posturing without actually creating a map that is even remotely visually appealing. Thus, lets produce a map that is actually not half bad! Here is one I prepared earlier:\n\n\n\n\n\n\nNote\n\n\n\nThere is a lot of code that happens to produce this map, if you’re interested in learning whats going on, I break it down in detail over in my other blog post: Making Beautiful Maps in R\n\n\n\n\n\nBeautiful Map of Netherlands\n\n\nThat looks pretty good, right?\n\n\n5 Some Other Changes\nBefore I’m done, I’d like to drop a few more findings. Not only is the syntax of version 4 better, or the consistency improved, or just the visuals of the maps better. But the method for a bunch of other aspects has been improved as well. This includes things such as mapping raster data types, and faceting maps. Lets cover both at once!\nBefore the update this is how I used to create a faceted map of rasters (noting this code won’t work right now as the data isn’t available). I had to create a map for each year of data individually, then keep track of which map I was on. For the first map I created a legend, and for all other maps I didn’t create a legend. Finally, I had to stitch all of these maps together, and then I could save the result.\n\n\nCode\n#using unique regions\nfor (i in n3_marine_names) {\n  \n  #filter all basins by region\n  region_basins &lt;- n3_marine_region |&gt; filter(Region == i)\n  \n  #get the associated basins\n  basins &lt;- n3_basins |&gt; filter(Region == i)\n  \n  #create counter for j loop\n  count &lt;- 0\n  \n  #using years vector created by data sourcing script\n  for (j in time(n3_dhw_5y)){\n    \n    #track counter\n    count &lt;- count + 1\n    \n    #mask to the specific region and year\n    single_year_region &lt;- trim(mask(n3_dhw_5y[[time(n3_dhw_5y) == j]], vect(region_basins)))\n  \n    #for the first map make a legend\n    if (count == 1){\n      \n      #plot\n      map &lt;- tm_shape(single_year_region) +\n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab) +\n        tm_shape(qld) +\n        tm_polygons(col = \"grey80\", border.col = \"black\") +\n        tm_shape(region_basins, is.master = T) +\n        tm_borders(col = \"black\") +\n        tm_shape(basins) +\n        tm_polygons(col = \"grey90\", border.col = \"black\") +\n        tm_layout(asp = 5, legend.show = F, main.title = year(time(single_year_region)), main.title.position = \"centre\")\n      \n      #save the map\n      assign(glue(\"map{count}\"), map)\n      \n      #make a legend map\n      legend_map &lt;- tm_shape(single_year_region) + \n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab, legend.reverse = T, \n                  title = \"Coral bleaching likelihood \\n and number of DHW's\") +\n        tm_layout(legend.only = T, legend.title.size = 3,\n                  legend.text.size = 1.6, legend.position = c(0, 0.3))\n      \n    #otherwise, no legend\n    } else {\n        \n      #plot\n      map &lt;- tm_shape(single_year_region) +\n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab) +\n        tm_shape(qld) +\n        tm_polygons(col = \"grey80\", border.col = \"black\") +\n        tm_shape(region_basins, is.master = T) +\n        tm_borders(col = \"black\") +\n        tm_shape(basins) +\n        tm_polygons(col = \"grey90\", border.col = \"black\") +\n        tm_layout(asp = 5, legend.show = F, main.title = year(time(single_year_region)), main.title.position = \"centre\")\n      \n      #save the map\n      assign(glue(\"map{count}\"), map)\n        \n    }\n  }  \n  \n  #arrange into two rows\n  facet_map &lt;- tmap_arrange(map1, map2, map3, map4, map5, nrow = 2)\n  \n  #edit variable name for better save path\n  i_lower &lt;- tolower(gsub(\" \", \"-\", i))\n \n  #save the map as a png\n  tmap_save(facet_map, filename = glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}.png\"))\n  \n  #save the legend separately\n  tmap_save(legend_map, glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}_legend.png\"))\n  \n}\n\n\nAnd this is how I create them now. The change is purely down to the introduction of the tm_facets_... group of functions, which are modelling after the ggplot2 facet_wrap functions if you are familiar. Without a doubt, much easier, much simpler, much quicker to understand.\n\n\nCode\n#plot\nfacet_map &lt;- tm_shape(all_year_region) +\n  tm_raster(col.scale = tm_scale_intervals(values = dhw_cols, \n                                           breaks = c(1:6),\n                                           labels = dhw_lab),\n            col.free = FALSE,\n            col.legend = tm_legend(title = \"Coral bleaching likelihood and number of DHW's\")) +\n  tm_shape(qld) +\n  tm_polygons(fill = \"grey80\",\n              col = \"black\") +\n  tm_shape(region_basins, is.main = T) +\n  tm_borders(col = \"black\") +\n  tm_shape(basins) +\n  tm_polygons(fill = \"grey90\", \n              col = \"black\") +\n  tm_layout(panel.labels = year(time(n3_dhw_5y))) +\n  tm_facets_hstack()\n\n#save the map as a png\ntmap_save(facet_map, filename = glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}.png\"))\n\n\n\n\n6 Caveats\nThis blog has been written documenting some of the changes that have occurred with the R package “tmap”. I explain how some of the changes have impacted my work, and cover a few instances where the update from v3 to v4 might not be so obvious. However! In no way am I pretending to understand all of the changes that have occurred. I would highly recommend visiting the main tmap page here."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html",
    "href": "posts/making_beautiful_maps_in_r/index.html",
    "title": "Making Beautiful Maps in R",
    "section": "",
    "text": "The art of the map is an ancient skill, since the age of movement has the requirement of orientation been required. Just ask the first lobe-finned fishes from over 365 million years ago. Without maps, how would they have known to walk out of the ocean!\nSeriously though the human race has been making maps since forever, and there is just something so en-capturing about it. The ability to create a useful and visually map is incredible fun, however knowing where to start can be daunting. There are just so many options, most of which are incredibly complicated, expensive, or time consuming. Just to name a few you have ArcGIS, QGIS, Google Maps, Mapbox, Inkscape, and R. Where to start? What to choose?\nToday I’d like to share how I create my maps using R. We will cover a range of things including:\n\nWhy I use R over other programs,\nWhat are the requirements of creating maps in R,\nWhat other programs I use to help me along the way,\nThe type of code you can expect to write for maps.\n\nAnd at the end of all this, I will walk through the exact code used to produce this map:\n\n\n\nNetherlands Population Map\n\n\nLets get right into it."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#requirements-to-map-in-r",
    "href": "posts/making_beautiful_maps_in_r/index.html#requirements-to-map-in-r",
    "title": "Making Beautiful Maps in R",
    "section": "2.1 Requirements to Map in R",
    "text": "2.1 Requirements to Map in R\nTo map in R you only need a couple of things. You need your spatial data (duh), you need to decide on the R package you want to use for mapping (a bit harder), and you need patience (difficulty level: 100).\nPicking the right R package might seem hard, there are a lot of options including ggplot2, leaflet, mapview, tmap, and ggmap, however, I’ll make it easy. Pick tmap. The syntax makes sense, it has almost 100% coverage of things you would want to do, and it recently got a great update. Sorted."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#other-helpful-programs",
    "href": "posts/making_beautiful_maps_in_r/index.html#other-helpful-programs",
    "title": "Making Beautiful Maps in R",
    "section": "2.2 Other Helpful Programs",
    "text": "2.2 Other Helpful Programs\nAs I noted above, I don’t restrict myself to just R for maps, I also use ArcGIS and QGIS, because there are some things that R is just never going to be able to do well. For example, StoryMaps by ESRI (ArcGIS) - which are online interactive experiences that include excellent mapping tools. StoryMaps are great for education and outreach, and to take the reader on a journey. StoryMaps are not good for creating maps for you scientific reports. Additionally, I use QGIS for quick visualizations, you can simply drag and drop your data and instantly see what it looks like - no code necessary."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#find-the-datasets",
    "href": "posts/making_beautiful_maps_in_r/index.html#find-the-datasets",
    "title": "Making Beautiful Maps in R",
    "section": "3.1 Find the Datasets",
    "text": "3.1 Find the Datasets\nThe first thing you always want to do when making maps is to gather up all of the data you want to use. You might scoff at how obvious this seems, but you would be surprised how often you get halfway through designing the style of your core layer and you realise, wait… I don’t have a layer for the background. For the map we are going to make today, we have several datasets:\n\nThe core layer: “Netherlands Province”\nThe supporting layers;\n\n“Netherlands Municipality”\n“Netherlands District”\n\nThe background layers;\n\n“World”\n“Europe”\n\n\nA lot more than you might think for one map, don’t stress, not all maps have this many layers… some have more! Thankfully, most of our data comes wrapped up within the tmap package already, so we don’t have to do any work to find those. However, we will have to find our own source for the “Europe” background layer. There are plenty of options online, but if I’m honest I can’t even remember which one I used. Anyway, lets bring all these datasets into a global environment.\n\n\n\n\n\n\nNote\n\n\n\nFun side note, a lot of R packages are loaded in with their own testing datasets you can muck around with, such as tmap with its Netherlands dataset. Even base R has its own datasets you can access right away!\n\n\n\n\nCode\n#get a visual on the district dataset (it is pre-loaded in tmap, but we can't see it in our global environment yet)\nnld_district &lt;- NLD_dist\n\n#get the CRS of our main dataset, and use this to convert all others as needed\nproj_crs &lt;- st_crs(nld_district)\n\n#get a visual on the municipality dataset\nnld_municipality &lt;- NLD_muni |&gt; \n  st_transform(proj_crs)\n\n#get a visual on the province dataset\nnld_province &lt;- NLD_prov |&gt; \n  st_transform(proj_crs)\n\n#load in the Europe background data\neurope_background &lt;- read_sf(\"Europe_coastline_poly.shp\") |&gt; \n  st_transform(proj_crs)\n\n#get a visual on the world dataset\nworld_background &lt;- World |&gt; \n  st_transform(proj_crs)\n\n\nAt this point it is usually a great idea to map each layer individually to start to get an idea of what you are working with."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#edit-the-datasets",
    "href": "posts/making_beautiful_maps_in_r/index.html#edit-the-datasets",
    "title": "Making Beautiful Maps in R",
    "section": "3.2 Edit the Datasets",
    "text": "3.2 Edit the Datasets\nThe day that I don’t have to conduct edits on my data before mapping is the same day that I win the lottery. Editing the data is another step that is often overlooked in the process of making maps, usually because in demonstrations the edits and changes have been made before hand. That will not be the case here, I will be working step by step through each of the changes I made to the raw data to ensure that I got the best visualisation possible.\nFirst up is the Europe background layer. The first issue I have with this dataset is to do with its resolution compared to the Netherlands dataset. For example, here is each layer:\n\n\nCode\ntm_shape(europe_background) + \n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\") +\n  tm_add_legend(fill = c(\"#00252A\", \"#e6aa04\"),\n                labels = c(\"Europe\", \"Netherlands\"))\n\n\n\n\n\n\n\n\n\nWe are going to assume that the border for the Netherlands dataset is indeed more accurate and precise than the Europe dataset (given the scale). Therefore, looking at these two layers we can see that there should be a gap in the Europe dataset in the upper middle portion of the Netherlands that corresponds to a shallow bay. However the Europe dataset seems to be too low resolution to pick this up, so we will have to do it ourselves. This is achieved fairly easily:\n\nConvert the Netherlands dataset into one big polygon (remove any interior detailing)\nUse this to cut a hole out of the Europe dataset.\nThe Europe dataset will no longer appear in the shallow bay.\n\n\n\nCode\n#create a single polygon of the district dataset\nnld_dist_tmp &lt;- st_union(nld_district)\n\n#remove any holes within the polygon then make the shape valid\nnld_dist_tmp &lt;- st_remove_holes(nld_dist_tmp) |&gt; \n  st_make_valid()\n                      \n#take the Europe background and cut a hole out of it using the dataset above\neurope_sans_nld &lt;- st_difference(europe_background, nld_dist_tmp)\n\n\n\n\nCode\n#the first map is before any changes\nmap1 &lt;- tm_shape(europe_background) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n   tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#The netherlands dataset as a single polygon\nmap2 &lt;- tm_shape(nld_dist_tmp) +\n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#after cutting out a section of the data\nmap3 &lt;- tm_shape(europe_sans_nld) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_dist_tmp, is.main = T) +\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#how the finished product looks\nmap4 &lt;- tm_shape(europe_sans_nld) + \n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#arrange the maps into a row of 4\ntmap_arrange(map1, map2, map3, map4, nrow = 1)\n\n\n\n\n\n\n\n\n\nNot perfect, but definitely better as we can now clearly tell that area is supposed to be a shallow bay.\nThe second issue I have with the Europe dataset is that it is too big, and it takes quite a while to create each map - which is frustrating. To fix this we are going to crop the outer area of the dataset, because we aren’t interested in that part and won’t actually be showing it on our map. This is also achieved fairly easily:\n\nConvert the Netherlands dataset into one big bounding box (I.e., the most N,E,S,W points of the dataset)\nExpand the bounding box until it is just large than the map we intent to create\nUse this to crop the Europe dataset.\nThe Europe dataset will no longer contain all of Europe.\n\n\n\nCode\n#buffer (make bigger) the temporary dataset\nnld_dist_buf &lt;- st_buffer(nld_dist_tmp, 30000) #units is meters for this dataset (it can change)\n\n#convert the temporary dataset to a bounding box object, then back to an sf object (must be sf for mapping)\nnld_dist_bb &lt;- st_as_sfc(st_bbox(nld_dist_buf))\n\n#crop the europe background dataset\neurope_final &lt;- st_intersection(europe_sans_nld, nld_dist_bb)\n\n\n\n\nCode\n#the first map is the buffered Netherlands dataset with a bbox over the top\nmap1 &lt;- tm_shape(nld_district) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\") +\n  tm_shape(nld_dist_buf) +\n  tm_polygons(fill = NULL, \n              col = \"#8E3B46\") +\n  tm_shape(nld_dist_bb, is.main = T) +\n  tm_polygons(fill = NULL,\n              col = \"#8E3B46\")\n\n#then Europe before changes with the bbox shown\nmap2 &lt;- tm_shape(europe_sans_nld) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_dist_bb) +\n  tm_polygons(fill = NULL,\n              col = \"#8E3B46\") +\n  tm_shape(frame_position, is.main = T) + #this is just to position the frame, dw about it\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#after cropping the data\nmap3 &lt;- tm_shape(europe_final) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(frame_position, is.main = T) + #this is just to position the frame, dw about it\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#arrange the maps into a row of 4\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nA subtle change, but one which leads to much faster processing and therefore greater efficiency.\nThe Europe dataset has now received all of its edits, but we are not done yet. Next up is the “world” dataset. The problem I have with this dataset is ironically that the data is stored too efficiently! What I mean by this is that the data is stored in multipolygons, not [single]polygons. For example, a single polygon dataset would have rows of data like this:\n\n\n\nCountry\nCode\ngeom\n\n\n\n\nFrance\nFRA\npolygon(…)\n\n\nFrance\nFRA\npolygon(…)\n\n\nFrance\nFRA\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nand so on…\n…\n…\n\n\n\n(There are only multiple rows if there are multiple seperate landmasses belonging to the country).\nWhere as a single multipolygon dataset would have rows of data like this:\n\n\n\n\n\n\n\n\nCountry\nCode\ngeom\n\n\n\n\nFrance\nFRA\nmultipolygon(polygon(…), polygon(…), polygon(…)\n\n\nGermany\nDEU\nmultipolygon(polygon(…), polygon(…), polygon(…)\n\n\nand so on…\n…\n…\n\n\n\nIf a row shares all the same metadata, then the geometry information is combined into a single multipolygon. Normally this is fine, but what I want to do with this dataset is put country labels on the surrounding countries so that the reader can be better orientated if they are not overly familiar with Europe. The issue is the labels are essentially applied “per row”, and are put at the exact center of that rows’ geometry. Which in some cases for multipolygons…:\n\n\nCode\n#extract france from the world dataset\nfrance_example &lt;- world_background |&gt; \n  filter(name == \"France\")\n\n#create the example map\ntm_shape(world_background) +\n  tm_polygons(fill = \"#99B5B1\",\n              col = \"#7bba9d\") +\n  tm_layout(bg.color = \"#C1DEEA\") +\n  tm_shape(france_example, is.main = T) +\n  tm_polygons(fill = \"name\",\n              fill.scale = tm_scale_categorical(values = c(\"#e6aa04\", \"#8E3B46\", \"#00252A\"))) +\n  tm_text(text = \"name\")\n\n\n\n\n\n\n\n\n\nis in the middle of the ocean!\nOnce again, this is a relatively easy fix. All we need to do is convert from a dataset that stores its geometry information as multipolygons, to a dataset that stores the information as just polygons:\n\n\nCode\n#extract rows into single polygons for our example\nfrance_example &lt;- world_background |&gt; \n  filter(name == \"France\") |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  mutate(Id = row_number())\n\n#then do it for real on the dataset we will actually be using \nworld_final &lt;- world_background |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  mutate(Id = row_number())\n\n#create the example map\ntm_shape(world_background) +\n  tm_polygons(fill = \"#99B5B1\",\n              col = \"#7bba9d\") +\n  tm_layout(bg.color = \"#C1DEEA\") +\n  tm_shape(france_example, is.main = T) +\n  tm_polygons(fill = \"Id\",\n              fill.scale = tm_scale_categorical(values = c(\"#e6aa04\", \"#8E3B46\", \"#00252A\"),\n                                                labels = c(\"Polygon 1\", \"Polygon 2\", \"Polygon 3\")),\n              fill.legend = tm_legend(title = \"France Polygons\")) +\n  tm_text(text = \"name\")\n\n\n\n\n\n\n\n\n\nAwesome! And that now concludes the major data edits that we needed to conduct before starting the mapping (nearly there I promise).\n\n\n\n\n\n\nNote\n\n\n\nSide note, before I create each map I do a few more minor data edits. But these don’t really need explaining and are better suited sitting with the mapping code."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-inset-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-inset-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.3 Create the Inset Map",
    "text": "3.3 Create the Inset Map\nThe final stage to address before we get to the real juicy part, is asking the question “would this map benefit from a secondary/inset map?” Sometimes this inset map is a more zoomed in version of the main map that gives a closer look at a particular part of the area, or sometimes it is a more zoomed out version that provided context about where the main map in located in a wider region. In either case, consider if your map would be better if it had it. For the purposes of this demonstration I will of course be saying that my map needs an inset map, whether you agree or not is up to you.\nIn the case of creating an inset map that is more zoomed in, I would recommend doing that after the main map. However, when creating an inset map that is more zoomed out - I usually try to do that first. So off we go, its pretty easy, there are often only 2 or 3 components to my inset maps:\n\nThe background. Sometimes I can get away with recycling one of the main datasets, but for this map I had to get a whole new dataset. This background is the “world” dataset that we were editing above.\nThe reference box. I create a bounding box that extends the perimeter of my main map, then I use this bounding box as a layer in the inset map. This bounding box tells the reader exactly where the main map is located.\nIn some cases it might be helpful to have another bounding box if there are a few key areas. For us, we don’t need that today.\n\n\n\nCode\n#create a bounding box for the extent of our main layer and/or focus area (remember we have to covert this back to an sf object for mapping purposes)\nnld_bbox &lt;- st_as_sfc(st_bbox(nld_district))\n\n#create a larger bounding box of the main layer to use to set the perspective (play around with the distance to get what suits you)\ninset_view_positioning &lt;- nld_municipality |&gt; \n  st_buffer(dist = 1000000) |&gt; #create a much larger buffer around our main layer\n  st_bbox() |&gt; #turn the data into a bounding box\n  st_as_sfc() #turn the bounding box into an sf object (required for mapping)\n\n#create the inset map\ninset_map &lt;- tm_shape(inset_view_positioning) + #this positions our perspective\n  tm_polygons(fill = NULL, #make both null so we don't actually see anything\n              col = NULL) + \n  tm_shape(world_final) + #this is the full background\n  tm_polygons(fill =  \"#99B5B1\") + #a muted green colour (land)\n  tm_text(text = \"iso_a3\", #each country gets its named printed \n          size = 0.3, #not too big\n          options = opt_tm_text(shadow = T, #with a shadow on the text to make it pop\n                                shadow.offset.x = 0.01,\n                                shadow.offset.y = 0.01)) + \n  tm_shape(nld_bbox) + #this is the bounding box of our main layer\n  tm_borders(lwd = 2, #we could also have used tm_polygons, but lets try something new\n             col = \"#8E3B46\") + #a deep red colour\n  tm_layout(asp = 0, #set aspect to zero to make the map full fill the frame when printed\n            bg.color = \"#C1DEEA\", # mute blue colour (ocean)\n            outer.bg.color = \"#F2F2F2\") # a very subtle eggshell (the background of my website)\n\n\nDon’t worry if this seems like a lot, once you start creating your own maps you will find it all makes sense :) Lets take a quick peak at the map now as well:\n\n\nCode\ninset_map"
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-main-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-main-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.4 Create the Main Map",
    "text": "3.4 Create the Main Map\nOkay nerds, the part you’ve all been waiting for, the main map. This map has a few layers to it:\n\nThe background, we are using the Europe background for this map (not the world background) as the Europe dataset has much greater detail which is necessary for viewing up close. Remember that we have also already edited the Europe dataset to have the better borders around the Netherlands as well.\nThe Netherlands district data, the focus of our map is population count per province. This is where a lot of the detail is going to come from so we have picked the dataset with the smallest cells.\nThe Netherlands municipality data, which we just use for the larger borders.\nThe Netherlands province data. We are also using this for its borders (larger again), but just to add a pop of red into the map as that is my colour scheme.\n\nAnd that’s it, the rest of the detail comes just from the code:\n\n\nCode\n#add a value of 1 to every row of population data (because you cant log transform zero)\nnld_district &lt;- nld_district |&gt; \n  mutate(population = population+1)\n\n#edit the province dataset to have a column named \"province' (a trick to streamline the legend)\nnld_province &lt;- nld_province |&gt; \n  mutate(UseMe = \"Province\")\n\n#edit the municipality dataset to have a column named \"municipality\" (same trick)\nnld_municipality &lt;- nld_municipality |&gt; \n  mutate(UseMe = \"Municipality\")\n\n#create the map\nmain_map &lt;- tm_shape(europe_final) + #the background data\n  tm_polygons(fill = \"#99B5B1\", #a muted green (land)\n              col = \"#7bba9d\") + #a slightly darker green for the borders\n  tm_shape(nld_district, is.main = T) + #the main dataset (use \"is.main\" to make sure the perspective is set by this layer)\n  tm_polygons(fill = \"population\", #what column dictates the styling\n              fill.scale = tm_scale_continuous_log(values = c(\"#FFFFFF\", \"#00252A\"), #white to a dark green\n                                                   ticks = c(1, 10000, 100000)), #what ticks are visible in the legend\n              fill.legend = tm_legend(title = \"Population\", #what is the title of the legend\n                                      position = tm_pos_out(\"right\", \"center\"), #where is the legend positioned\n                                      title.color = \"black\", #what is the colour of the legend title\n                                      reverse = TRUE), #reverse the numbers (1 at the bottom) of the legend\n              col = NULL) + #make sure the borders between districts are not visible\n  tm_shape(nld_municipality) + #municipality borders\n  tm_borders(col = \"UseMe\", #what column dictates the styling - this will also determine the name for this legend\n             col.scale = tm_scale_categorical(values = \"#e6aa04\"), #bright orange\n             col.legend = tm_legend(title = \"\", #no title (the name is in the key instead)\n                                    position = tm_pos_out(\"right\", \"center\"), #legend positioning\n                                    lwd = 2), #how thick is the line in the legend\n             lwd = 1) + #how thick is the line on the map\n  tm_shape(nld_province) + #province borders\n  tm_borders(col = \"UseMe\", #what column dictates the styling - this will also determine the name for this legend\n             col.scale = tm_scale_categorical(values = \"#8E3B46\"), #a dark red\n             col.legend = tm_legend(title = \"\",  #no title (the name is in the key instead)\n                                    position = tm_pos_out(\"right\", \"center\")), #legend positioning\n             lwd = 2) + #how thick is the line on the map\n  tm_layout(bg.color = \"#C1DEEA\", #a muted blue (ocean)\n            legend.bg.color = \"white\", #background colour of the legend\n            asp = 0, #set aspect to zero to make the map full fill the frame when printed\n            outer.bg.color = \"#F2F2F2\") + # a very subtle eggshell (the background of my website)\n  tm_credits(\"© Data: Statistics Netherlands, Software: R-tmap\\n Map Created: Adam Shand\", #some credit text to add to the map\n             position = c(\"left\", \"bottom\")) + #where does the text go\n  tm_compass(type = \"rose\", #what type of compass\n             position = c(\"LEFT\", \"TOP\"), #where is the compass positioned\n             color.dark = \"black\", #what is the \"dark\" colour of the compass\n             color.light = \"white\", #what is the \"light\" colour of the compass\n             size = 3.5) #how big is the compass\n\n\nOnce again this might seem like a lot, but I promise it really isn’t that bad once you get the hang of it. Plus, the control you will have over the styling of your maps is amazing. Anyway, here’s how the main map looks:\n\n\nCode\nmain_map"
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-final-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-final-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.5 Create The Final Map",
    "text": "3.5 Create The Final Map\nThe final stage of making our map is to combine the inset map and the main map. To achieve this we need to create a “viewport” using the grid package, which is essentially a little box somewhere on the main map that we can put the inset map inside. Getting the correct positioning of the viewport can be a bit tricky as the controls are not completely intuitive, so I will give a little demonstration of how things work.\nThe viewport is created with the function viewport, which has a few main arguments; viewport(x, y, width, height, just). Each of these control the follow aspects:\n\nx = the position of the viewport on the x-axis as a proportion (scaled from 0 to 1)\ny = the position of the viewport on the y-axis as a proportion (scaled from 0 to 1)\nwidth = the width of the viewport window as a proportion of the whole image.\nheight = the height of the viewport window as a proportion of the whole image.\njust = the “starting” point of the positioning. This is where things become less intuitive.\n\nWidth and height are pretty obvious so we will skip over those. However, x, y, and just all interact with one other. For example, if we set x and y to 0.3 , and just to “right”/“bottom” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"right\", \"bottom\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport1.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 1\n\n\nBut if we, change just to “left”/“bottom” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"left\", \"bottom\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport2.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 2\n\n\nFurther again, lets change just to “left”/“top” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"left\", \"top\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport3.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 3\n\n\nOkay, so what the hell is happening here? I have made this graphic to help explain:\n\n\n\nExample Viewport 4\n\n\nSo essentially the “just” argument is telling the x and y arguments where to measure to on the viewport. If you try to make a viewport and it doesn’t appear on your final map, the chances are that the interaction between x/y and just is putting the viewport window off the edge of the image. My positioning for the viewport is as follows:\n\n\nCode\n#place the viewport\ninset_viewport &lt;- viewport(y = 0.015, #just a tiny bit off the bottom\n                           x = 0.98, #almost all the way over to the right\n                           width = 0.33, #make window 1/3 the size of the main image\n                           height = 0.33, #make window 1/3 the size of the main image\n                           just = c(\"right\", \"bottom\")) #x and y are to the right bottom of the window\n\n#save map\ntmap_save(main_map, \"final_map.png\", \n          insets_tm = inset_map, \n          insets_vp = inset_viewport,\n          height = 6, \n          width = 8)\n\n\nHere’s how that looks:\n\n\n\nFinal Image"
  },
  {
    "objectID": "posts/learning_to_create_custom_functions/index.html",
    "href": "posts/learning_to_create_custom_functions/index.html",
    "title": "Learning To Create Your Own Custom Functions",
    "section": "",
    "text": "If you’re anything like me, when you first thought of creating your own custom functions in R it felt so wildly out of your comfort zone that you decided against it. They seemed big and scary, and only something the “really good” R coders did. The reality is far from that, and I hope this post does something to dissuade that fear and push you to start creating your own functions. Below, I’d like to discuss the essentials:\n\nWhat are functions really?\nWhy you should consider making custom functions.\nHow you can make your own functions.\nSome compelling reasons for making your own functions.\n\n\n\n\n\n\n\nNote\n\n\n\nWant to see one of my custom functions in action? I’ve already written an entire blog post about a function I use almost every day! Check it out here, it is all about cleaning and organizing dataframes (yes, I am aware that sounds boring, I promise its not)."
  },
  {
    "objectID": "posts/learning_to_create_custom_functions/index.html#an-example-of-a-useful-custom-function",
    "href": "posts/learning_to_create_custom_functions/index.html#an-example-of-a-useful-custom-function",
    "title": "Learning To Create Your Own Custom Functions",
    "section": "3.1 An Example of A Useful Custom Function",
    "text": "3.1 An Example of A Useful Custom Function\nAnyway, lets actually create our own (useful) function. When I first understood how to create a function I was super excited to get started, but I quickly realized that I didn’t actually have a good reason to write a function. I find this is the case with a lot of intermediate coders, you might know the theory, but then finding places to implement it presents a whole new challenge. So lets refresh and hopefully come up with some good ideas:\n\nFunctions are bits of code that are used lots - is there anything code you have written that you have used more than once?\nFunctions usually do one thing really well - your first function doesn’t have to change the world!\nThere are thousands of functions already out there - the “best” problems probably already have functions written for them, focus on problems specific to your niche of work to find gaps.\n\nUsing these points, here are some ideas relevant to me (I encourage you to think of your own):\n\nA function that cleans tables how I specifically like them to look (see here).\nA function that run specific statistical calculates I use for my scientific reports.\nA function that calculates landuse change by class (check out my long-form projects to read about this one).\nA function that calculates important summary statistics about fish observations.\n\nFor demonstration purposes, lets learn together how to create that fourth function; calculating summary stats for fish observation data. First, here is some example data that I made up. It has observation counts for three different fish species across four different locations:\n\n\nCode\n#read in the example dataset\nfish_obs_df &lt;- read.csv(\"fish_obs_df.csv\")\n\n#view the dataframe\ncond_form_tables(head(fish_obs_df, 10))\n\n\n\n\nLocationSpeciesObservations\n\nLocation DSpecies 20\n\nLocation BSpecies 217\n\nLocation ASpecies 39\n\nLocation ASpecies 213\n\nLocation DSpecies 19\n\nLocation CSpecies 24\n\nLocation DSpecies 215\n\nLocation CSpecies 38\n\nLocation BSpecies 310\n\nLocation ASpecies 29\n\n\n\n\n\nCode\n#plot the data\nggplot(fish_obs_df) +\n  geom_density(aes(x=Observations, color = Species, fill = Species), bw = 0.4, alpha = 0.5) +\n  scale_fill_manual(values = c(\"#e6aa04\", \"#00252A\", \"#8E3B46\")) +\n  scale_colour_manual(values = c(\"#e6aa04\", \"#00252A\", \"#8E3B46\")) +\n  theme_bw() +\n  facet_wrap(~Location)\n\n\n\n\n\n\n\n\n\nThe data looks fairly standard. Normally, we probably then proceed to calculate some basic stats like the mean, median, min, max, etc. So lets do that:\n\n\nCode\n#generic summary stats\nsummary_table &lt;- fish_obs_df |&gt; \n  group_by(Location, Species) |&gt; \n  summarise(Mean = round(mean(Observations),2),\n            Median = median(Observations),\n            Min = min(Observations),\n            Max = max(Observations),\n            Range = Max - Min) |&gt; \n  ungroup()\n\n#print the table\ncond_form_tables(summary_table)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRange\n\nLocation ASpecies 112.8 139178\n\nLocation ASpecies 211   1102222\n\nLocation ASpecies 37.82821210\n\nLocation BSpecies 115.1 1542622\n\nLocation BSpecies 217   1713207\n\nLocation BSpecies 311.9 1261711\n\nLocation CSpecies 16.11601818\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.0885116\n\nLocation DSpecies 19.84107136\n\nLocation DSpecies 28.09801919\n\nLocation DSpecies 34.94501212\n\n\n\nCool, and for fun, lets also say that we are interested to know how many times the observation count of the species was above 10 at each site:\n\n\nCode\n#number of observations above n\nsummary_table_2 &lt;- fish_obs_df |&gt; \n  filter(Observations &gt; 10) |&gt; \n  group_by(Location, Species) |&gt; \n  summarise(CountAbove10 = n()) |&gt; \n  ungroup()\n\n#add the count to the main table\nsummary_table &lt;- left_join(summary_table, summary_table_2)\n\n#print the table\ncond_form_tables(summary_table)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\nNow, for the purposes of this learning experience, lets say that this initial analysis above is something that I will need to do every time I load a dataset, and is therefore a perfect time to write a function to do the analysis for me. So do I go from the code I have written to a function? Like this:\n\nIdentify the code to go in the function (we’ve done this).\nPut the code inside the function wrapper my_custom_function &lt;- function(input){right here!}:\n\n\n\nCode\nmy_custom_function &lt;- function(inputs){\n  \n  #generic summary stats\n  summary_table &lt;- fish_obs_df |&gt; \n    group_by(Location, Species) |&gt; \n    summarise(Mean = round(mean(Observations),2),\n              Median = median(Observations),\n              Min = min(Observations),\n              Max = max(Observations),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- fish_obs_df |&gt; \n    filter(Observations &gt; 10) |&gt; \n    group_by(Location, Species) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nIdentify the inputs required for the code to run:\n\n\n\nCode\nmy_custom_function &lt;- function(inputs){\n  \n  #generic summary stats\n  summary_table &lt;- fish_obs_df |&gt; #the fish_obs_df dataset is an input, we need to tell the function what dataset to use\n    group_by(Location, Species) |&gt; #the Location and Species columns are inputs, we need to tell the function what columns to group by\n    summarise(Mean = round(mean(Observations),2), #the Observation column is an input, we need to tell the function what columns calculate on\n              Median = median(Observations),\n              Min = min(Observations),\n              Max = max(Observations),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- fish_obs_df |&gt; \n    filter(Observations &gt; 10) |&gt; #the value 10 is an input, we need to tell the function what cut off value to use\n    group_by(Location, Species) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nLooks like we have five different inputs, next we give each of those inputs their own placeholder:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; #the fish_obs_df dataset is now \"x\"\n    group_by({{y}}, {{z}}) |&gt; #the Location and Species columns are now \"y\" and \"z\" we need to use curly-curly brackets for column names provided externally\n    summarise(Mean = round(mean({{a}}),2), #the Observation column is now \"a\"\n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; #the value is now \"b\"\n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nSpecify the output of the function:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(Mean = round(mean({{a}}),2), \n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n  #what should be returned?\n  return(summary_table)\n\n}\n\n\n\nRun the function:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(Mean = round(mean({{a}}),2), \n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; #the value is now \"b\"\n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nnow, if we run the code, the function will appear in your global environment on the right. Congratulations, you just made a function. Lets see if it works:\n\n\nCode\n#run the function\nmy_custom_function(x = fish_obs_df,\n                   y = Location,\n                   z = Species,\n                   a = Observations,\n                   b = 10) \n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\nToo Easy!\nOk, spoiler, we are not actually done yet. First of all, lets make those place holders more helpful:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt;\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nSecondly, lets make the column name for the cut off value adaptive to the actual cut off value:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt; #we have to use !!sym() when the name is not named col. We also use \":=\" in place of the normal equals\n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nThird, lets identify what kind of dependencies this function has, i.e., what kind of functions does it rely on and what packages would we have to load for it to work:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #load the required packages\n  library(dplyr)\n  library(glue)\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; #the value is now \"b\"\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nForth, what if the packages haven’t been installed? Lets add a check and warning for this:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #set a vector of names of packages we need\n  pkg &lt;- c(\"dplyr\", \"glue\")\n  \n  # Loop through each package\n  for (p in pkg) {\n    if (!requireNamespace(p, quietly = TRUE)) {\n      warning(sprintf(\"The package '%s' is not installed. Please install it with install.packages('%s')\", p, p))\n    } else {\n      library(p, character.only = TRUE)\n    }\n  }\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; #the value is now \"b\"\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt;  \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nNow this is starting to look like a real function! Lets do some testing to make sure those adjustments worked fine:\n\n\nCode\ncut_off_is_10 &lt;- my_custom_function(df = fish_obs_df,\n                                    group_col_1 = Location,\n                                    group_col_2 = Species,\n                                    value = Observations,\n                                    cut_off_value = 10)\n\ncond_form_tables(cut_off_is_10)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\n\n\nCode\ncut_off_is_20 &lt;- my_custom_function(df = fish_obs_df,\n                                    group_col_1 = Location,\n                                    group_col_2 = Species,\n                                    value = Observations,\n                                    cut_off_value = 20) \n\ncond_form_tables(cut_off_is_20)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove20\n\nLocation ASpecies 112.8 139178\n\nLocation ASpecies 211   11022221\n\nLocation ASpecies 37.82821210\n\nLocation BSpecies 115.1 154262219\n\nLocation BSpecies 217   1713207\n\nLocation BSpecies 311.9 1261711\n\nLocation CSpecies 16.11601818\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.0885116\n\nLocation DSpecies 19.84107136\n\nLocation DSpecies 28.09801919\n\nLocation DSpecies 34.94501212\n\n\n\nLooking good to me. What we have now is our very own custom function that:\n\ntakes a dataframe, two grouping columns, a value column, and a cut-off/objective value\nand returns a summary dataframe as well as the number of observations that were above the cut-off\n\nHowever, there is still one glaring gap that I find alot of tutorials skip over… this code is still in the same script! All we have really done is make it longer and slightly abstracted so far!\nThe final stage of creating our custom function is saving and tucking away the function somewhere else so we can then refer to it later as we need. Doing this is not to hard:\n\nOpen a new R script. Not a .qmd file, or a markdown file, a pure R script.\nCopy and paste the custom function into the new R script.\nSave this script somewhere relevant, I like to create a folder in my work space called “functions”.\n\nDone. To access the function that we just put inside the script we then write the following code:\n\n\nCode\nsource(\"path_to_script/script_name.R\")\n\n\nThis will load the function into your global environment ready for use."
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html",
    "href": "posts/going_loopy_for_for_loops/index.html",
    "title": "Going Loopy for For Loops",
    "section": "",
    "text": "Here’s the scene, you’ve started on your R coding journey, know how to create an object, how to discern between vectors and lists, maybe even written a few short scripts. Then all of a sudden your professor/teacher/boss pulls a fast one on you and introduces for loops. For loops? What are they? How’s that work? Whats going on? You struggle through and complete the task, but didn’t quite understand what was going on when they explained it to you… Well at least that’s how it went for me.\nIn this post I wanted to quickly talk about for loops in R, specifically, I’m looking to cover:\n\nWhat is really happening in a for loop\nWhere you can go to read about for loops in much (much) more detail\nWhy you might want to write a for loop\nHow you can start to write your own loops\nAnd ironically, why I actually try to avoid using for loops"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#i-the-iteration",
    "href": "posts/going_loopy_for_for_loops/index.html#i-the-iteration",
    "title": "Going Loopy for For Loops",
    "section": "2.1 i The Iteration",
    "text": "2.1 i The Iteration\n“i” can be anything, which is not super helpful (sorry). What might be helpful is just seeing an example. This code:\n\n\nCode\n#write a for loop to print the numbers 1 to 10\nfor (PotatoSalad in 1:10){\n  print(PotatoSalad)\n}\n\n\nwill produce the exact same result as this code:\n\n\nCode\n#write a for loop to print the numbers 1 to 10\nfor (i in 1:10){\n  print(i)\n}\n\n\nWe use “i” as an iteration counter (hence usually getting called “i”) that keeps track of what loop we are on with respect to n. For our example above, n is 10. So on the first loop “i” is 1, on the second loop “i” is 2, on the third loop “i” is three…, all the way until “i” is 10. At that point, the for loop finishes. Hopefully it is now intuitive to see how print(i) produces the numbers it does."
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#n-the-range",
    "href": "posts/going_loopy_for_for_loops/index.html#n-the-range",
    "title": "Going Loopy for For Loops",
    "section": "2.2 n The Range",
    "text": "2.2 n The Range\n“n” is the range of the loop. “n” tells the for loop two things;\n\nHow many times to continue looping\nWhat elements to loop over\n\nIn our example above, n is 10. But wait, that’s not quite right, n is actually 1 to 10. This is an important distinction to make because it is usually one of the first places we make errors. Lets take a look:\n\n\nCode\n#write a for loop to print the number 10 (spot the issue)\nfor (i in 10){\n  print(i)\n}\n\n\n[1] 10\n\n\nAs you can see, when “n” is just 10, then the output is only the value “10”. This is because the range (AKA length) of “n” was only 1. An easy way to check this is to use the length() function:\n\n\nCode\nlength(10)\n\n\n[1] 1\n\n\n\n\nCode\nlength(1:10)\n\n\n[1] 10\n\n\nSo the first super important thing to remember is that “n” is a range, it has a point you want to start at, and a point you want to end at. The second super important thing to remember about “n” is that it directly tells the specific value to start and end at, and therefore determine the value that “i” is going to be. Here is a simple demonstration:\n\n\nCode\n#write a for loop to print the numbers 15 to 22\nfor (i in 15:22){\n  print(i)\n}\n\n\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n[1] 21\n[1] 22\n\n\nIn this case, we started at 15 and ended at 22. So on the first loop, “i” is 15, second loop “i” is 16, etc.\nThe last super important thing to remember about “n” is that it does not have to be numeric! It took me a while to realise this, but it can allow you to do some cool things. Here is another quick example:\n\n\nCode\n#create a vector of character elements\nn_range &lt;- c(\"PotatoSalad\", \"FishFingers\", \"Im... Kinda Hungry\")\n\n#write a for loop to print this vector one element at a time\nfor (i in n_range){\n  print(i)\n}\n\n\n[1] \"PotatoSalad\"\n[1] \"FishFingers\"\n[1] \"Im... Kinda Hungry\"\n\n\nThis does throw a minor curve ball though. Did you notice that the code is now for (i in n_range), there is no “1:” in front of “n_range”, this is just because the object “n_range” already has a range of elements that we can loop over. Lets use length() again to show this:\n\n\nCode\nlength(n_range)\n\n\n[1] 3\n\n\nIf you can remember these core things about for loops you will get very, very far with them. So to summarise. “n”:\n\nMust be a range, it needs a start and end point\nTells “i” what value it is going to be\nCan be a numeric, or character!"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#f-the-function",
    "href": "posts/going_loopy_for_for_loops/index.html#f-the-function",
    "title": "Going Loopy for For Loops",
    "section": "2.3 f The Function",
    "text": "2.3 f The Function\nThe final part of a for loop is by far the biggest part of the code, but it is ironically very straight forward to understand if you have a little bit of an R background. The function or functions inside a for loop are the exact same functions that you would be using outside a for loop! The hard part to figure out is where to place that stupid “i” value. Personally, I haven’t been able to find a method of explaining where “i” goes other than by being very conscious of the purpose of your for loop. Start with simple and short loops and work your way to more complicated tasks, it will come naturally. Generally you will find that “i” only needs to be placed in a few key locations, however if you miss a spot, happy debugging!\n\n\n\n\n\n\nNote\n\n\n\nWant to learn more about For Loops from the professionals? Check out the Iteration chapter in R for Data Science!"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#the-scenario",
    "href": "posts/going_loopy_for_for_loops/index.html#the-scenario",
    "title": "Going Loopy for For Loops",
    "section": "3.1 The Scenario",
    "text": "3.1 The Scenario\nSomething I do almost every day is make maps. Usually fairly simple maps, often they show sample site locations, or coral monitoring locations, or the size of a seagrass meadow, things like that. These maps are included in static word documents and are often needed over large chunks of areas. However, the combination of a large study location, a high quantity of sample site locations, and a static output (can’t put an interactive map into word), means that instead of one large map, I need to create lots of small maps for each little area.\nThis here, is an absolutely prime example of a compelling reason to use a for loop. To give you some numbers, in one of my projects I need to create 67 maps. If I was to manually write out the code for each of those 67 maps, my script would have 3886 lines of code just dedicated to creating the maps. Instead, I use a for loop and pull 67 maps out of less than 100 lines of code. Not only that, but I also reduce the chance of an error sneaking into my code by 67x.\nBelow is a simplified mock up of the code I would use for this, noting that I have used made up sampling locations for data privacy, and created interactive maps for your enjoyment. We will see the full code in action first, then break it down step by step.\n\n\nCode\n#read in some example data that I made up\nexample_sites &lt;- st_read(\"example_data.gpkg\")\n\n#extract an object that contains the three unique locations we are looking at\nlocations &lt;- unique(example_sites$Location)\n\n#create a list that will store my maps\nlist_of_maps &lt;- setNames(vector(\"list\", length(locations)), locations)\n\n#initialize the for loop\nfor (i in locations){\n  \n  #filter our dataset\n  sub_set_of_sites &lt;- example_sites |&gt; \n    filter(Location == i)\n  \n  #create a simple map\n  single_map &lt;- tm_shape(sub_set_of_sites) +\n    tm_dots(shape = \"Site\", size = 1, col = \"Site\", fill = \"Site\") +\n    tm_text(\"Site\", size = 2, ymod = 1) +\n    tm_layout(legend.show = F)\n  \n  #add each map to the list\n  list_of_maps[[i]] &lt;- single_map\n  \n}\n\n\nHere is how one of the maps looks:\n\n\nCode\n#view the map\nlist_of_maps[[\"Alligator Creek\"]]"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#the-breakdown",
    "href": "posts/going_loopy_for_for_loops/index.html#the-breakdown",
    "title": "Going Loopy for For Loops",
    "section": "3.2 The Breakdown",
    "text": "3.2 The Breakdown\nTime to take a closer look at whats happening here.\n\nFirst of all, i use a function called st_read() from the sf package to load in my dataset. For the purposes of this post, we don’t need to worry about this package and its functions. Check out my other posts for more details on this area. What I will do here though, is show a sneak peak of the data.\n\n\n\nCode\n#read in some example data that I made up\nexample_sites &lt;- st_read(\"example_data.gpkg\")\n\n\n\n\n\n\nLocationSiteXY\n\nAlligator CreekSite 1147-19.3\n\nAlligator CreekSite 2147-19.3\n\nAlligator CreekSite 3147-19.3\n\nThe StrandSite 1147-19.2\n\nTown CommonSite 1147-19.2\n\nTown CommonSite 2147-19.2\n\nTown CommonSite 3147-19.2\n\nMagnetic IslandSite 1147-19.2\n\nMagnetic IslandSite 2147-19.2\n\n\n\n\nFrom this dataset I then extract a vector of unique locations, which in this case we can easily see is just the four (Alligator Creek, The Strand, Town Common, Magnetic Island).\n\n\n\nCode\n#extract an object that contains the three unique locations we are looking at\nlocations &lt;- unique(example_sites$Location)\n\n\n\nI then create a list to store the outputs of my for loop. This step can be done in a wide range of ways, for example you could store each output as a separate object, if you know the number of outputs you could pre-define a list of that length to store the outputs (like I did), or if the number of outputs is a mystery you can grow the list as you go. There is no “best” way to do this, however it is generally frowned upon to grow the list as you go, as this can be computationally quite expensive. My recommendation would be to use the first two options, favoring a list with a pre-defined length if you can.\n\n\n\nCode\n#create a list that will store my maps\nlist_of_maps &lt;- setNames(vector(\"list\", length(locations)), locations)\n\n\n\nThe set up is done and it is now time to begin the for loop. This section of the code is a good time to review what we discussed above. We can see that I am going to loop over “locations”, and for each loop “i” will become of the elements in “locations”.\n\n\n\nCode\n#initialize the for loop\nfor (i in locations){\n\n\n\nWe are now working within the for loop. Remember, this section of the code will be run again and again and again. The first thing we do inside the for loop is take a subset of our data. We can filter the data by “i” because “i” has taken on the first element of “location”.\n\n\n\nCode\n  #filter our dataset\n  sub_set_of_sites &lt;- example_sites |&gt; \n    filter(Location == i)\n\n\n\nUsing the subset of the data, which will now only contains rows from one location thanks to our filter. We then create the map. I like to use the tmap package, however there is a wide range of options available. Maybe I will write a post on mapping with tmap one day… we will see.\n\n\n\nCode\n  #create a simple map\n  single_map &lt;- tm_shape(sub_set_of_sites) +\n    tm_dots(shape = \"Site\", size = 1, col = \"Site\", fill = \"Site\") +\n    tm_text(\"Site\", size = 2, ymod = 1) +\n    tm_layout(legend.show = F)\n\n\n\nThe final step of our for loop is to save the output of the loop somewhere. This step can catch alot of people off guard, they write the perfect loop, they check everything runs properly, and then they forgot to save the output each loop. Shame.\n\nIn my case, I have put the map into the list that we defined earlier. Notice that because I named each item in the list, I can then place the map under the correct item using “i”.\n\n\nCode\n  #add each map to the list\n  list_of_maps[[i]] &lt;- single_map\n  \n}"
  },
  {
    "objectID": "posts/ereefs_mapping_data/index.html",
    "href": "posts/ereefs_mapping_data/index.html",
    "title": "Creating Beautiful Maps Using eReefs Data",
    "section": "",
    "text": "This is part three of a series of blog that focus on eReefs and the data it provides. To follow along with this blog you will need to be able to download data from eReefs, you can learn how to do that in my first blog; The Extraction of Highly Specialised Modeled Data from eReefs. I would also recommend that you check my blog about Plotting eReefs Data, however it is not essential reading for this post.\nIn this post I would like to explore the spatial aspect of eReefs data. Together we will learn some of the most important steps when working with this type of data as we:\n\nManipulate and transform spatial data,\nUnderstand how to visualise the data, and\nBuild a series of informative maps"
  },
  {
    "objectID": "posts/ereefs_mapping_data/index.html#layer-manipulation",
    "href": "posts/ereefs_mapping_data/index.html#layer-manipulation",
    "title": "Creating Beautiful Maps Using eReefs Data",
    "section": "3.1 Layer Manipulation",
    "text": "3.1 Layer Manipulation\nBefore we can get right into analysis or mapping we need to get a better understanding of the data structure and what we are looking at. Simply by calling the object we can already get a pretty good breakdown:\n\n\nCode\n#get a summary of the data\nexample_data\n\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nChla  0.04715918 0.2284472 0.3236666 0.3413846 0.4579611 0.8344838 57762\ndimension(s):\n     from  to                  offset  delta                refsys x/y\nx       1 161                  408823   1426 GDA2020 / MGA zone 55 [x]\ny       1 179                 8051625  -1288 GDA2020 / MGA zone 55 [y]\ntime    1 365 2020-07-01 02:00:00 UTC 1 days               POSIXct    \n\n\nA couple of things to point out here.\n\nThe object is a “stars object”, this is a class introduced by the stars package 1.1 stars objects hold attribute(s) and dimensions 1.1.1 attributes are our values (e.g. chlorophyll a), and there can be more than one 1.1.2 dimensions are our lat, long, depth, time, spectral band, etc. 1.2 This means stars objects can be n-dimensional and hold multiple attributes - which is a lot to think about\nWe can see the summary statistics for our attribute (chlorophyll a)\nWe can also see some information about our dimensions\n\nTo see the names of our dimensions we can use dim().\n\n\nCode\n#view the dimension names and lengths\ndim(example_data)\n\n\n   x    y time \n 161  179  365 \n\n\nand to see the names of our attributes we can use names().\n\n\nCode\n#view the names of the attributes\nnames(example_data)\n\n\n[1] \"Chla\"\n\n\nWhen we look to analyse our data we are going to have to think about all of our dimensions and any attributes. The simplest way to interact with each of these is using the [] square brackets. The first element in the brackets corresponds to the attributes, and each element following this is one of the dimensions (in the order in which you see them using dim()): stars_object[att, i, j, k, time].\n\nIf we wanted to get just the first time step, we would write stars_object[], , , , 1] where the blank entry just means give me all of it.\nIf we wanted the 2nd i, the 1st-5th j, and the 3rd time step, we would write stars_object[, 2, 1:5, ,3]\nIf we have more than one attribute we can call that by name in the first argument stars_object[att,,,,]\n\nIn this way we have a cursory method of manipulating the data, and can use this to squeeze out our first map:\n\n\nCode\n#make a simple palette using our website colours\nmy_pal &lt;- c(\"#A7C3C7\", \"#7EA0A7\", \"#55807D\", \"#2D6056\", \"#00402F\", \"#00252A\")\n\n#create a simple map of the data\ntm_shape(example_data[\"Chla\",,,1]) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nNot bad, the shape might seem a bit odd but this is a result of the boundaries we originally used to extract the data. (Check the eReefs Extraction blog if you are curious). And just to be clear - if we don’t select just 1 time step we would get several maps:\n\n\nCode\n#create a simple map of the data\ntm_shape(example_data[\"Chla\",,,1:4]) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nHowever trying to do all of our analysis and manipulation this way would be very painful. Thankfully stars objects work with most tidyverse functions.\nWhen we use the tidyverse method, knowing the exact names of our dimensions and attributes is the key for layer manipulation rather than their specific order. For example, if we wanted to once again extract one time layer of data, it is as easy as specifying the dimension we want to slice (“time”), and the slice number:\n\n\nCode\n#slice to get a single time step\nsingle_timestep &lt;- example_data |&gt; \n  slice(time, 1)\n\n#slice to get multiple time steps\nmulti_timestep &lt;- example_data |&gt; \n  slice(time, 1:10)\n\n\n\n\nCode\n#create a simple plot of the data\ntm_shape(single_timestep) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nAlthough one downside here is that if you want to slice on multiple dimensions the calls must be run separately:\n\n\nCode\n#slice by latitude and time, not the \nslice_of_lat_and_time &lt;- example_data |&gt; \n  slice(x, 1:30) |&gt; \n  slice(time, 1)\n\n\n\n\nCode\n#visualise the slice of lat and time \ntm_shape(slice_of_lat_and_time) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nAs we can seem using slice() just about covers all of our layer manipulation needs without much work… Layer aggregation is not going to be so easy…\n\n\n\n\n\n\nNote\n\n\n\nThere are a few other functions from the tidyverse that can be used such as filter(), pull(), mutate(), and select(), but we won’t worry about those here, we will just focus on slice()."
  },
  {
    "objectID": "posts/ereefs_mapping_data/index.html#layer-aggregation",
    "href": "posts/ereefs_mapping_data/index.html#layer-aggregation",
    "title": "Creating Beautiful Maps Using eReefs Data",
    "section": "3.2 Layer Aggregation",
    "text": "3.2 Layer Aggregation\nAs we explored above we have quite a few time steps, too many to plot all of them. Our initial solution to be able to create a map was to simply slice out a few layers, but obviously this is not a good solution if we are trying to learn something about the entire dataset. Instead, a common method to deal with this kind of problem (too much data) is to aggregate the data into a workable size.\nThere are two main ways to do this, the first method is to use st_apply() - this method is more general purpose and gives you greater control, it can apply all sorts of function and is not just limited to reducing dimensions. The second method is to use aggregate() - this method is easier to use, but has limits on what it can achieved. We will cover both as the more complicated method gives a very helpful conceptual grasp of the data.\n\n3.2.1 st_apply()\nThe st_apply() function has three main arguments we are going to focus on:\n\nX (the stars object),\nMARGIN, and\nFUN (the function to apply)\n\nArguments 1 and three are pretty self explanatory, but MARGIN is a bit more confusing so I have drawn up some diagrams to help the explanation. Lets first look at a conceptual diagram of our data.\n\nIn this diagram we can see each of our dimensions represented (latitude, longitude, depth, and time), and our attribute would be the value in the cell. Also note that for this diagram we have included multiple depth layers, but our actual data only has the one depth at the moment.\nWhat MARGIN does, is ask “where do you want to apply the function?” As in what dimension. The dimension that you supply is the dimension that is preserved. For our data there are four margins to choose from:\n1 = Latitude 2 = Longitude 3 = Depth 4 = Time\nIf we say MARGIN = 1, we are applying our function over latitude, and the resulting data will only retain the latitude dimension. It would look like this:\n See how all of the cells that share the same latitude, but have different longitudes, times, or depths, are all combined into the same group.\nIf we say MARGIN = 2, we are applying our function over longitude, and the resulting data will only retain the longitude dimension. It would look like this:\n\nThis time note that all cells that share the same longitude, but have different latitudes times, or depths, are all combined into the same group.\nMARGIN = 3 (depth) would look like this:\n\nand MARGIN = 4 (time) like this:\n\nReasonably straight forward so far, but also largely unhelpful - none of these outputs retain data that is viable to be mapped. This is where things get a bit more intense, because you can actually supply multiple dimensions to the MARGIN argument, which allows for the preservation of multiple dimensions. For example, if we wanted to learn how our attribute changed as it moved offshore and how it changed over time we could say MARGIN = c(2,4) which would look like this:\n\nSee how both the time and the longitude dimensions are maintained, and only the latitude and depth dimensions are grouped up.\nBut probably the one we are most interested in is if we set MARGIN = c(1,2) (Latitude and Longitude) which would collapse the time and depth variables leaving us with one raster:\n\nNote this time the depth and time dimensions are aggregated.\nOne final thing to note with the MARGIN argument is that while it can take numeric inputs, it can also take the names of the dimensions. So instead of saying MARGIN = c(1,2) we could instead say MARGIN = c(\"x\",\"y\") to be a bit more clear about what we are doing.\nIn fact, this is what we will do right now. Note the dimensions of our dataset before the function is run:\n\n\nCode\n#look at dimensions\nexample_data\n\n\nstars object with 3 dimensions and 1 attribute\nattribute(s), summary of first 1e+05 cells:\n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nChla  0.04715918 0.2284472 0.3236666 0.3413846 0.4579611 0.8344838 57762\ndimension(s):\n     from  to                  offset  delta                refsys x/y\nx       1 161                  408823   1426 GDA2020 / MGA zone 55 [x]\ny       1 179                 8051625  -1288 GDA2020 / MGA zone 55 [y]\ntime    1 365 2020-07-01 02:00:00 UTC 1 days               POSIXct    \n\n\nversus after:\n\n\nCode\n#take the mean over time\nchl_a_mean &lt;- st_apply(example_data, \n                       c(\"x\",\"y\"),\n                       mean)\n\n#look at dimensions\nchl_a_mean\n\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n           Min.    1st Qu.    Median      Mean   3rd Qu.      Max.  NA's\nmean  0.0678172 0.09142314 0.1768633 0.1596316 0.1922283 0.3919046 17086\ndimension(s):\n  from  to  offset delta                refsys x/y\nx    1 161  408823  1426 GDA2020 / MGA zone 55 [x]\ny    1 179 8051625 -1288 GDA2020 / MGA zone 55 [y]\n\n\nAs we explained above, only the latitude and longitude dimensions remain. What we did was apply the mean function to the data, where the data is grouped by latitude and longitude (collapsing depth and time) to form pools of data to get the mean from. There is then one mean value for each latitude * longitude pair and we are left with a map that looks like this:\n\n\nCode\n#create a simple plot of the data\ntm_shape(chl_a_mean) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nCongratulations, using this method we now have a way of aggregating our data - i.e. by getting the mean of all the data into a single spatial layer. But more importantly we now have a very good conceptual understanding of our data, and we also know how we would apply some really complicated functions across different dimensions. This is extremely useful when you move on to more in depth spatial analysis.\nUnfortunately the single layer we aggregated to above doesn’t cut it. It returns an annual overview that doesn’t really tell us too much other than what locations have consistently higher chlorophyll a. No, instead we want to learn something about seasonal or monthly trends. To do this we need to provide some kind of indication to st_apply() that we want multiple groups.\nThis is achieved using the following steps:\n\nExtract a table that contains the date and time of each layer\nGroup the individual layers by month and find the first and last layer per month:\n\n\n\nCode\n#extract a table that contains the date and time of each layer\ntime_table &lt;- data.frame(DateTime = st_get_dimension_values(example_data, \"time\"))\n\n#extract the year and month into their own columns, add a column that counts the row number\ntime_table &lt;- time_table |&gt; \n  mutate(Year = year(DateTime),\n         Month = month(DateTime),\n         RowId = row_number()) \n\n#combine the year and month columns\ntime_table &lt;- time_table |&gt; \n  unite(YearMonth, \"Year\", \"Month\", sep = \"_\")\n  \n#group by the YearMonth column and get the min and max row index (layer number) for each month, order by index number\ntime_table &lt;- time_table |&gt; \n    group_by(YearMonth) |&gt; \n    summarise(MinIndex = min(RowId),\n              MaxIndex = max(RowId)) |&gt; \n    arrange(MinIndex)\n\n#visualise the data\nhead(time_table)\n\n\n# A tibble: 6 × 3\n  YearMonth MinIndex MaxIndex\n  &lt;chr&gt;        &lt;int&gt;    &lt;int&gt;\n1 2020_7           1       31\n2 2020_8          32       62\n3 2020_9          63       92\n4 2020_10         93      123\n5 2020_11        124      153\n6 2020_12        154      184\n\n\n\nUse slice() to extract all the layers per month\nUse st_apply() to apply the mean function to all the layers in the month\nPut this inside a map2() function to run the code for each month at the same time:\n\n\n\nCode\n#use map to work through each start-end index and use st_apply to apply the mean\nmonthly_chla &lt;- map2(time_table$MinIndex, time_table$MaxIndex, function(a,b) {\n  \n  #apply mean to the data slice\n  st_apply(slice(example_data, time, a:b),\n           MARGIN = c(\"x\",\"y\"), #using margin x and y to keep lat and long information\n           FUN = mean,\n           na.rm = T,\n           keep = T)\n       \n})\n\n\n\nCombine the list output back into a single stars object:\n\n\n\nCode\n#bind the output into a single stars object. Note there are two \"c\"s here. The first (left) one binds the args. The second (right one) provides the args (list of stars object) plus the final argument (along = \"time\") which tells the first c to bind along a new dimension.\nmonthly_chla &lt;- do.call(c, c(monthly_chla, along = \"time\"))\n\n\nDone! We can then visualise the data to confirm it worked:\n\n\nCode\n#create a simple plot of the data\ntm_shape(monthly_chla) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nSeems good to me! Although we would likely have to fix up those layer names/dates.\n\n\n\n\n\n\nNote\n\n\n\nThe st_apply() function is not limited to just the mean() function, or even just simple functions at all. It can take in any custom function that you write - provided it has been written to work with matrices. For example, you could run the min() function to get a map that shows the min value at each cell, or if your data has spectral bands you could write a function to calculate the NDVI value for a vegetation assessment. The possibilities are endless!\n\n\n\n\n3.2.2 aggregate()\nAs noted earlier, the aggregate() function is a much simpler method for aggregating a stars object and returning data with a lower spatial or temporal resolution. This function works in a similar way, it also has three main arguments:\n\nX (the stars object),\nby, and\nFUN (the function to apply)\n… (additional arguments such as na.rm = T)\n\nAgain, arguments 1 and 3 are self explanatory, but the second argument is not. The “by” argument takes either an sf object (a spatial object) to do spatial aggregation, or a vector of grouping values. The sf object is fairly simple, it acts similar to a mask - anything inside the object is part of the group, anything outside is not. The vector is a bit more flexible, it could be a vector of time values - for temporal aggregation, or it could be a vector of latitude values for spatial aggregation, or a vector of longitude values for spatial aggregation. What it can’t be is more than one of those things, if you want a combination you must use the st_apply() method. To be fair, I cannot think of a single reason why you would want to supply a lat/long value for aggregation this way when st_apply is so much better, so we will effectively treat the “by” argument as either an sf object, or a time vector.\nLets first demonstrate this with a spatial object, for this we are going to need to load in an sf object, so lets just use the one we originally used to extract the data:\n\n\nCode\n#read in the dry tropics region dataset and update crs to projected cords\ndt_region &lt;- st_read(\"dt_region.gpkg\") |&gt; \n  st_transform(\"EPSG:7855\")\n\n\nReading layer `dt_region' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\ereefs_mapping_data\\dt_region.gpkg' \n  using driver `GPKG'\nSimple feature collection with 32 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 146.1444 ymin: -19.70039 xmax: 148.2985 ymax: -17.62597\nGeodetic CRS:  GDA2020\n\n\nCode\n#demonstrate the aggregate function with an sf object\nagg_example &lt;- aggregate(example_data,\n                         dt_region,\n                         mean,\n                         na.rm = T)\n\n#create a simple plot of the data\ntm_shape(agg_example[,,1]) +\n  tm_polygons(fill = \"Chla\", \n              fill.scale = tm_scale_intervals(n = 6,\n                                              values = my_pal,\n                                              label.format = list(digits = 2)),\n              fill.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nWhich is kind of interesting as we can see that there must be a slight overlap between land and marine for those land polygons to contain values. However, generally I find I don’t use this method all that often - despite really wanting to find reasons too.\nOf course, the other options is the temporal vector. This actually has some handy short cuts where you can supply a vector of time values, or just a simple string like “months”, or “5-days”, etc. For our purposes we will use the string “months” which seems to work just fine:\n\n\nCode\n#this aggregates data by month\nagg_example_2 &lt;- aggregate(example_data,\n                           by = \"months\",\n                           mean)\n\n\nHowever due to weirdness inside the function before we can visualise the output we need to now fix the dimension values as they are out of order. Specifically, after the aggregation they are:\n\n\nCode\n#look at the dimensions of the object\ndim(agg_example_2)\n\n\ntime    x    y \n  12  161  179 \n\n\nWhile we need them to be:\n\n\nCode\n#reorder dimensions\nagg_example_2 &lt;- aperm(agg_example_2, c(\"x\", \"y\", \"time\"))\n\n#look at dimensions\ndim(agg_example_2)\n\n\n   x    y time \n 161  179   12 \n\n\nOnce reordered, we can then visualise just fine:\n\n\nCode\n#create a simple plot of the data\ntm_shape(agg_example_2) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T))\n\n\n\n\n\n\n\n\n\nAnd look at that, we now have 12 layers with monthly mean concentration values, which much less effort than st_apply(), cool! However it should be noted that we also have much less control over this method, for example if we had ver specific date ranges, or lat and long values it might be a better idea to use the st_apply() function."
  },
  {
    "objectID": "posts/automated_output_organisation/index.html",
    "href": "posts/automated_output_organisation/index.html",
    "title": "Automate Your Output Folders",
    "section": "",
    "text": "1 Introduction\nBeing organised is hard work, no doubt about it. Being consistent is hard work, no doubt about it. But automating your output folders to be both organised and consistent? Well that’s easy, and today I’ll be showing you how.\nLets set a scene that I’m sure many of you have encountered. You create a new repository for your next work task or personal project, initially you take your time, you’re careful, you organise everything neatly. But time goes on and competing priorities appear, suddenly you are in a rush, naming falls to the wayside and you just need to get things done. One day you look back at your work and you realise you have created a monster, your results folder probably looks something like this…\n\nand your scripts folder a bit like this:\n\n… its a sight to behold…\n\n\n\n\n\n\nNote\n\n\n\nI will give you $100 if you successfully match the output(s) to the script that produce it.\n\n\nconceptually, you could represent the above arrangement like this:\n\nWith every script and every output all dumped into the project directory folder.\n\n\n2 The Manual Fix\nNot to worry, to keep things organised you put your scripts into a scripts folder. Maybe you have so many scripts that you also create sub folders!:\n\n\n\nVariation 1\n\n\n\n\n\nVariation 2\n\n\nhowever, in both these cases, the outputs are still getting dumped directly into the project directory. The natural next step here is to create a folder that stores all of the outputs:\n\nOr, if you have sub folders it could look like this:\n\nBut that’s hard work! And as we covered above, overtime the maintenance on these structures fall away and get forgotten.\n\n\n3 The Automated Solution\nThankfully there is an easy fix, a little something called getting the active document context. Effectively what we are going to do is create a function that we can run at the start of every single script. This function will get information about the active document (i.e. the script being run), and use this to create a series of folders and paths that perfectly mirror the path to the script.\nFor example by running rstudioapi::getActiveDocumentContext() we return a bit of information about the path of the script, the id, and where the mouse is. We can use this to our advantage by changing the function to rstudioapi::getActiveDocumentContext()$path and just selecting the path. This will return something like this:\n“~/GitHub/scripts/posts/automated_output_organisation/climate_final_report.qmd”\nFollowing this, we can isolate the specific name of the script using a little bit of stringr() magic, which leaves us with the string “climate_final_report”. Next, we can inject this string into the dir.create() function and hey presto the function automatically creates a folder with the exact same name as the script, right next to our script. Then we can just save the path to this folder as an object that we can reference whenever we want to save one of our outputs.\nThe bones of this function are presented below:\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load in the required libraries\n  library(rstudioapi)\n  library(stringr)\n  \n  #get the file path and name of the active document\n  script_path &lt;- getActiveDocumentContext()$path\n    \n  #remove the unnecessary components of the file path\n  output_path &lt;- str_remove_all(script_path, \".*/|.qmd|.R\")\n  \n  #create a folder at the location\n  dir.create(output_path)\n  \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nHowever, there are a two key issues with this set up so far:\n\nThis function only works when you manually run an R script or quarto document, it fails when you try to render a quarto document (bit too complicated to cover why here).\nThis function is currently assuming the output folder should be created in the same folder that the script is located - in our examples above this would only work in the very first conceptual diagram.\n\nThe first issue is a relatively simple fix, we just need to include the “quarto render” version of the rstudioapi::getActiveDocumentContext() function, which is knitr::current_input(). Below we update the function to first try rstudioapi::getActiveDocumentContext() , and if that fails, switch to knitr::current_input():\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load in the required libraries\n  library(rstudioapi)\n  library(stringr)\n  \n  #try the first method, if it fails \"result\" becomes an object of class \"try-error\"\n  result &lt;- try({\n  \n    #get the file path and name of the active document\n    script_path &lt;- getActiveDocumentContext()$path\n      \n    #remove the unnecessary components of the file path which is everything left of the last /, and the \".qmd\" or \".R\"\n    output_path &lt;- str_remove_all(script_path, \".*/|.qmd|.R\")\n    \n  }, silent = TRUE)\n  \n  #if the result object became a class \"try-error\" then we attempt the second method\n  if (inherits(result, \"try-error\")){\n    \n    #get the file path and name of the active document\n    script_path &lt;- knitr::current_input()\n    \n    #remove unnecessary components of the file path\n    output_path &lt;- str_remove_all(script_path, \".rmarkdown\")\n  }\n   \n  #create a folder at the location\n  dir.create(output_path)\n    \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nHowever the second issue is a bit more abstract as it depends on your exact folder structure. As we covered above, your scripts might be stored in a “scripts/” folder, or maybe there are layers of sub folders.\nThe best solution I have found for this is to mirror the script folder organisation into a newly created “outputs” folder. This is achieved by modifying the function to use here(). What here() does is return a path to the R project directory that the script is being run within (i.e. the parent folder in our conceptual diagram). By combining the here() function with our custom function above we can isolate the exact path from the parent folder of the project to the script being run, and then mirro this to create the folder and path structure for our outputs folder. For example, if we use rstudioapi::getActiveDocumentContext() and the path to the script is:\n“~/GitHub/website/scripts/automated_output_organisation/climate_final_report.qmd”\nand the here() function returns:\n“C:/Users/adams/XYZ/Documents/GitHub/website”\nwe can first identify that the parent folder is the final phrase from the here() function, which in our case is “website”. Following this, we can then see that everything between “website” and the file type from the rstudioapi::getActiveDocumentContext() function must be the folder(s) in which the R script is stored and the name of the script. In our case this would be:\n“scripts/automated_output_organisation/climate_final_report/”\nfinally, we just take this string and substitute any text before the first slash (/) with “output” to create:\n“output/automated_output_organisation/climate_final_report/”\nConceptually this would look almost exactly the same as our previous diagram, except that we have created an additional folder with the same name as the script to provide clarity on the source of the output, and that all of highlighted folders were created automatically:\n\nThe code to achieve this is as follows:\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load the required packages\n  library(rstudioapi)\n  library(stringr)\n  library(here)\n  library(glue)\n  \n  #try the first method, if it fails \"result\" becomes an object of class \"try-error\"\n  result &lt;- try({\n  \n    #get the file path and name of the active document\n    script_path &lt;- getActiveDocumentContext()$path\n\n    #get the name of the folder that the R project sits in. Regex is to grab everything after the last slash\n    parent_folder_name &lt;- str_extract(here(), \"[^/]+$\")\n    \n    #get everything after the parent folder, note this may be several sub folders.\n    sub_folder_names &lt;- str_extract(script_path, glue(\"(?&lt;=/{parent_folder_name}/).*\"))\n    \n    #replace the very first subfolder with \"output\" this will split any additional subfolders off down the output folder chain\n    output_path &lt;- str_replace(sub_folder_names, \"^[^/]+\", \"output\")\n    \n    #drop the \".qmd\" or \".R\" off the end\n    output_path &lt;- str_remove_all(output_path, \".qmd|.R\")\n\n  }, silent = TRUE)\n  \n  #if the result object became a class \"try-error\" then we attempt the second method\n  if (inherits(result, \"try-error\")){\n    \n    #get the file path and name of the active document\n    script_path &lt;- paste0(getwd(), \"/\", knitr::current_input())\n    \n    #get the name of the folder that the R project sits in. Regex is to grab everything after the last slash\n    parent_folder_name &lt;- str_extract(here(), \"[^/]+$\")\n    \n    #get everything after the parent folder, not this may be several sub folders.\n    sub_folder_names &lt;- str_extract(script_path, glue(\"(?&lt;=/{parent_folder_name}/).*\"))\n    \n    #replace the very first subfolder with \"output\" this will split any additional subfolders off down the output folder pathway\n    output_path &lt;- str_replace(sub_folder_names, \"^[^/]+\", \"output\")\n    \n    #remove unnecessary components of the file path\n    output_path &lt;- str_remove_all(sub_folder_names, \".rmarkdown\")\n    \n  }\n   \n  #create a folder at the location\n  dir.create(here(output_path), recursive = TRUE)\n    \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nAnd thats it! You can save this function, call it at the start of any script you write, and it will automatically create the required output folders for you to store all of the awesome work you create. To access the path to the created folders all you need to do is call the “output_path” object.\n\n\n4 When Is This Useful When Is it Not?\nBroadly speaking, you will find this function useful when the repository you are working in is high traffic, and/or has a lot of scripts and outputs. If you have several scripts that produce outputs with similar names this is also a life save.\nConversely, this function is certainly overkill for a simple repository that only contains one or two scripts.\n\n\n5 Extensions\nThere are a few natural extensions to this function that I have also implemented in my day to day. The most obvious of which is creating another mirrored folder structure for my input data to be stored. This is particularly helpful when the data is automatically downloaded and then stored for later retrieval.\nTo the contrary, if you manually access and store your data this extension might not work so well for you - since you would have to be manually creating the folders to store the data to begin with."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, Im Adam.",
    "section": "",
    "text": "Document\n\n\n\n  \n    \n      \n      An environmental data analyst decoding nature's secrets.\n      \n      \n       With experience in R, Tableau, SQL and GIS, I transform raw environmental data into meaningful narratives with stunning visuals. From crafting data-driven solutions to fostering sustainability, I am dedicated to bridging the gap between technology and the environment."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "ADAM SHAND",
    "section": "",
    "text": "Document\n\n\n\n  \n    \n      \n    \n      Download Current CV"
  },
  {
    "objectID": "posts/an_opinionated_dataframe_cleaner/index.html",
    "href": "posts/an_opinionated_dataframe_cleaner/index.html",
    "title": "An Opinionated Dataframe Cleaner",
    "section": "",
    "text": "1 Introduction\nInconsistent and illogical naming conventions can ruin even the best analysts flow, cause sneaky errors, and potentially lead to misleading or completely incorrect results. Throughout my time as an environmental data analyst I have come across countless instances where the names used in a dataframe mess up my analysis, and I can guarantee I’m not the only one. Just Google “the importance of file naming” to find countless monologues (just like this one), or “bad naming conventions” to realize, actually it could be worse!\nSo if this is such a widely acknowledged issue, why is it still an issue? How has it not been fixed? Simply put, because a) its boring, and b) everyone is unique and has their own idea of what a “good” system looks like. This leads to people not bothering, or instances where you might pull together several datasets from a range of sources, each using their own (different) naming conventions. Thankfully, if each dataset is at least internally consistent, we can address these differences.\nBelow, I introduce my method of addressing this issue. It is a highly opinionated dataframe cleaner that focuses exclusively on ensuring every dataframe I touch receives exactly the same column naming convention. Before we dive into it, I believe it is critical to recognise that this method is customized to my needs, it may work for you as well, but I recommend instead that you use this as inspiration to develop your own method.\n\n\n2 The Naming Convention\nSo what naming convention am I using exactly? “Upper Camel Case” is my choice, however some people may also refer to it as “Pascal Case”. If your are unfamiliar, here are some examples of naming conventions:\n\nUpperCamelCase\nsnake_case\nkebab-case\nUPPERFLATCASE\netc.\n\nWhy UpperCamelCase? As noted above, everyone has their own idea of what is good. I find that upper camel case suite my purposes well, it is fairly easy to read, it only contains A-Z, 0-9 (no underscores or dashes), and most importantly it does not clash with my object names when coding it in R. What I mean by this is that I use snake_case to name my objects, and UpperCamelCase to name columns within my objects. Lets consider the following example.\nLets say I have a dataframe that counts fish (called “fish”):\n\n\nCode\n#load the dplyr package\nlibrary(dplyr)\n\n#create an example dataframe\nfish &lt;- data.frame(species = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                   fish_count_location_1 = c(6,9,3,5,10),\n                   fish_count_location_2 = c(1,16,3,2,7))\n\n#print the dataframe. If you want to learn about this function, check out my pretty tables post!\ncond_form_tables(fish)\n\n\n\n\nspeciesfish_count_location_1fish_count_location_2\n\nA61\n\nB916\n\nC33\n\nD52\n\nE107\n\n\n\n(Note that both the object and column names are in snake_case).\nThen I decide to figure out the mean number of each species of fish, across all locations (called “mean_fish”):\n\n\nCode\n#get the rowwise mean of the fish counts per species\nmean_fish &lt;- fish |&gt; \n  rowwise() |&gt; \n  mutate(mean_fish = mean(c(fish_count_location_1, fish_count_location_2))) |&gt; \n  ungroup()\n\n#print the dataframe\ncond_form_tables(mean_fish)\n\n\n\n\nspeciesfish_count_location_1fish_count_location_2mean_fish\n\nA613.5\n\nB91612.5\n\nC333  \n\nD523.5\n\nE1078.5\n\n\n\nWhoops, just by using some logical naming I now accidentally have a dataframe object named “mean_fish”, and a column within that dataframe named “mean_fish”. Now obviously this is a silly example, but imaging we have 1000+ lines of code, and we need to know something about the mean number of fish. Suddenly we can’t remember whats an object and whats a column and we can run into subtle errors, or have very confusing lines of codes.\nThus; my final reason for choosing UpperCamelCase:\n\n\nCode\n#create a new example dataframe\nfish &lt;- data.frame(Species = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                   FishCountLocation1 = c(6,9,3,5,10),\n                   FishCountLocation2 = c(1,16,3,2,7))\n\n#get the rowwise mean of the fish counts per species\nmean_fish &lt;- fish |&gt; \n  rowwise() |&gt; \n  mutate(mean_fish = mean(c(FishCountLocation1, FishCountLocation2))) |&gt; \n  ungroup()\n\n#print the dataframe\ncond_form_tables(mean_fish)\n\n\n\n\nSpeciesFishCountLocation1FishCountLocation2mean_fish\n\nA613.5\n\nB91612.5\n\nC333  \n\nD523.5\n\nE1078.5\n\n\n\n\n\n3 The Function\nMy custom function takes advantage of the janitor R package, which includes a wide range of functions to perform standard cleaning and organisation steps (check out the janitor documentation to see what it can do). Specifically, we are going to use the clean_names() function, along with some bells and whistles to catch our edge cases. Lets take a look:\n\n\nCode\n#create the custom function\nname_cleaning &lt;- function(df){\n\n  #load and install (if required) the pacman package handler package, which we will use for all future package downloads\n  if(!require(\"pacman\")){install.packages(\"pacman\")}\n\n  #use the pacman function to load and install (if required) all other packages\n  pacman::p_load(janitor, dplyr, sf, stringr)\n\n  #check if the df is an sf object and if so, apply clean names to every column but the last column\n  if(inherits(df, \"sf\")){\n    \n    #convert all but the geometry column to upper camel type\n    df_new &lt;- df |&gt; \n      st_drop_geometry() |&gt;\n      clean_names(case = \"upper_camel\")\n    \n    #bind the geometry column back on with its new name. Note that it should also be named \"geom\"\n    df_new &lt;- df_new |&gt;\n      dplyr::mutate(geom = st_geometry(df)) |&gt; \n      st_as_sf()\n  \n  } else {\n    \n    #convert ALL columns to upper camel type, don't have to worry about geometry\n    df_new &lt;- df |&gt; \n      clean_names(case = \"upper_camel\")\n    \n  }\n  \n  #for every character type column, run a encoding check and fix, then remove weird new line characters\n  df_new &lt;- df_new  |&gt; \n    mutate(across(where(is.character), ~ iconv(., from = 'UTF-8', to = 'ASCII//TRANSLIT'))) |&gt; \n    mutate(across(where(is.character), ~str_replace_all(., \"\\r\\n\", \" \")))\n  \n  return(df_new)\n  \n}\n\n\nOk, so even though that is a relatively short function, there is still a few things going on. Lets break it down a bit.\n\nFirst we will initialize the function (if you are unfamiliar with creating your own functions check out my functions post).\n\n\n\nCode\n#initialize the function\nname_cleaning &lt;- function(df){\n\n\n\nThen we load each of our required packages. Noting that generally we would expect these packages to already have been loaded in by the script calling this function, but we can’t be sure. Here we use the pacman package to make the install/load steps a bit more streamline, documentation for pacman can be found here.\n\n\n\nCode\n  #load and install (if required) the pacman package handler package, which we will use for all future package downloads\n  if(!require(\"pacman\")){install.packages(\"pacman\")}\n\n  #use the pacman function to load and install (if required) all other packages\n  pacman::p_load(janitor, dplyr, sf, stringr)\n\n\n\nWe then check if the dataframe we are cleaning is actually an “sf” (simple feature) object. Sf objects are special types of dataframes used in geospatial analytics that have an extra column containing coordinate information. This special column has its own rules for column naming and therefore sf objects should be handled differently. In my work I encounter sf objects very often.\n\n\n\nCode\n  #check if the df is an sf object and if so, apply clean names to every column but the last column\n  if(inherits(df, \"sf\")){\n\n\n\nIf we are looking at an sf object, we copy the sf object and remove the geometry column from this copy. Following this, we can then run janitor’s clean_names() function on the copy with no geometry column. The reason we do this is that the janitor package has no precedent for sf objects. In the clean_names() function, we specify that we want the column names to follow the “upper_camel” format. This will convert all our column names to the desired format.\n\n\n\nCode\n    #convert all but the geometry column to upper camel type\n    df_new &lt;- df |&gt; \n      st_drop_geometry() |&gt;\n      clean_names(case = \"upper_camel\")\n\n\n\nOnce we have cleaned the names of every column in the sf object, we can then add the special geometry column back on to the dataset. At this point we also need to convert the object back to the “sf” type.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that this special geometry column is called “geom” rather than “Geom”… which doesn’t adhere to our naming convention. Unfortunately, this is an annoying quirk of spatial datasets. When they are loaded, the geometry column can take on 1 of 3 different names depending on the source of the data; “geom”, “geometry”, or “shape”. In all cases the name is lowercase, even when the data is saved in uppercase, it will be reloaded in lowercase. Thus, for this issue, we simply ensure that the 3 different possibilities are all just converted to the “geom” option.\n\n\n\n\nCode\n    #bind the geometry column back on with its new name. Note that it should also be named \"geom\"\n    df_new &lt;- df_new |&gt;\n      dplyr::mutate(geom = st_geometry(df)) |&gt; \n      st_as_sf()\n\n\n\nIf the object is a simple dataframe (not an sf object), we can just move straight to the clean_names() step that we explained above.\n\n\n\nCode\n  } else {\n    \n    #convert ALL columns to upper camel type, don't have to worry about geometry\n    df_new &lt;- df |&gt; \n      clean_names(case = \"upper_camel\")\n    \n  }\n\n\n\nNext we look to catch strange edge cases related to the encoding column of columns. You are likely familiar with the concept of a column being of type “character” or “numeric” or “boolean”, etc. Our strange edge case is similar to this. What we have found is that in some instances the character column type is encoded as “UTF-8”, while other times it is encoded as “ASCII”. Much like how you can’t combine character and numeric columns, you also can’t combine columns encoded as UTF-8 and ASCII. Below we convert all columns encoded as UTF-8 to ASCII to avoid this issue.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that these encodings are hidden from the user and you will never normally need to interact with them, the reason this happens doesn’t matter, and is frankly some mysterious property of excel. Broadly, you probably don’t need to ever understand why/how this step works.\n\n\n\n\nCode\n  #for every character type column, run a encoding check and fix, then remove weird new line characters\n  df_new &lt;- df_new  |&gt; \n    mutate(across(where(is.character), ~ iconv(., from = 'UTF-8', to = 'ASCII//TRANSLIT'))) |&gt; \n    mutate(across(where(is.character), ~str_replace_all(., \"\\r\\n\", \" \")))\n\n\n\nThe object is then returned and the function is complete.\n\n\n\nCode\n  return(df_new)\n  \n}\n\n\n\n\n4 In Practice\nNow that we understand how the function works, lets demonstrate its use with another example dataset that has a wide range of column names. Here is before:\n\n\nCode\n#create an example table with example names\nexample_df &lt;- data.frame(\"column 1\" = c(1,2,3,4,5),\n                         \"column-2\" = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                         \"column_3\" = c(NA, NA, NA, NA, NA),\n                         \"column-four\" = c(\"1A\", \"2B\", \"3C\", \"4D\", \"5E\"),\n                         \"Column Five\" = c(TRUE, FALSE, TRUE, FALSE, TRUE))\n\n#print the table\nprint(example_df)\n\n\n  column.1 column.2 column_3 column.four Column.Five\n1        1        A       NA          1A        TRUE\n2        2        B       NA          2B       FALSE\n3        3        C       NA          3C        TRUE\n4        4        D       NA          4D       FALSE\n5        5        E       NA          5E        TRUE\n\n\nAnd after:\n\n\nCode\n#run the clean name functions\nexample_df_cleaned &lt;- name_cleaning(example_df)\n\n#print the cleaned dataset\ncond_form_tables(example_df_cleaned)\n\n\n\n\nColumn1Column2Column3ColumnFourColumnFive\n\n1A1ATRUE\n\n2B2BFALSE\n\n3C3CTRUE\n\n4D4DFALSE\n\n5E5ETRUE\n\n\n\n\n\n5 Caveats\nIt is also important to acknowledge the caveats of your own work. To my knowledge the only caveat of this function is that it relies on a sensible preexisting column name, even if the format is horrible. What I mean by this is that a column named “Mean-fish_in Townsville” can be cleaned, but a column with no name… well how can you rename that to something appropriate? As a side note R does generally replace empty column names with “X1”, “X2”, etc. however this still does not provide any information about the column."
  },
  {
    "objectID": "posts/ereefs_extracting_data/index.html",
    "href": "posts/ereefs_extracting_data/index.html",
    "title": "The Extraction of Highly Specialised Modelled Data from eReefs",
    "section": "",
    "text": "In this blog post I’d like to cover the essentials of extracting data from the eReefs platform. First of all, hands up - who’s heard of eReefs? Fair enough if you haven’t. Despite its star power in my world it is still a relatively niche topic. To summarise, eReefs is “a comprehensive view of dynamic marine environment conditions across the Great Barrier Reef from end-of-catchments and estuaries to reef lagoons and the open ocean.” What this means for us is that it has a wide range of modelled environmental datasets that are relatively easy to access, backed by top-notch science, and heavy (i.e. we really get to flex our coding and spatial “muscles”).\nThe goal today is to learn how to:\n\nExtract data from eReefs - a surprisingly hard thing to do\nSave the data to file\nThat’s it - the data extraction section is already going to make this a long post.\n\nThere are lots of things we can do once we have got the data, some of these such as creating maps, or converting the data to a tabular format and creating plots are explored in my other blogs. Each of these blogs pick up right where this blog leaves off, and details exactly how you will need to transform and manipulate the data to achieve what you want. In particular, the mapping blog covers how to spatially and/or temporarily aggregate raster data, whilst the plotting blog focuses more on how to conver raster data into a more familar table, and the things you can do from there."
  },
  {
    "objectID": "posts/ereefs_extracting_data/index.html#step-1---obtain-facilitating-dataset",
    "href": "posts/ereefs_extracting_data/index.html#step-1---obtain-facilitating-dataset",
    "title": "The Extraction of Highly Specialised Modelled Data from eReefs",
    "section": "2.1 Step 1 - Obtain Facilitating Dataset",
    "text": "2.1 Step 1 - Obtain Facilitating Dataset\nThis is just about the only simple step in this blog.\nBelow I load in my boundaries of the Dry Tropics region, unfortunately this is a custom dataset that cannot be made available for download, however any polygon area within the Great Barrier Reef region will work so I encourage you to create your own.\n\n\nCode\n#read in the dry tropics region dataset and start with a lat/long crs\ndt_region &lt;- st_read(\"dt_region.gpkg\") |&gt; \n  st_transform(\"EPSG:7844\")\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn a pinch for coordinates? Use these for a simple box:\n\n\nCode\nlibrary(sf)\n\nexample_box &lt;- matrix(c(144.227, -24.445,   # bottom-left\n                        144.227, -15.195,   # top-left\n                        151.329, -15.195,   # top-right\n                        151.329, -24.445,   # bottom-right\n                        144.227, -24.445),    # close the polygon by repeating the first point\n                      ncol = 2, \n                      byrow = TRUE)\n\n#create a polygon geometry and set the CRS\nexample_box &lt;- st_polygon(list(example_box)) |&gt; \n  st_sfc(crs = \"EPSG:7844\")"
  },
  {
    "objectID": "posts/ereefs_extracting_data/index.html#step-2---getting-ereefs-data",
    "href": "posts/ereefs_extracting_data/index.html#step-2---getting-ereefs-data",
    "title": "The Extraction of Highly Specialised Modelled Data from eReefs",
    "section": "2.2 Step 2 - Getting eReefs Data",
    "text": "2.2 Step 2 - Getting eReefs Data\nI would first like to note that there are several resources online that contain useful information about accessing data from eReefs. These include:\n\nAn eReefs R Package\nMultiple eReefs Tutorials\nAnd, the National Computing Infrastructure’s (NCIs) THREDDS server\n\nHowever, I personally find that most of these resources either A) gloss over what is really happening behind the scenes, or B) don’t provide critical information needed for you to go away and conduct your own analysis (for example, how to see what indicators are available in eReefs). It is these reasons among others that prompted me to write this blog.\nAnyway, time to buckle your seatbelts, kids.\n\n2.2.1 Connecting to the Data\nThe first thing we are going to do today is establish a connection to the database. To do this we are going to need to load the ereefs() package as this contains the handy functions substitute_filename(\"catalog\") and get_ereefs_grids(). First, we will run the substitute_filename(\"catalog\") function, which will return a list of all the available datasets that we can choose from. This list is interactive and requires user input - I have picked “5” - “eReefs GBR1 biogeochemistry and sediments v3.2” as this is the most up-to-date model. However, there are older models and models that have been run under different scenarios if you are interested in those instead.\n\n\nCode\n#load in the ereefs package\nlibrary(ereefs)\n\n#run the function\nsubstitute_filename(\"catalog\")\n\n\nOnce we have made our selection it will return a url. This url is how we are going to connected to the correct database, if you are interested, this is a manual view of the range of data that we are choosing from.\nWe can these use this url as an argument in the get_ereefs_grids() function from the ereefs package.\n\n\nCode\n#manually assign the url from above into a variable (so I don't have to interact with the code)\ninput_file &lt;- \"https://dapds00.nci.org.au/thredds/dodsC/fx3/GBR1_H2p0_B3p2_Cfur_Dnrt.ncml\"\n    \n#get all grids\ngrids &lt;- get_ereefs_grids(input_file)\n\n\n\n\n2.2.2 Gaining Perspective\nIf the lines of code above worked, congratulations - you can access the data. Now take a look at the object grids and you will probably realise that we are only 3 lines of code in and things are already pretty intense - WTF is this object and what does it tell us?\nWhat we just did is get the dimensions of the dataset. The grids object should be a list of length 3, the 3 items in the list should be “x_grid”, “y_grid”, and “z_grid”, each of the these tell us something about one dimension of the data (the longitude, latitude, and depth). Unfortunately, because each of these items are bloody huge manually viewing the object to try and learn about it is essentially useless. Below we use some simple code to explore the grids.\n\n\nCode\n#extract just the x grid\nx_grid &lt;- grids[[\"x_grid\"]]\n\n#get some basic information about the dataset\nxmin &lt;- min(x_grid, na.rm = T)\nxmax &lt;- max(x_grid, na.rm = T)\nx_dim &lt;- dim(x_grid)\n\n\nThe x_grid tells us about longitude. The min x value is 142.018379, the max x value is 155.379686, and the dimensions of the x_grid are 511, 2390.\n\n\nCode\n#extract just the y grid\ny_grid &lt;- grids[[\"y_grid\"]]\n\n#get some basic information about the dataset\nymin &lt;- min(y_grid, na.rm = T)\nymax &lt;- max(y_grid, na.rm = T)\ny_dim &lt;- dim(y_grid)\n\n\nThe y_grid tells us about latitude. The min y value is -28.600577, the max x value is -7.386315, and the dimensions of the y_grid are 511, 2390.\nBy looking at the x and y values we can get an idea of where we are in the world:\n\n\nCode\nas.data.frame(head(grids[[\"y_grid\"]], n = c(5,5)))\n\n\n         V1        V2        V3        V4        V5\n1 -28.59505 -28.57945 -28.56385 -28.54808 -28.53231\n2 -28.59506 -28.57942 -28.56378 -28.54800 -28.53222\n3 -28.59508 -28.57940 -28.56371 -28.54792 -28.53214\n4 -28.59510 -28.57938 -28.56367 -28.54787 -28.53206\n5 -28.59511 -28.57937 -28.56362 -28.54781 -28.53199\n\n\nCode\n#create a bbox\nereefs_extent_bbox &lt;- matrix(c(xmin, ymin,   # bottom-left\n                               xmin, ymax,   # top-left\n                               xmax, ymax,   # top-right\n                               xmax, ymin,   # bottom-right\n                               xmin, ymin),    # close the polygon by repeating the first point\n                      ncol = 2, \n                      byrow = TRUE)\n\n#create a polygon geometry and set the CRS\nereefs_extent_bbox &lt;- st_polygon(list(ereefs_extent_bbox)) |&gt; \n  st_sfc(crs = \"EPSG:7844\")\n\ntm_shape(World) +\n  tm_polygons() +\n  tm_shape(ereefs_extent_bbox) +\n  tm_polygons(col = \"red\",\n              fill = NULL)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#extract just the z grid\nz_grid &lt;- grids[[\"z_grid\"]]\n\n#get some basic information about the dataset\nz_min &lt;- min(grids[[\"z_grid\"]], na.rm = T)\nz_max &lt;- 0 #using max returns the \"wrong\" value for our discussion\nz_dim &lt;- dim(grids[[\"z_grid\"]])\n\n\nThe z_grid tells us about depth (eReefs models the entire water column). The min z value is -4000m, the max x value is 0m, and the dimensions of the z_grid are 45. These values tell us at what depth each layer of the model is at, and how many layers there are.\nIn combination these three grids tell us everything we need to know about the data. Let’s first look at the x_grid, as we noted above, the dimensions of the x_grid are 511, 2390, thus picture a table that has 511 rows, and 2390 columns. Once again, here is a snapshot of the first five rows and columns of the grid:\n\n\nCode\nas.data.frame(head(x_grid, n = c(5,5)))\n\n\n        V1       V2       V3       V4       V5\n1 151.8048 151.8046 151.8044 151.8042 151.8039\n2 151.8140 151.8138 151.8137 151.8134 151.8132\n3 151.8231 151.8230 151.8229 151.8227 151.8226\n4 151.8324 151.8323 151.8322 151.8321 151.8319\n5 151.8416 151.8416 151.8415 151.8414 151.8413\n\n\nIn contrast, let’s now consider the y_grid, this grid has the exact same dimensions as the x_grid, and we can picture it much the same way:\n\n\nCode\nas.data.frame(head(y_grid, n = c(5,5)))\n\n\n         V1        V2        V3        V4        V5\n1 -28.59505 -28.57945 -28.56385 -28.54808 -28.53231\n2 -28.59506 -28.57942 -28.56378 -28.54800 -28.53222\n3 -28.59508 -28.57940 -28.56371 -28.54792 -28.53214\n4 -28.59510 -28.57938 -28.56367 -28.54787 -28.53206\n5 -28.59511 -28.57937 -28.56362 -28.54781 -28.53199\n\n\nIf we combine these two grids together we can get a table in which every cell contains a pair of values, one x_grid value and one y_grid value:\n\n\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\n151.8048, -28.59505\n151.8046, -28.57945\n151.8044, -28.56385\n151.8042, -28.54808\n151.8039, -28.53231\n\n\n2\n151.8140, -28.59506\n151.8138, -28.57942\n151.8137, -28.56378\n151.8134, -28.54800\n151.8132, -28.53222\n\n\n3\n151.8231, -28.59508\n151.8230, -28.57940\n151.8229, -28.56371\n151.8227, -28.54792\n151.8226, -28.53214\n\n\n4\n151.8324, -28.59510\n151.8323, -28.57938\n151.8322, -28.56367\n151.8321, -28.54787\n151.8319, -28.53206\n\n\n5\n151.8416, -28.59511\n151.8416, -28.57937\n151.8415, -28.56362\n151.8414, -28.54781\n151.8413, -28.53199\n\n\n\nWhat we have now is a table where every single cell corresponds to a cell (value) in the eReefs model. That is to say, that for every cell in this table we just made, there is information about water temperature, turbidity, nutrients, etc., etc. To take things even further, if we include the z dimension depth we would have 45 copies of this table, with each copy of the table corresponding to 1 depth layer in the model.\nAdd that all up and we have a table that has 1221290 cells, where the table is stacked 45 times in a row (depth), where every cell in every table has more than 200 different environmental variables. Hopefully that makes sense.\nOK so sure, that’s kind of cool I suppose, but why does this matter? Who cares?\nWell, the reason this matters is that we can use this conceptual understanding of the model to be able to sift through all that data to pinpoint the exact thing that we want. You could use this almost like a GPS. For example, If I wanted to figure out the water temperature at 151.4, -23.2, at a depth of -40m, all I would need to do is say “give me the information at row 2, column 4”.\n\n\n2.2.3 Specify Our Target Location\nTo explain how we are going to specify our target I am going to keep the analogy of the table going. The idea is simple, let’s once again imagine the table, the table is the exact same dimensions as the table we were talking about above, except the values in this table are all just “FALSE”:\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n2\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n3\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n4\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n5\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n\nlet’s say that we want to extract all the information within 151.2 to 151.4, and -23.3 to -23.5. What we then do is figure out where those cells are (based on their row and column number) using the table in the previous section, and then set those cells to TRUE in our current table:\n\n\n\n\n1\n2\n3\n4\n5\n\n\n\n\n1\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n2\nFALSE\nFALSE\nFALSE\nFALSE\nFALSE\n\n\n3\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\n4\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\n5\nFALSE\nTRUE\nTRUE\nTRUE\nFALSE\n\n\n\nWe can then use this table to communicate with the database and tell it “only give me data that lines up with my true values, remove the rest”. And that’s kind of it! If all goes well, the database will return the exact data you requested. let’s see how that looks in code.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to highlight here that the code we are about to write and the data we are working with does not take the form of an actual table, the above description is just a handy analogy to describe what is happening.\n\n\nThe first thing we are going to do is get the boundaries of our target area.\n\n\nCode\n#use the bbox function to get the boundaries\ntarget_bounds &lt;- st_bbox(dt_region)\n\ntarget_bounds\n\n\n     xmin      ymin      xmax      ymax \n146.14439 -19.70039 148.29854 -17.62597 \n\n\nThen we use a series of logical steps that check the xmin, xmax, ymin, and ymax values of our target area and changes cells that fall inside these bounds to TRUE (those outside are given FALSE). There are also some cells that start as NA, so we change those to FALSE.\n\n\nCode\n#if the value is inside the bounds of each of our coords, change it to TRUE. Those outside are automatically false\ntrue_false_array &lt;- x_grid &gt;= target_bounds[1] & \n  x_grid &lt;= target_bounds[3] & \n  y_grid &gt;= target_bounds[2] & \n  y_grid &lt;= target_bounds[4]\n  \n#if the value is NA, change it to false.\ntrue_false_array[is.na(true_false_array)] &lt;- FALSE\n\n\n\n\n2.2.4 Obtain the “Coordinates” of Our Target Location\nSo what we did above was create an array that contains TRUE and FALSE values. The dimensions of this array perfectly match the dimensions of the data. Next up, we need to find the exact positions in the array where the values change from FALSE to TRUE (noting that TRUE means inside our area of interest). These positions will then correspond to the positions we need to send to the database. Here is the code to achieve this:\n\n\nCode\n#return the row index for every row that contains at least one true value:\ntrue_rows &lt;- which(apply(true_false_array, 1, any))\n\n#find the first row that contains a true value\nfirst_row &lt;- true_rows[1]\n\n#find the number of rows that contains a true value\nnum_of_rows &lt;- tail(true_rows, n = 1) - first_row\n\n#return the row index for every row that contains at least one true value:\ntrue_cols &lt;- which(apply(true_false_array, 2, any))\n\n#find the first col that contains a true value\nfirst_col &lt;- true_cols[1]\n\n#find the number of cols that contains a true value\nnum_of_cols &lt;- tail(true_cols, n = 1) - first_col\n\n\nOur values are as follows:\n\nFirst Row = 1\nNumber of Rows = 312\nFirst Col = 1077\nNumber of Cols = 314\n\nWith that done we now have our “coordinates” to send to the database to tell it where to extract data from.\n\n\n2.2.5 Specify Our Target Variable\nAlmost there, only one more part. In this section we are going to learn how to specify what variable to download. So far all I have told you is that eReefs has hundreds of variables, that’s cool and all but what are their names? How do you access them? Thankfully the function nc_vars() from the ncmeta package can help us. Simply run the function for the input path we figured out earlier and it will return a table with all the variables available:\n\n\nCode\n#return a table of all possible variables\nall_variables &lt;- nc_vars(input_file)\n\n\nBe careful though, the dimensions on some of these variables are different, and you might need to provide more (or less) information to make it work. For example, some variables might not have a depth (z) aspect to them and you would need to drop this from the data request.\nBy looking at this table we can establish that to get the chlorophyll a and the nitrites datasets we need to supply the names “Chl_a_sum” and “NO3”.\n\n\n2.2.6 Extract the Data\nIt’s finally time, after all that we can start our data extraction!\nThe function we are going to use to extract the data is the read_ncdf() function from the stars package. This function takes several inputs, such as the source of the data, the variable we are interested in, and the “coordinates” we are interested in. Thanks to all the work we have done above we have all of the “hard” information, however there are still a few little things to tick off.\nWhen we are talking about the “coordinates” of the data I have previously spoken about the x, y, and z dimensions of the data (the longitude, latitude, and depth). However there is one more dimension I haven’t spoken about yet - time. Yes this data actually has 4 dimensions we need to specify. To keep things simple we will start off my just asking for a single point in time, but we’ll later retrieve a full time series. So, when we supply the details we are essentially going to tell the database,\n\nOn the x dimension; start at this cell, and keep going until this cell\nOn the y dimension; start at this cell, and keep going until this cell\nOn the z dimension; just give us one depth layer\nOn the t dimension; just give us one time step\n\nIn code, this is how it looks:\n\n\nCode\n#set our dimensions (44 is the surface depth, 100 is the 100th time step)\nour_dimensions_request &lt;- cbind(start = c(first_row, first_col, 44, 100),\n                                count = c(num_of_rows, num_of_cols, 1, 1))\n\n\nWhich we can supply to the read_ncdf function (it will take a little while to run, that’s fine):\n\n\nCode\n#extract the data\nextracted_data &lt;- read_ncdf(input_file, \n                            var = \"Chl_a_sum\", \n                            ncsub = our_dimensions_request)\n\n\nIf that code ran, congratulations you have official got the data."
  },
  {
    "objectID": "posts/ereefs_extracting_data/index.html#cropping-data",
    "href": "posts/ereefs_extracting_data/index.html#cropping-data",
    "title": "The Extraction of Highly Specialised Modelled Data from eReefs",
    "section": "3.1 Cropping Data",
    "text": "3.1 Cropping Data\nThe next step I’d like to explore is conducting an initial crop of our data, this is because as we noted above, the data extends outside our area of interest, despite our efforts when requesting the data. This is for two main reasons:\n\nBecause when we request data we need to use the maximum bounds of our object, and\nBecause the data is on a curvilinear grid - which messes with the grid layout.\n\n\n3.1.1 Curvilinear Grid Data\n\n\n\n\n\n\nNote\n\n\n\nRaster data can be provided in a range of different grid types. The most common, and the one you are probably familiar with is the regular grid. In this type of grid each cell is consistent. In a curvilinear grid, cells bend and twist to allow for a higher concentration of cells in area that require greater resolution. This has the benefit of reducing file size, but the downside of inflicting psychic damage to the uninitiated spatial analyst.\nIf you would like to learn more about different grid types, check out this handy explainer.\n\n\nTo help us understand this, here is a map showing the bounding box in red that we used to request the data:\n\n\nCode\n#create a simple plot of the data\ntm_shape(extracted_data) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(st_as_sfc(target_bounds)) +\n  tm_polygons(fill = NULL,\n              col = \"#E6AA04\")\n\n\n\n\n\n\n\n\n\nwhich demonstrates how the top left and bottom right corners are defining the region in which data is collected - at least a little bit. In this map we can also start to see the effect of the curvilinear grid and how it twists the data. A closer look at the actual grid lines of the data might demonstrate this a bit clearer:\n\n\nCode\n#extract each cell as a polygon\ncurvilinear_polygons &lt;- st_as_sf(extracted_data, as_points = FALSE, merge = FALSE)\n\n#create a simple map\ntm_shape(curvilinear_polygons) +\n  tm_borders(col = \"#00252A\") \n\n\n\n\n\n\n\n\n\nEspecially if you compare this to a linear grid representing the same area:\n\n\nCode\n#create a simple map\ntm_shape(warped_data_sf) +\n  tm_borders(col = \"#00252A\") \n\n\n\n\n\n\n\n\n\nSo what can we do with this new found knowledge? Well for one thing it gives us a better understanding of how the data is organised, for example if you run extracted_data you might now understand why offset is NA - because the offset changes per cell. But secondly, it is about to play an important role in cropping the data.\nWe can crop almost any stars object (curvilinear grid, or regular grid) using st_crop() and it will broadly do what we want:\n\n\nCode\n#crop to the actual area of interest\ncurv_lini_cropped_data &lt;- extracted_data |&gt; \n  st_crop(dt_region)\n\n\n\n\nCode\n#create a simple map\ntm_shape(curv_lini_cropped_data) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2)),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region, is.main =  T) +\n  tm_polygons(fill = NULL,\n              col = \"black\")\n\n\n\n\n\n\n\n\n\nHowever you may notice that the st_crop() function is providing a warning. like this:\n“Warning in st_crop.stars(st_transform(extracted_data,”EPSG:7844”), dt_region) : crop only crops regular grids: maybe use st_warp() first?”\nAdditionally, if you inspect the dimensions of the original data in comparison to the cropped data it is clear something funky is going on:\n\n\nCode\ndim(extracted_data)\n\n\n   i    j    k time \n 312  314    1    1 \n\n\n\n\nCode\ndim(curv_lini_cropped_data)\n\n\n   i    j    k time \n 312  314    1    1 \n\n\nIndeed, if you inspect each of the objects again, using extracted_data, and curv_lini_cropped_data you can see that the only thing that really changed is that there are now more NA values. So what st_crop() actually did in this scenario was just replace values outside our area with NA:\n\n\nCode\n#create a simple map\ntm_shape(curv_lini_cropped_data) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2),\n                                           value.na = \"#E6AA04\"),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(dt_perspective, is.main =  T) +\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n\n\n\n\n\n\n\n\nWhich is not necessarily a bad thing, but can become a big problem if we are particularly concerned about file size.\nThe reason this occurs is mostly a mystery to me, but I believe it has to do with the way the grid cells on a curvilinear raster are set out - and that those NA cells are needed to provide positioning context to the rest of the cells.\n\n\n3.1.2 Regular Grid Data\nThe solution to this is of course what was recommended in the original warning message - to use st_warp() to shift the data from a curvilinear grid onto a regular grid. This is thankfully not to difficult, and only has four main steps:\n\nObtain the xmin, xmax, ymin, and ymax bounds of our curvilinear object\nObtain the x and y dimensions of our curvilinear object (i.e. number of rows and cols)\nCreate a “destination” regular grid using the values determined in steps 1. and 2.\nWarp the curvilinear object onto the destination grid, matching cell to cell.\n\n\n\nCode\n#convert our curvilinear object into just a bbox then update the crs on the bbox\ncurvilinear_bbox &lt;- extracted_data |&gt; \n  st_bbox() |&gt;\n  st_as_sfc()\n\n#get a linear grid target with the same dimensions (number of cells) as our curvilinear grid \nreg_stars &lt;- st_as_stars(curvilinear_bbox, #using the bbox to provide the xmin, xmax etc., \n                         nx = dim(extracted_data)[[1]], #and the dimensions to provide the x and y count. \n                         ny = dim(extracted_data)[[2]], \n                         values = NA_real_) #Fill each cell with NA\n\n#run st warp, it requires a curvilinear object, and a regular object as a target\nwarped_data &lt;- st_warp(extracted_data, reg_stars)\n\n\nWith the warped data we can then use the st_crop() function again:\n\n\nCode\n#crop to the actual area of interest\nreg_grid_cropped_data &lt;- warped_data |&gt; \n  st_crop(dt_region)\n\n\n\n\nCode\n#create a simple map\ntm_shape(reg_grid_cropped_data) +\n  tm_raster(col.scale = tm_scale_intervals(n = 6,\n                                           values = my_pal,\n                                           label.format = list(digits = 2),\n                                           value.na = \"#E6AA04\"),\n            col.legend = tm_legend(reverse = T)) +\n  tm_shape(dt_region, is.main =  T) +\n  tm_polygons(fill = NULL,\n              col = \"black\") +\n  tm_shape(dt_perspective, is.main = T) +\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n\n\n\n\n\n\n\n\nThus actually cropping the data to the bounding box of our target area, and create a map with significantly few NA cells, specifically the curvilinear version has 74496 NA values, and the regular grid version has 17086 NA values - a difference of 57410"
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html",
    "href": "posts/ereefs_plotting_data/index.html",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "",
    "text": "In this blog we are going to learn how to create some visually interesting plots in R. The package we are going to be using is ggplot2, and the data we are going to be using is from eReefs. If you are interested in getting an exact copy of the data I recommend you check out my other blog; The Extraction of Highly Specialised Modeled Data from eReefs, however you can still follow along just fine using your own data if it shares a similar format. Once you have completed this blog, I also highly recommend you check out my blog about Mapping eReefs Data, as this blog uses exactly the same dataset and explores its’ spatial aspects."
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html#converting-to-tabular-format",
    "href": "posts/ereefs_plotting_data/index.html#converting-to-tabular-format",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "2.1 Converting to Tabular Format",
    "text": "2.1 Converting to Tabular Format\nOnce we have the raster data we then need to convert it to a tabular format so it can be used by our ggplot2 functions. There is a very handy function for this called st_as_sf() from the stars package that converts stars objects into sf objects. Sf objects are essentially tables with an extra column for spatial information, we can then convert the sf object to a “normal” table by simply removing the column with the extra spatial information.\n\n\n\n\n\n\nNote\n\n\n\nSf objects (from the sf package), and stars objects (from the stars package) are designed to work together as the packages were written by the same author. Thank god for that guy right!\n\n\nThe st_as_sf() function has a few key arguments;\n\nx (this is the stars object)\nas_points (should each raster cell become a polygon (F) or a point (T)) - I usually prefer polygons\nmerge (should cells with equal values be merged? Yes (T) or No (F)) - I usually say no, as this can mess with statistics based on cell count\nlong (should the table be wide or long) - ggplot loves long data, so I usually set this to TRUE\n\n\n\nCode\n#convert data to a simple feature object\nsf_data &lt;- st_as_sf(example_data, as_points = F, merge = F, long = T)\n  \n#drop the geometry column from the data\nsf_data &lt;- sf_data |&gt;\n  st_drop_geometry()"
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html#exploring-the-data",
    "href": "posts/ereefs_plotting_data/index.html#exploring-the-data",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "2.2 Exploring the Data",
    "text": "2.2 Exploring the Data\nOkay, the data has been loaded in, and converted to tabular format. Let’s take a quick look at what we are dealing with. This data has 161 rows and 179 columns. The column names are time and Chla:\n\n\nCode\n#view the first few rows of the data\nhead(sf_data)\n\n\n                 time       Chla\n1 2020-07-01 02:00:00 0.07818966\n2 2020-07-01 02:00:00 0.07410832\n3 2020-07-01 02:00:00 0.07653029\n4 2020-07-01 02:00:00 0.08475932\n5 2020-07-01 02:00:00 0.07026779\n6 2020-07-01 02:00:00 0.07251479\n\n\nIf we ordered this data by date we would see that there are several hundred rows of data that belong to the exact same date and time. This is because the original source of this data was a spatial file - it had a grid of values for each date and time:\n\n\nCode\n#order the first few rows by date\nhead(arrange(sf_data, time))\n\n\n                 time       Chla\n1 2020-07-01 02:00:00 0.07818966\n2 2020-07-01 02:00:00 0.07410832\n3 2020-07-01 02:00:00 0.07653029\n4 2020-07-01 02:00:00 0.08475932\n5 2020-07-01 02:00:00 0.07026779\n6 2020-07-01 02:00:00 0.07251479\n\n\nThe value column is the concentration of chlorophyll a in the water column, measured in micrograms per litre. Let’s update that.\n\n\nCode\n#rename the data column\nsf_data &lt;- rename(sf_data, \"Chla (ug/L)\" = \"Chla\")\n\n\nAnd to keep things consistent we will capitalise “Time” for the time column.\n\n\nCode\nsf_data &lt;- rename(sf_data, \"Time\" = \"time\")\n\n#view the first few rows of data\nhead(sf_data)\n\n\n                 Time Chla (ug/L)\n1 2020-07-01 02:00:00  0.07818966\n2 2020-07-01 02:00:00  0.07410832\n3 2020-07-01 02:00:00  0.07653029\n4 2020-07-01 02:00:00  0.08475932\n5 2020-07-01 02:00:00  0.07026779\n6 2020-07-01 02:00:00  0.07251479\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that it is generally bad form to include spaces in your column names, but I am doing it to reduce the code needed for the plotting section. You will see I have to refer to the column name using ticks (``) to make the ggplot code work because of the space in the column name."
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html#distribution-and-log-transformation",
    "href": "posts/ereefs_plotting_data/index.html#distribution-and-log-transformation",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "3.1 Distribution and Log Transformation",
    "text": "3.1 Distribution and Log Transformation\nHowever what we don’t know is about the distribution of the data. Below is a histogram of our data.\n\n\nCode\n#create a basic histogram plot of the data\nggplot(sf_data) +\n  geom_histogram(aes(x = `Chla (ug/L)`), \n                 bins = 150,\n                 fill = \"#00252A\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nClearly this data is heavily right skewed. Although we wont be doing any statistical analysis this distribution will still impact how our plot looks. So to make things a bit nice we will look at the data with a log 10 transformation:\n\n\nCode\n#create a histogram plot of the data on a log10 scale\nggplot(sf_data) +\n  geom_histogram(aes(x = `Chla (ug/L)`), \n                 bins = 150,\n                 fill = \"#8E3B46\") +\n  scale_x_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nYup, using a log10 scale will likely help us get a better visual representation of the data.\nNow, knowing a bit more abut the distribution lets consider what kind of plot we want to make. Of course it is up to you, but I know that when I see a time variable and a whole bunch of continuous values, I am thinking lines and/or dot plots.\nIn its most basic form here is a dot plot:\n\n\nCode\n#creat a basic dot plot of the data\nggplot(sf_data) +\n  geom_point(aes(x = Time, y = `Chla (ug/L)`), \n             color =  \"#E6AA04\") +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nA few things to note:\n\nAs we have already covered, there is a shit ton of data and actually plotting all the points takes several minutes (boring).\nIt looks like there are some trends but it is a bit hard to tell, particularly because the number of points makes it difficult to identify areas of high, mid, or low density that might affect the trends\n\nFor the first point, a simple solution is to take a random subset of data to make plotting more efficient. For the second point, this will in part be fixed by the sub sampling, but we will also be adding extra visuals to this plot as we go along."
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html#subsetting-data",
    "href": "posts/ereefs_plotting_data/index.html#subsetting-data",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "3.2 Subsetting Data",
    "text": "3.2 Subsetting Data\nTo do our sub-setting we will use the slice_sample() function. To ensure that we get the same number of randomly sampled points from each time step (day) we will make sure to first group our data by the Time column. In summary we want to randomly select 150 data points from each day - still quite a lot.\n\n\nCode\n#get a random subset of data, ensuring an equal sample is taken from each day\nsf_data_subset &lt;- sf_data |&gt; \n  group_by(Time) |&gt; \n  slice_sample(n = 150) |&gt; \n  ungroup()\n\n\n\n\nCode\n#creat a basic dot plot of the data\nggplot(sf_data_subset) +\n  geom_point(aes(x = Time, y = `Chla (ug/L)`), \n             color =  \"#E6AA04\") +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAwesome, right away we can see that there appears to be a downtrend in concentration values around the middle of the time series before the values then increase again towards the end of the graph. It is easier to see this using this plot because we can see that there is a lower density of points in the middle of the plot near the top, where as even with the random sampling there is still a very high density of points in the middle of the plot near the bottom. It should be noted that it is theoretically possible the areas of high and low density are a product of the random sampling, but that is highly unlikely."
  },
  {
    "objectID": "posts/ereefs_plotting_data/index.html#additional-visuals",
    "href": "posts/ereefs_plotting_data/index.html#additional-visuals",
    "title": "Creating Beautiful Plots using eReefs Data",
    "section": "3.3 Additional Visuals",
    "text": "3.3 Additional Visuals\nSomething that will help us determine with greater precision how the data trends over time, would be a nice line that follows the daily mean. We will calculate this line using the full dataset to make 100% sure of the trend we spotted above.\n\n\nCode\n#calculate a daily mean value\nsf_data_daily_mean &lt;- sf_data |&gt; \n  group_by(Time) |&gt; \n  summarise(`Chla (ug/L)` = mean(`Chla (ug/L)`))\n\n\n#create a basic dot plot plus daily mean line\nggplot() +\n  geom_point(data = sf_data_subset, \n             aes(x = Time, y = `Chla (ug/L)`), \n             color = \"#E6AA04\") +\n  geom_line(data = sf_data_daily_mean, \n            aes(x = Time, y = `Chla (ug/L)`), \n            color = \"#00252A\",\n            lwd = 1) +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis line confirms that the values do indeed decrease towards the middle of the time series before increasing again towards the end of the graph, but the line is a bit ugly no? A common replacement in this scenario is to use a Generalized Additive Model (GAM) which creates a smoothing spline that also reveals trends but is not so harsh. Noting that the GAM makes use of the multiple samples per day to achieve the desired results:\n\n\nCode\n#create a basic dot plot plus GAM line\nggplot() +\n  geom_point(data = sf_data_subset, \n             aes(x = Time, y = `Chla (ug/L)`), \n             color = \"#E6AA04\") +\n  geom_smooth(data = sf_data,\n              aes(x = Time, y = `Chla (ug/L)`), \n              method = \"gam\", \n              formula = y ~ s(x), \n              color = \"#00252A\",\n              se = F) +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nSomething else of interest with time series data is how things are doing relative to a long-term mean. This long-term mean might be an annual mean, or a mean of all the available data going several years back, or a mean of some historical reference period. For us, we will just look at the annual mean:\n\n\nCode\n#calculate group mean to use for the yintercept line from the full dataset\nannual_mean &lt;- sf_data |&gt;\n  summarise(`Mean Chla (ug/L)` = mean(`Chla (ug/L)`, na.rm = T)) |&gt; \n  as.numeric() |&gt; \n  round(4)\n\n#create a more sophisticated plot\nggplot() +\n  geom_point(data = sf_data_subset, \n             aes(x = Time, y = `Chla (ug/L)`), \n             color = \"#E6AA04\") +\n  geom_hline(yintercept = annual_mean,\n             colour = \"#628395\",\n             lwd = 1.3) +\n  geom_smooth(data = sf_data,\n              aes(x = Time, y = `Chla (ug/L)`), \n              method = \"gam\", \n              formula = y ~ s(x), \n              color = \"#00252A\",\n              se = F) +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAs expected, the smoothed GAM line fluctuates above and below the annual mean. You might initially think that the GAM line goes waaaay below the mean compared to how much it goes above the mean and that surely the mean isn’t correct, but remember this is all visualised with a log10 y axis.\nThe next thing I would like to add is some sort of visual cue to signify season. In the Townsville region (where we are currently looking at the data) there are only two season; “wet” and “dry”. This is loosely associated with summer and winter, with hundreds to thousands of millimeters of rain falling in summer and often less than one hundred millimeters falling across all of winter. The reason we care about rainfall is that it is one of the most significant drivers of chlorophyll a concentrations in the ocean. The rain on land brings lots of nutrients down the rivers and out onto the reef - nutrients which phytoplankton consume and then produce chlorophyll a (simplified explanation).The exact cut-off dates we will use for the wet season/dry season are March and October.\n\n\nCode\n#assign either the wet or dry season to each row of data\nsf_data_subset &lt;- sf_data_subset |&gt; \n  mutate(Season = case_when(month(Time) &gt; 4 & month(Time) &lt; 11 ~ \"Dry\", T ~ \"Wet\"))\n  \n#create a more sophisticated plot\nggplot() +\n  geom_point(data = sf_data_subset, \n             aes(x = Time, y = `Chla (ug/L)`, \n                 color = Season)) +\n  geom_hline(yintercept = annual_mean,\n             colour = \"#628395\",\n             lwd = 1.3) +\n  scale_color_manual(values = c(\"#E6AA04\", \"#8E3B46\")) +\n  geom_smooth(data = sf_data,\n              aes(x = Time, y = `Chla (ug/L)`), \n              method = \"gam\", \n              formula = y ~ s(x), \n              color = \"#00252A\",\n              se = F) +\n  scale_y_log10() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nIt is interesting to see that there is a clear relationship visible between season and chlorophyll a concentration. This graph suggests there is a sort of “recharge” and “use” cycle occurring. Where chlorophyll a reach a maximum concentration right after the end of the wet season, before being “used up” over the dry season and requiring a “recharge” by the following wet season.\nThere is only one more thing I would like to add to this plot, and it is mainly due to personal preference. I would like to overlay a violin plot to further highlight the distribution of the data we are dealing with. Specifically, it will highlight any regions of the plot in which large amounts of data are concentrated, as well as any spots that are particularly skewed.\n\n\nCode\n#mutate the date column back into an actual date variable\nsf_data_subset &lt;- sf_data_subset |&gt; \n  mutate(Time = as.Date(Time))\n\n#do the same for the full dataset\nsf_data &lt;- sf_data |&gt; \n  mutate(Time = as.Date(Time))\n\n#create a more sophisticated plot\nggplot() +\n  geom_point(data = sf_data_subset, \n             aes(x = Time, y = `Chla (ug/L)`, \n                 color = Season)) +\n  geom_hline(yintercept = annual_mean,\n             colour = \"#628395\",\n             lwd = 1.3) +\n  scale_color_manual(values = c(\"#E6AA04\", \"#8E3B46\")) +\n  geom_smooth(data = sf_data,\n              aes(x = Time, y = `Chla (ug/L)`), \n              method = \"gam\", \n              formula = y ~ s(x), \n              color = \"#00252A\",\n              se = F) +\n  geom_violin(data = sf_data_subset,\n              aes(x = Time, y = `Chla (ug/L)`),\n              alpha = 0.4, \n              color = \"Black\") +\n  scale_y_log10() +\n  scale_x_date(breaks = pretty_breaks(6)) +\n  labs(x = \"Time\", y = \"Chla (ug/L) (Log10 Scale)\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nAnd there you have it, a fairly nice looking singular dot plot. It contains a heck of a lot of information without being too crowed (in my opinion). Some extensions you could play around with for this plot could be downloading several years of data and faceting by year, comparing different water quality indicators, or even comparing different locations around the reefs."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Environmental Bytes for a Better Earth",
    "section": "",
    "text": "Document\n\n\n  \n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nLinear Modelling in R\n\n\n“Modelling” has become a pretty big buzz word these days, almost as much as “AI”. “Why cant you just use AI for that?”, “Can you just model it?” The unfortunately reality is these terms are often too complicated to concisely explain, which is one of the root causes of their misuse. In this blog I go back to basics and try to build an understanding of the fundamentals that will help us all answer those tricky questions in the future.\n\n\n\n\n\n16 Jul, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nThe Extraction of Highly Specialised Modelled Data from eReefs\n\n\nIn this blog I talk about the skills needed and the steps taken to execute the extraction of modelled environmental data such as ocean currents, nutrient loads, and water clarity from the online platform “eReefs”.\n\n\n\n\n\n23 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Beautiful Maps Using eReefs Data\n\n\nIn this blog I demonstrate how you can make beautiful maps in R using example data extracted from the eReefs platform. You can also follow along with any of your own data.\n\n\n\n\n\n23 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCreating Beautiful Plots using eReefs Data\n\n\nIn this blog I demonstrate how you can make beautiful plots in R using example data extracted from the eReefs platform. You can also follow along with any of your own data.\n\n\n\n\n\n23 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAutomate Your Output Folders\n\n\nGot a bunch of scripts and are confused which output belongs to which script? Maybe you promised yourself your file organisation would be good this time? Never fear, this post is for you!\n\n\n\n\n\n19 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn Opinionated Dataframe Cleaner\n\n\nNaming your dataframe columns doesn’t have to be hard, does it? Here I demonstrate the benefits of implimenting an opionated dataframe cleaner to help keep your columns organised.\n\n\n\n\n\n13 Apr, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Beautiful Maps in R\n\n\nCreating maps using a programming language can be a painful process. In this blog I explore the R package ‘tmap’ and how it can be used to programmatically make beautiful, report ready maps, with as little stress as possible.\n\n\n\n\n\n23 Mar, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating to Version 4 of the tmap R Package\n\n\nMy favourite mapping package just released a major update! In this post I discuss the changes made as they relate to my work, and provide some tips and tricks I have learnt so far when migration from Version 3.0 to Version 4.0.\n\n\n\n\n\n04 Feb, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn Epic Battle Between Vectorised Functions and For Loops\n\n\nThe choice between using a vectorised function or a for loop can sometimes be a hard one. Don’t know what they are? Or which one is right for which scenario? This blog is for you.\n\n\n\n\n\n29 Jan, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning To Create Your Own Custom Functions\n\n\nHaving the ability to create functions is both a blessing and a curse. You are gifted with limitless potential, but absolutely limited power supply (AKA your brain). In this post I discuss how I learnt to make my own functions and the trials I faced along the way.\n\n\n\n\n\n10 Jan, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGoing Loopy for For Loops\n\n\nWhats the point of for loops? Well for looping of course. In this post I break down how to write for loops in R, how you can easily understand them, and compelling reasons that you might want to learn them yourself!\n\n\n\n\n\n18 Dec, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html",
    "href": "posts/loops_and_vectorised_functions/index.html",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "",
    "text": "Functions and for loops… If you haven’t read my blog posts on these two topics I will shamelessly self promote and say that you should. But honestly, I would like to think that the content written here can be understood without any prior knowledge. So please, read on if you want!\nIf you are not aware, the tension between vectorise functions and for loops is weirdly high. Some people say that vectorised functions are easier to read and understand, others swear that for loops are a more obvious and legible option. Some people prioritise processing time above all else and will create vectorised functions for everything, others say that in todays’ coding languages it really doesn’t make much of a different.\nSo which is it? What do I use? Why do I use it? The idea of my blog today to try and pull apart the differences between vectorised functions and for loops as they relate to my work. I will then dive deeper into how you can use vectorised functions, and code up some examples."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#vectorised-functions",
    "href": "posts/loops_and_vectorised_functions/index.html#vectorised-functions",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.1 Vectorised Functions",
    "text": "2.1 Vectorised Functions\nThis is a bit of a weird analogy but you can think of a vectorised function almost like a shotgun. The idea is that all the actions are completed at the same time in one big boom (like how all the pellets in a shotgun fire at the same time). To do this, you take any “normal” function with the following structure your_function_name &lt;- function(inputs){code} and place this inside a “special” vectoring function such as map() or lapply(). The special vectoring function does all the repetition of the normal function.\n\n\n\n\n\n\nNote\n\n\n\nI will once again promote my functions blog, which will help you understand the “normal” function within the “special” vectoring function."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#for-loops",
    "href": "posts/loops_and_vectorised_functions/index.html#for-loops",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.2 For Loops",
    "text": "2.2 For Loops\nIn contrast, a for loop is more like a chain-gun, with each action completed one after the other in rapid succession. In this analogy you take any “normal” function with the following structure your_function_name &lt;- function(inputs){code} and place this inside the repeating for loop. The normal function is then repeated for each iteration of the loop.\n\n\n\n\n\n\nNote\n\n\n\nYou know where to go if you want to know more… for loops."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#similarities-and-differences",
    "href": "posts/loops_and_vectorised_functions/index.html#similarities-and-differences",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.3 Similarities and Differences",
    "text": "2.3 Similarities and Differences\nWith a better understanding of the idea behind each method, we can now summarise some of the main similarities and differences that exist in practice;\nSimilarities:\n\nUsed for repeated action, to do something several times over\nAre often lauded as “efficient”, “effective”, and “scalable”\nCan be daunting for newcomers\n\nDifferences:\n\nDifferent structure/syntax/method\nVectorised functions are (in my opinion) more vague\nLoops are great to work on products of previous loops (vectorised functions can’t do that)\nVectorised functions are an equal speed or faster (depending the scenario)"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#examples-of-each",
    "href": "posts/loops_and_vectorised_functions/index.html#examples-of-each",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.4 Examples of Each",
    "text": "2.4 Examples of Each\nSometimes its just easier with some practical examples: lets pretend that we have a vector of numbers from 1 to 10 that each need to have 1 added to them. (Yes I know we don’t actually need a vectorised function or for loop to do this, but lets pretend we do).\nSo how would this look in a for loop?\nFirst we have our numbers:\n\n\nCode\nour_numbers &lt;- c(1:10)\n\nour_numbers\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThen we have our loop:\n\n\nCode\n#create an empty vector to store the output\nour_num_plus_1 &lt;- vector(mode = \"numeric\", length = length(our_numbers))\n\n#run the loop\nfor (i in 1:length(our_numbers)){\n  \n  #replace the number in the empty vector with tbe new number\n  our_num_plus_1[i] &lt;- our_numbers[i] + 1\n  \n}\n\n\nAnd then our output:\n\n\nCode\nour_num_plus_1\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nTo contrast, here is the vectorised function. First we would create our numbers (we did that above), then we have the vectorised function:\n\n\nCode\n#use map_int to return a integer vector\nour_num_plus_2 &lt;- map_int(our_numbers, \\(x) x + 1)\n\n\nAnd our output:\n\n\nCode\nour_num_plus_2\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nIdentical outputs, as expected.\nComparing the two side by side in this way confirms our early observations, and tell us more. For example, I stated above that I find vectorised functions to be more vague, and in this example it seems to be the case. Looking at the code it is not at all clear what the function map_int() does, nor how our_numbers and + 1 interact with each other, and what the hell is that \\(x) doing there? In contrast, the for loop might take a few more lines of code, but it is probably a bit easier to understand that the code is meant to do something a few times in a row, even if you can’t quite pick what exact it is."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#purrr",
    "href": "posts/loops_and_vectorised_functions/index.html#purrr",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "3.1 Purrr",
    "text": "3.1 Purrr\n\n\n\n\n\n\nNote\n\n\n\nAs with most topics I have written about, I would highly recommend that you check out the webpage written for the Purrr package. It has excellent documentation and some basic examples that cover the major use cases you would expect to encounter.\n\n\nThe basic syntax of a purrr mapping function is as follows:\n\noption 1: map(vector, function)\noption 2: map(vector, \\(x) x + 1)\noption 3: map(vector, \\(x) function(x))\n\nAs well as some older methods that are no longer actively recommended, but that you might find in old code:\n\nout-dated option 1: map(vector, ~ .x + 1)\nout-dated option 2: map(vect, ~ function(.x))\n\nYou will note that I have written more than one example of the syntax, this is not a mistake. Annoyingly there are multiple “correct” ways to write the code depending on the situtation… Yea I know. I’ll do my best to explain each one.\n\n3.1.1 Syntax Option 1\nSyntax: map(vector, function)\nSummary: no explicit designation of x and y, but does have a named function.\n\n3.1.1.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. Finally we write the function that we want to repeat a bunch of times. IMPORTANT! In this syntax the function we are repeating does not have any brackets after it.\n\n\n3.1.1.2 Why Choose This Way?\nThis option is the most streamlined choice. It has very little moving parts and thus very little that you can write incorrectly. However, the downside of using this method is that you need to be familiar with the map() function otherwise it is really not clear what the code is doing. There are two addition downsides to this method that mean it is not always the right choice:\n\nThe function you want to repeat has to be named, i.e. it needs to be something like mean(), sum(), etc. Or a custom function that you have written and given a name.\nmap() assumes that the first argument in the function you want to repeat is where the vector should go, if the vector needs to go somewhere else you will have to use a different syntax.\n\nAs an example\n\n\nCode\n#This won't do anything to the numbers because it is taking the mean of 1 number each time\nmap_int(our_numbers, mean)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\nNote\n\n\n\nI’m using the map_int() function simply because I want the output to be an integer, the syntax for map_int() is the same as for map().\n\n\n\n\n\n3.1.2 Syntax Option 2\nSyntax: map(vector, \\(x) x + 1)\nSummary: for anonymous (not named) functions that are simple.\n\n3.1.2.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. In this option we then write \\(x), this is essentially saying that from this point on the vector is now “x”. We follow this up by writing the code we want to apply to the vector such as adding 1 to the each element in the vector, which looks like x + 1. IMPORTANT! In this syntax this is no comma between \\(x) and the code you want to apply.\n\n\n3.1.2.2 Why Choose This Way?\nThis option is great when the thing you want to do to the vector does not have a pre-existing function. An additional benefit of this option is that it is one of the clearest ways to demonstrate what is happening to the vector. However, this option is not always the best choice if the thing you want to do to the vector is super complicated and take several lines of code to write. If that is the case, I would recommend writing your own custom function that encapsulates the things you want to do, and then providing this custom function to map using option 1 or 3.\n\n\n\n\n\n\nNote\n\n\n\nThe “x” in \\(x) x + 1 is just a placeholder. If you really wanted to, you could write \\(cashew) cashew + 1 and it would work fine. However, you can’t change the backslash, brackets, or lack of comma.\n\n\nExample 1:\n\n\nCode\nmap_int(our_numbers, \\(x) x + 1)\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nExample 2:\n\n\nCode\nmap_int(our_numbers, \\(cashew) cashew + 1)\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\n\n\n\n3.1.3 Syntax Option 3\nSyntax: map(vector, \\(x) function(x))\nSummary: for custom functions and/or functions that need strong control over where x goes\n\n3.1.3.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. Once again we then write \\(x), which is saying that from this point on the vector is now “x”. This is then followed by a named function, that includes brackets, and within the brackets you place your “x” which looks like: function(x). IMPORTANT! In this syntax this is no comma between \\(x) and the code you want to apply.\n\n\n3.1.3.2 Why Choose This Way?\nThis final option is the most verbose, but most robust method of writing a map function. This option is useful when you need to be very specific about where x goes in your repeated function. For example, if x is meant to be the second argument for some reason. There are no real downsides to this method, and it can technically be used in any scenario, but in a lot of cases you will find that it is overkill.\nHere are a few demonstrations of the benefits of this option, first we need to create a custom function where we deliberately want x to be the second argument:\n\n\nCode\n#create a custom function to demonstrate if for some reason x needs to go somewhere else\ncustom_func &lt;- function(a = 1, b) {b = b + a}\n\n\nThen, if we don’t explicitly put x as the second argument (i.e. if we were to try and use syntax option 1) we would get this error:\n\n\nCode\n#will not work without direct placement\nmap_int(our_numbers, custom_func)\n\n\nError in `map_int()`:\nℹ In index: 1.\nCaused by error in `.f()`:\n! argument \"b\" is missing, with no default\n\n\nSo instead, we use syntax option 3 and directly tell “x” where it needs to go and that it is the second argument:\n\n\nCode\n#working version\nmap_int(our_numbers, \\(x) custom_func(b = x))\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nAnd as a bonus, our custom function also allows us to change how much is added to each element by changing the first argument:\n\n\nCode\n#working version\nmap_int(our_numbers, \\(x) custom_func(5, x))\n\n\n [1]  6  7  8  9 10 11 12 13 14 15"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#purrr-x2",
    "href": "posts/loops_and_vectorised_functions/index.html#purrr-x2",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "3.2 Purrr x2!",
    "text": "3.2 Purrr x2!\nHopefully those above examples made sense, if not we will be working on another more realistic example further below. But before we do I want to take a slight detour to touch on an important additional feature of Purrr called map2()!\nLets set the scene, imagine you have a vector of number just like we did above. And for this vector of numbers you want to add something to each one, just like we did above. Except this time, the thing you want to add to each number in the vector is different every time! How are we going to do that? I’ve spoiled it already, but of course we would use map2() (the “2” means it takes two vectors or lists). Thankfully, the syntax is not very different from the normal map(), here is a quick example:\n\n\nCode\n#create our second vector of numbers in the reverse order of the orginal numbers\nour_numbers2 &lt;- c(10:1)\n\n#run the map2 function\nmap2_int(our_numbers, our_numbers2, \\(x,y) x + y)\n\n\n [1] 11 11 11 11 11 11 11 11 11 11\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat if you have more than two vectors/lists? You can use the pmap() functions."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#get-the-data",
    "href": "posts/loops_and_vectorised_functions/index.html#get-the-data",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.1 Get The Data",
    "text": "4.1 Get The Data\nI have pre-prepared some data for us to use, lets make a object that lists the names of each of the datasets we want to open.\n\n\nCode\n#list the files found in the directory with the extension \".RData\"\nrasters_to_open &lt;- list.files()[str_detect(list.files(), \".RData\")]\n\n#list the files found in the directory with the extension \".gpkg\"\ncrops_to_open &lt;- list.files()[str_detect(list.files(), \".gpkg\")]"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#create-a-list-of-data",
    "href": "posts/loops_and_vectorised_functions/index.html#create-a-list-of-data",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.2 Create A List of Data",
    "text": "4.2 Create A List of Data\nAs you know (because you are a talented spatial data analyst), you can you put almost anything you want into a list in R. Using this knowledge we are going to load each of the raster datasets into a single list.\n\n\nCode\n#ironically we need to use a loop to load Rdata objects (quirk of the fucnction)\nfor (i in rasters_to_open){load(i)}\n\n#before putting the datasets into a list\nlist_of_rasters &lt;- map(str_remove_all(rasters_to_open, \".RData\"), get)\n\n#for gpkg objects we can directly open and put into a list\nlist_of_crops &lt;- map(crops_to_open, st_read)\n\n\nReading layer `dt' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 146.2958 ymin: -19.30559 xmax: 148.2985 ymax: -17.62597\nGeodetic CRS:  GDA2020\nReading layer `wt' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 145.346 ymin: -18.898 xmax: 147.2067 ymax: -15.19529\nGeodetic CRS:  GDA2020\nReading layer `mwi' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_3.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 147.7533 ymin: -22.17852 xmax: 151.3289 ymax: -18.58484\nGeodetic CRS:  GDA2020\n\n\nHa! Look at that, we already had to use a vectorised function ;)"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#vectorise-our-function",
    "href": "posts/loops_and_vectorised_functions/index.html#vectorise-our-function",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.3 Vectorise Our Function",
    "text": "4.3 Vectorise Our Function\nWith the data ready in a list, all we have left to do is decide what function we want to apply. In this example there are a few things we want to do:\n\nReplace any value in the raster that is greater than 200 with “NA”. The reason we want to do this is the data provider designated NA values as very large numbers (because they couldn’t store them directly as NA),\nCrop each dataset to a specific shape in the area, and\nConvert the raster from an nc object to a sf object (don’t worry if you don’t know what that means, its not important for this blog).\n…\n\n\n4.3.1 Custom Function\nLets quickly write that custom function:\n\n\nCode\nreplace_with_na &lt;- function(raster_dataset, crop_dataset){\n  \n  #replace values greater than 200 with NA\n  raster_dataset[raster_dataset &gt; 200] &lt;- NA\n  \n  #change the crs of the dataset and crop the dataset to the area of interest\n  raster_dataset &lt;- raster_dataset |&gt; \n    st_transform(\"EPSG:7844\") |&gt; \n    st_crop(crop_dataset)\n  \n  #extract just one day from the multiyear dataset (for demonstration purposes)\n  raster_dataset &lt;- raster_dataset[,,,,1]\n  \n  #return the object\n  return(raster_dataset)\n}\n\n\n\n\n4.3.2 Vectorising\nAnd now we can put that custom function to work!\n\n\nCode\n#vectorise our custom function\noutput &lt;- map2(list_of_rasters, list_of_crops, \\(x, y) replace_with_na(x, y))\n\n\nDone!\nLets take a look at what we just did by putting each of the datasets into one map, I’ve coloured each location differently:\n\n\nCode\ntm_shape(output[[1]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.greens\")) +\n  tm_shape(output[[2]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.blues\")) +\n  tm_shape(output[[3]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.oranges\")) +\n  tm_layout(legend.show = F)\n\n\n\n\n\n\n\n\n\nPretty cool.\nAs you can see, by using vectorised functions we made the entire workflow of getting the data, manipulating the data, and mapping the data take less than 20 lines of code. This is the power of vectorised functions - clean, efficient, effective code.\nThats all for now!"
  },
  {
    "objectID": "posts/modelled_data_basics/index.html",
    "href": "posts/modelled_data_basics/index.html",
    "title": "Linear Modelling in R",
    "section": "",
    "text": "In this blog I would like to explore the basics of data modelling using the tidymodels set of packages in R. If you’re anything like me the phrase “modelling” has probably come to fill you with doubt. The term is often thrown around loosely and could apply to everything from simple regression, all the way to some crazy AI implimentation that only 2 people in the entire world understand. It can be hard to differentiate between these extremes, and to the lay person sometimes these are basically the same thing. I have imagined several scenarios in which I say “model” to a manager and they picture the next AI revolution has come to fix all of their problems (what I actually mean is I did some linear regression).\nTo combat this, for my own peace of mind, and hopefully yours, I have decided to write a blog (or two) about learning how to use the tidymodels packages in R. Ideally, by the end of this series we are both able to explain in more detail exactly what we are doing to our colleagues."
  },
  {
    "objectID": "posts/modelled_data_basics/index.html#building-a-dataset",
    "href": "posts/modelled_data_basics/index.html#building-a-dataset",
    "title": "Linear Modelling in R",
    "section": "2.1 Building A Dataset",
    "text": "2.1 Building A Dataset\nThe first thing we need to do is build a dataset that explores this relationship between elevation and tree_height. I am going to manually add 11 elevations values evenly spaced between 0m and 1000m, and then assign tree heights to each elevation. Note that we want to show a relationship so I will deliberately increase tree heights as elevation increases, however to make things interesting I will make sure it is not a completely perfect reationship.\n\n\nCode\nlibrary(tidymodels)\nlibrary(gt)\n\n#create an example dataset, note that we are being very delibrate to make tree height increase with elevation\ntree_height_x_elevation_df &lt;- data.frame(elevation = seq(0, 1000, 100),\n                                         tree_height = c(0,5,23,32,24,33,40,48,40,50,55))\n\n\nOn a plot, this is how our make believe data looks, obviously the trend is pretty easy to spot since we specifically created the data for this purpose:\n\n\nCode\n#create a simple plot to show the data pointsw\nggplot(tree_height_x_elevation_df, aes(x = elevation, y = tree_height)) +\n    geom_point(col = \"#8E3B46\") + \n    labs(x = \"Elevation (m)\", y = \"Tree Height (m)\") +\n    theme_bw()"
  },
  {
    "objectID": "posts/modelled_data_basics/index.html#building-a-linear-regression-model",
    "href": "posts/modelled_data_basics/index.html#building-a-linear-regression-model",
    "title": "Linear Modelling in R",
    "section": "2.2 Building A Linear Regression Model",
    "text": "2.2 Building A Linear Regression Model\nTo make a linear model using this dataset we are going to leverage the tidymodels() package, which is actually a wrapper for a collection of packages that do a wide range of things within the modelling ecosystem such as preparing data, creating models, and evaluating model performance. Making a linear model is actually pretty straight forward when we use these packages, there are only really three steps:\n\nDefine the model to be used (for us this is a linear regression)\nDefine the “engine” that runs the model (for us there is only one option “linear model”, but for more complex models there are multiple methods of implementation)\nFit the model, i.e. input the required variables (for us our independant variable is elevation, and our dependent variable is tree height)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you are using “real” data that wasn’t made up for educational purposes there are extra steps in the model building stage focused on properly preparing the data such as removing colinearity and normalsing numeric data to have a standard deviation of one. But we are not going to cover those in this example.\n\n\n\n\n\n\n\n\nNote\n\n\n\nInterestingly, for a lot of models (not just linear regression) made using tidymodels packages, these general steps are almost the same. You can swap out a different model and engine while keeping the same inputs if you wanted.\n\n\nThe code to create our linear model is as follows:\n\n\nCode\n#create a linear model based on the example data\nmy_lin_mod &lt;- linear_reg() |&gt; #set up the framework\n    set_engine(\"lm\") |&gt; #choose the linear model method as the \"engine\"\n    fit(tree_height ~ elevation, data = tree_height_x_elevation_df) #dependent ~ independent, define the dataset\n\n\nNext, to view the model that we just created we can use the tidy() function to return a nice layout:\n\n\nCode\n#get a nicer view of the information inside the object\ngt(tidy(my_lin_mod))\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.63636364\n3.494899865\n1.898871\n9.005051e-02\n\n\nelevation\n0.05036364\n0.005907459\n8.525431\n1.326928e-05\n\n\n\n\n\n\n\nWhich, on its own, it is not really anything special - just a table. But intepreting the table can allow us to understand a bit about the linear regression model that we just created.\n\nThe intercept estimate is the y-intercept (where the regression line crosses the y axis)\nThe elevation estimate is the slope of the regression line\nTogether, these define the equation of the regression line, which would be: y(pred) = 6.64 + 0.05x\n\nPutting this line on the plot from earlier demonstrates this nicely:\n\n\nCode\nggplot(tree_height_x_elevation_df, aes(x = elevation, y = tree_height)) +\n    geom_point(col = \"#8E3B46\") +\n    geom_abline(slope = 0.0517, intercept = 6.24, col = \"#00252A\") +\n    labs(x = \"Elevation (m)\", y = \"Tree Height (m)\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nPretty cool. Note how the regression line seems to split right down the middle of all our datapoints, the close the line is to all the points, the better our model is “fitted” to the data. Speaking of model fit, we can now look at the “fit” (i.e. the accuracy) of our model by the numbers!"
  },
  {
    "objectID": "posts/modelled_data_basics/index.html#evaluating-a-linear-regression-model",
    "href": "posts/modelled_data_basics/index.html#evaluating-a-linear-regression-model",
    "title": "Linear Modelling in R",
    "section": "2.3 Evaluating a Linear Regression Model",
    "text": "2.3 Evaluating a Linear Regression Model\nThe performance of a linear model can be evaluated several ways, and we are going to touch on pretty much all of them. Firstly lets bring up the tabular summary of the model again:\n\n\nCode\n#re print the tabular summary of the model\ngt(tidy(my_lin_mod))\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.63636364\n3.494899865\n1.898871\n9.005051e-02\n\n\nelevation\n0.05036364\n0.005907459\n8.525431\n1.326928e-05\n\n\n\n\n\n\n\nLooking at the table again there are a few additional columns to cover, these speak to the accuracy of the regression line.\n\nThe std.error column is the standard error of the estimate, with smaller values indicating greater precision. You can think of this as how each of the observations are from the regression line.\nThe statistic is the estimate divided by the std.error. In this case large values are often a case for some kind of statistical significance (that is to say that the std.error is much smaller than what ever the estimate value is).\nThe p.value is the one familar to most introductory statistics students, and represents the likelihood of randomly observing the slope (estimate for dependent variable - e.g. elevation). I.e. if elevation had no real effect on tree height (slope of the regression line = 0), then the chances of getting a slope as large as 0.05 just from random noise are about 0%.\n\nSo why is one row significant and one row not? Well the first row is talking about the intercept. It is saying, is the intercept statistically different from 0? I.e., when elevation is 0, is a tree of 6.64m any more or less likely than a tree of 0m? The answer is no (because the p value is high (&gt;0.05) and the statistic is low we don’t have any strong evidence to disprove this). Conversely, the second row the table is talking about the slope. It is saying, is the slope significantly different from 0? (zero being no relation) I.e. Does tree height change with elevation? The answer is yes - because the p value is low (&lt;0.05) and the statistic is high we have strong evidence to disprove the null hypothesis. Further more, because the slope is positive, not negative, we can say that tree height increases with elevation.\n\n\n\n\n\n\nNote\n\n\n\nThere are additional methods for evalutation the performance of our model, but we will explore these further into the blog."
  },
  {
    "objectID": "posts/modelled_data_basics/index.html#using-a-linear-regression-model",
    "href": "posts/modelled_data_basics/index.html#using-a-linear-regression-model",
    "title": "Linear Modelling in R",
    "section": "2.4 Using a Linear Regression Model",
    "text": "2.4 Using a Linear Regression Model\nNow that we have established our linear model is not useless, what is the point of the model, and how do we use it? Well point 1 is simply to be able to confirm “yes, tree height does change with elevation”, congratulations we can all go home. But that is kind of boring and doesn’t have a satisifying conclusion, particularly because we specifically made this data up to have that relationship. Point 2 is that we can use this model to predict the height of trees that we have never observed before.\nImagine that the data I just made up is from Hill A, and just over the way, is a second hill; Hill B:\n\nUnfortunately there is no road to get to that hill and all you know about the hill is its elevation profile, but your team is particularly interested in the height of trees there. If we assume that the tree species is the same on each hill, we can use our fancy new model to predict the height of the trees on Hill B, without ever going there.\nThis is acheived using the predict() function from the tidymodels group of packages. To use predict, obviously I need to create some elevation data for Hill B for us to predict on, so I will also do that here.\n\n\nCode\n#create another set of fake data, this time its is the elevation of Hill B, it will not contain tree height - we are going to predict that\nhill_b_elevation &lt;- data.frame(elevation = c(317, 842, 569, 74, 926, 458, 13, 731, 287, 652))\n\n#use the linear model to predict values\nhill_b_output &lt;- my_lin_mod |&gt; \n    predict(hill_b_elevation) #note that the column name must match what was used in the model\n\n#view the output\ngt(hill_b_output)\n\n\n\n\n\n\n\n\n.pred\n\n\n\n\n22.601636\n\n\n49.042545\n\n\n35.293273\n\n\n10.363273\n\n\n53.273091\n\n\n29.702909\n\n\n7.291091\n\n\n43.452182\n\n\n21.090727\n\n\n39.473455\n\n\n\n\n\n\n\nThe output of the predict function is provided as a table, rather than a vector, because a common next step with the predicted values is to join them back to the original elevation values. Thus we will do that now:\n\n\nCode\n#add the columns from the original dataset onto the predicted values\nhill_b_output &lt;- hill_b_output |&gt; \n    bind_cols(hill_b_elevation)\n\n#view the data\ngt(hill_b_output)\n\n\n\n\n\n\n\n\n.pred\nelevation\n\n\n\n\n22.601636\n317\n\n\n49.042545\n842\n\n\n35.293273\n569\n\n\n10.363273\n74\n\n\n53.273091\n926\n\n\n29.702909\n458\n\n\n7.291091\n13\n\n\n43.452182\n731\n\n\n21.090727\n287\n\n\n39.473455\n652\n\n\n\n\n\n\n\nAnd now we have predicted tree height values for trees on Hill B, without ever having gone to that hill! Thats fun.\nAlso here is a visualisation of the new data combined with the old data. Something that might not be clear until seeing this that each of the predictions land exactly on the regression line:\n\n\nCode\n#plot the original data, the line from the linear model, and the predicted dat\nggplot() +\n    geom_point(data = tree_height_x_elevation_df, aes(x = elevation, y = tree_height), col = \"#8E3B46\") +\n    geom_abline(slope = 0.0517, intercept = 6.24, col = \"#00252A\") +\n    geom_point(data = hill_b_output, aes(x = elevation, y = .pred), col = \"#E6AA04\") +\n    labs(x = \"Elevation (m)\", y = \"Tree Height (m)\") +\n    theme_bw()\n\n\n\n\n\n\n\n\n\nWhat probably comes to mind looking at this is “how accurate is this line?” Yes we know that the linear model proved there was a significant relationship between elevation and tree height, but how strong is the relationship? How accurate is that line on the graph?\nAn easy way to test the accuracy of the model is to have some training data, and some testing data. Training data is data used to train the model. This data is like the red dots on our graph, for each data point we know both the height of the tree, and the elevation. The training data is shown to the model, and the regression line is created. Testing data is additional data that we withhold from the model.\nNote that in the testing dataset we also know both the height of the tree, and the elevation. Generally, training data and testing data come from the same parent dataset, and each group is created randomly. The training dataset normally receives about 80% of the total data, and 20% of the data is withheld for testing, however the split could be whatever you want - if you can justify it.\nTo split the dataset into testing and training we can use the initial_split() function:\n\n\nCode\n#create a split of the data\nt_e_split &lt;- initial_split(tree_height_x_elevation_df)\n\n\nNote that the output of this is no longer just a df, it is a rplit object. When looking at it you can see the division of rows:\n\n\nCode\n#view object\nt_e_split\n\n\n&lt;Training/Testing/Total&gt;\n&lt;8/3/11&gt;\n\n\nTo access specifically the training or testing data from this object you can use the training() or testing() functions:\n\n\nCode\n#to see the training or testing part of the data, use training() or testing()\nt_e_split |&gt; \n    testing() |&gt; \n    gt()\n\n\n\n\n\n\n\n\nelevation\ntree_height\n\n\n\n\n600\n40\n\n\n700\n48\n\n\n1000\n55\n\n\n\n\n\n\n\nWith our dataset split we can the create a new linear model the same way we did before, but this time we are only going to show it 80% of the data (the training data):\n\n\nCode\n#train a new model on just the training data\nnew_lm &lt;- linear_reg() |&gt; \n    set_engine(\"lm\") |&gt; \n    fit(tree_height ~ elevation, data = training(t_e_split))\n\n#view new model\ngt(tidy(new_lm))\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.43055556\n4.064626303\n1.582078\n0.1647180391\n\n\nelevation\n0.04861111\n0.008129253\n5.979776\n0.0009817677\n\n\n\n\n\n\n\nWith the new model trained, we can now use it to predict values based on the testing data. Remember that in this case we know both the elevation value and the true tree height value of our testing data (this varies from the scenario above with Hill B where we only knew the elevation). The goal of predicting on values that we already know the tree height for is to see how close we get to the real answer:\n\n\nCode\n#test new model on just the testing data\ntesting_output &lt;- new_lm |&gt; \n    predict(testing(t_e_split)) |&gt; #use model to predict tree heights based on elevation\n    bind_cols(testing(t_e_split)) #bind the full testing dataset on to the predicted outputs\n\ngt(testing_output)\n\n\n\n\n\n\n\n\n.pred\nelevation\ntree_height\n\n\n\n\n35.59722\n600\n40\n\n\n40.45833\n700\n48\n\n\n55.04167\n1000\n55\n\n\n\n\n\n\n\nLooking at the table, the .pred column is the models predictions based on the elevation, and the tree_height column is the actual height of the tree measured at that elevation. The model does seem to be broadly correct, but how correct? Thankfully the tidymodels package also gives us an easy way to compare the predicted values against the true values using the metric() function:\n\n\nCode\n#you can see accuracy metrics using the metrics() function\ntesting_output_metrics &lt;- testing_output |&gt; \n    metrics(truth = tree_height, estimate = .pred)\n\ngt(testing_output_metrics)\n\n\n\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n5.0419217\n\n\nrsq\nstandard\n0.9013427\n\n\nmae\nstandard\n3.9953704\n\n\n\n\n\n\n\nOkay cool, but what do these values actually mean?\n\nRMSE is Root Mean Square Error, it is the average difference between the predicted values and the actual values. So for us, it is saying our model is on average 5.04 meters from the real value.\nRSQ is “R Squared”, it tells us as a proportion how much of the variance in the dependent variable is predictable from the independent variable. In our case it is saying our model can explain 90.13% of the variance in tree height using the elevation. Nice!\nMAE is Mean Absolute Error, it is also looking at the average distance between the predicted and actual values, but it is not squaring this number. In a nutshell this make RMSE very sensitive to large errors, but MAE treats all errors equally. In our case, the MAE is saying our model is on average 4 meters from the real value. This is a fairly similar value to RMSE and thus also tells us that there are not any particularly large errors distorting the average error."
  }
]