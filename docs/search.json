[
  {
    "objectID": "posts/tmap_migrating_to_v4/index.html",
    "href": "posts/tmap_migrating_to_v4/index.html",
    "title": "Migrating to Version 4 of the tmap R Package",
    "section": "",
    "text": "1 Introduction\nThe tmap package is one of my all time favourite R packages, and this latest update only solidified this opinion. I highly recommend that you check out the main page here, and take the time to read over some of the documents in each of the tabs.\nAs of the 27th of January, 2025, tmap version 4.0 was released, and with it came some BIG changes. The authors have done a great job making sure that the update is backwards compatible with your current code, however moving forward it is very important to start doing things the new way, as the old way will no longer be receiving updates.\nOne of the most impactful updates in my opinion, is changes to the syntax used within a lot of the core functions. This new syntax makes things easier to understand, cleaner, and provides greater flexibility in the creation of your maps. However it can be a confusing journey to undergo the transition. You will find that for a lot of the old v3 code there are now warnings/pop-up messages to help you with the transition, but there are a few things that slip few the cracks and I think it is sometimes just easier to see some examples.\nBelow I run through how my code for making maps using tmap used to look, and then discuss the changes that have happened and how things look now!\n\n\n\n\n\n\nNote\n\n\n\nNew datasets have also been added for demonstration purposes as well as the ability to extent tmap (to do things like map very unique spatial data types, or creating map overlays in a 3D environment). We won’t be covering those in this blog, but they are very interesting in their own right.\n\n\n\n\n2 How things Used to Look\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this blog I will assume a basic understanding of the tmap package and won’t be explaining in detail what each function is/does.\n\n\nRight, so in version 3.0, how did things look? Well, if I’m honest they looked a little messy. I found that there was only a moderate level of consistency between the arguments in each function, and that arguments within a function that matched together didn’t always make that fact obvious. For example, lets look at the arguments in the tm_polygons() function:\n\n\nCode\ntm_polygons(col,\n            border.col,\n            alpha,\n            palette,\n            legend.show,\n            ...) #There are many more arguments in `tm_polygons()` than what I have listed, but the general idea remains the same for the rest.\n\n\n… seems fine I guess. The arguments make sense, it looks like col would affect the colour of the polygon, and border.col would affect the border colour of the polygon. But what if I now add in the tm_borders() function:\n\n\nCode\ntm_borders(col,\n           ...)\n\n\nThe two functions share the argument col. F tm_polygons() we are pretty sure that col affects the colour of the polygon, however this tm_borders() function sounds like it has been written specifically for the borders of polygons only… So now we need the additional context of knowing that “col” in tm_polygons() changes the colour inside the polygon, but in tm_borders() changes the colour of the border of the polygon. (Technically we could guess this from contextual clues such as how tm_polygons() also has a “border.col” argument but tm_borders() does not). Confusing! Moving on, lets look at the alpha (transparency) argument. Does that change the transparency of col or border.col? You would have to read the documentation to know that. What about the palette argument? Is that the colour palette for the col or border.col argument?… Extra confusing. But it gets worse! There is only one palette argument, so how do you change the palette for the inside of the polygon independently to the palette for the border of the polygons. You can’t, so thats where tm_borders() function comes in. Just making the conflict between col more obvious. Arrgh!\nEnough ranting, lets look at a worked example of a map made using tmap version 3.0:\n\n\nCode\n#extract a subset of dat from the dataset provided with the tmap package\nexample_data &lt;- NLD_muni |&gt; \n  filter(province == \"Fryslan\")\n\n#create a map using the version 3.0 syntax\nv3_map &lt;- tm_shape(example_data) +\n  tm_polygons(col = \"name\", border.col = \"black\", alpha = 0.8, palette = \"Pastel1\", legend.show = T) +\n  tm_text(\"name\", shadow = T, auto.placement = T, size = 0.6) +\n  tm_shape(example_data) +\n  tm_borders(\"name\") +\n  tm_layout(legend.bg.color = \"white\", legend.frame.color = \"black\", asp = 1.1,\n            legend.outside = TRUE)\n\n\nAgain. Overall it doesn’t look too bad, with enough patience we can piece together what things probably do, and by trial and error we can confirm our ideas. However, it is when this code is compared to the new code that the short comings become apparent.\nBy the way, here is what the map produced by this code looks like:\n\n\nCode\n#print the map\nv3_map\n\n\n\n\n\n\n\n\n\n\n\n3 How Things Look Now\nCompared to Version 3.0, the new syntax available with tmap version 4.0 is much more consistent, concise, and also somehow does more with less. For example, lets look at the arguments in the tm_polygons() function again, right now we will just replace 1 to 1 the arguments we looked at before:\n\n\nCode\ntm_polygons(fill, #this was \"col\"\n            col, #this was \"border.col\"\n            fill_alpha, #this was \"alpha\"\n            fill.scale, #this was \"palette\"\n            fill.legend, #this was \"legend.show\"\n            ...)\n\n\nAt first glance this doesn’t seem a whole lot better, there are now just a lot of “fill” arguments. But lets have a look at tm_borders() now:\n\n\nCode\ntm_borders(col, #this was \"col\", and still is \"col\"\n           ...)\n\n\nRight away we can see the first problem has been addressed. col now always refers to the colour of the line/outline/border, whereas fill always refers to the inside/fill of the shape. Secondly, the confusion around alpha has been removed, we can see that alpha is now fill_alpha, but even cooler, there is actually also a col_alpha now, the alphas’ are independent! The same logic has been applied to palette, which is now fill.scale (too detailed to explain that change right now), and the legend arguments, which now have “fill” in front of them. So what we have now is something like this:\n\n\nCode\ntm_polygons(fill, #what variable defines the \"fill\" of the polygons\n            fill.scale, #what palette, breaks, style, should the be used to colour the polygons\n            fill.legend, #do you want a legend? What should it look like for the fill variable\n            fill.free, #should the scale be free for multiples (facets etc.)\n            fill_alpha, #how transparent should the fill colour be\n            col, #what variable defines the \"col\" (border) of the polygons\n            col.scale, #what palette, breaks, style, should the be used to colour the borders\n            col.legend, #do you want a legend? What should it look like for the col variable\n            col.free, #should the scale be free for multiples (facets etc.)\n            col_alpha, #how transparent should the border colour be\n            ...)\n\n\nIt couldn’t be more obvious what each argument does now. Lets now make the same map as above, but this time with the version 4.0 syntax:\n\n\nCode\n#create a map using the version 4.0 syntax\nv4_map &lt;- tm_shape(example_data) +\n  tm_polygons(fill = \"name\", \n              fill.scale = tm_scale_categorical(values = \"brewer.pastel1\"),\n              fill.legend = tm_legend(show = T),\n              fill_alpha = 0.8,\n              col = \"name\",\n              col.scale = tm_scale_categorical(values = \"brewer.set2\"),\n              col.legend = tm_legend(show = T)) +\n  tm_text(\"name\", \n          size = 0.6,\n          options = opt_tm_text(shadow = TRUE, \n                                point.label = TRUE)) +\n  tm_layout(legend.bg.color = \"white\",\n            legend.frame.color = \"black\",\n            legend.outside.position = tm_pos_out(\"right\", \"top\"),\n            asp = 1.1)\n\n\nTwo things are obvious in this code.\n\nWhich arguments should be associated with each other\nThe code looks slightly more verbose, but this is due to less assumptions by the functions, and more explicit directions by me\n\nSide note, you may have also noticed I have changed the structure of my code. This is purely to assist in determining which arguments match together.\nHere is what the map produced by this code looks like:\n\n\nCode\n#print the map\nv4_map\n\n\n\n\n\n\n\n\n\nIgnoring the fact that for both examples these aren’t the most visually appealing maps, it is now very easy to isolate exactly what each argument does and how we can adjust different aspects of the map. Alongside the changes surrounding the fill and col arguments, some other changes include:\n\nThe arguments: shadow = TRUE, and auto.placement = TRUE, have been nested under the new argument options, which has its own helper function opt_tm_text(). (auto.placement has also been changed to point.label). +\n\nThus: options = opt_tm_text(shadow = TRUE, point.label = TRUE)\n\nMore helper functions have been added such as the afor mentioned opt_tm_text(), the various scale helpers tm_scale_..., and functions for positioning things such as tm_pos_out.\n\nI never used to really use the helper functions before this update, but they really do make things easier. For a deeper dive on these, check out my other blog post: Making Beautiful Maps in R\n\n\n\n\n4 A Visually Appealing Map\nOkay lets be honest, I’ve been doing a lot of posturing without actually creating a map that is even remotely visually appealing. Thus, lets produce a map that is actually not half bad! Here is one I prepared earlier:\n\n\n\n\n\n\nNote\n\n\n\nThere is a lot of code that happens to produce this map, if you’re interested in learning whats going on, I break it down in detail over in my other blog post: Making Beautiful Maps in R\n\n\n\n\n\nBeautiful Map of Netherlands\n\n\nThat looks pretty good, right?\n\n\n5 Some Other Changes\nBefore I’m done, I’d like to drop a few more findings. Not only is the syntax of version 4 better, or the consistency improved, or just the visuals of the maps better. But the method for a bunch of other aspects has been improved as well. This includes things such as mapping raster data types, and faceting maps. Lets cover both at once!\nBefore the update this is how I used to create a faceted map of rasters (noting this code won’t work right now as the data isn’t available). I had to create a map for each year of data individually, then keep track of which map I was on. For the first map I created a legend, and for all other maps I didn’t create a legend. Finally, I had to stitch all of these maps together, and then I could save the result.\n\n\nCode\n#using unique regions\nfor (i in n3_marine_names) {\n  \n  #filter all basins by region\n  region_basins &lt;- n3_marine_region |&gt; filter(Region == i)\n  \n  #get the associated basins\n  basins &lt;- n3_basins |&gt; filter(Region == i)\n  \n  #create counter for j loop\n  count &lt;- 0\n  \n  #using years vector created by data sourcing script\n  for (j in time(n3_dhw_5y)){\n    \n    #track counter\n    count &lt;- count + 1\n    \n    #mask to the specific region and year\n    single_year_region &lt;- trim(mask(n3_dhw_5y[[time(n3_dhw_5y) == j]], vect(region_basins)))\n  \n    #for the first map make a legend\n    if (count == 1){\n      \n      #plot\n      map &lt;- tm_shape(single_year_region) +\n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab) +\n        tm_shape(qld) +\n        tm_polygons(col = \"grey80\", border.col = \"black\") +\n        tm_shape(region_basins, is.master = T) +\n        tm_borders(col = \"black\") +\n        tm_shape(basins) +\n        tm_polygons(col = \"grey90\", border.col = \"black\") +\n        tm_layout(asp = 5, legend.show = F, main.title = year(time(single_year_region)), main.title.position = \"centre\")\n      \n      #save the map\n      assign(glue(\"map{count}\"), map)\n      \n      #make a legend map\n      legend_map &lt;- tm_shape(single_year_region) + \n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab, legend.reverse = T, \n                  title = \"Coral bleaching likelihood \\n and number of DHW's\") +\n        tm_layout(legend.only = T, legend.title.size = 3,\n                  legend.text.size = 1.6, legend.position = c(0, 0.3))\n      \n    #otherwise, no legend\n    } else {\n        \n      #plot\n      map &lt;- tm_shape(single_year_region) +\n        tm_raster(palette = dhw_cols, breaks = c(1:6), labels = dhw_lab) +\n        tm_shape(qld) +\n        tm_polygons(col = \"grey80\", border.col = \"black\") +\n        tm_shape(region_basins, is.master = T) +\n        tm_borders(col = \"black\") +\n        tm_shape(basins) +\n        tm_polygons(col = \"grey90\", border.col = \"black\") +\n        tm_layout(asp = 5, legend.show = F, main.title = year(time(single_year_region)), main.title.position = \"centre\")\n      \n      #save the map\n      assign(glue(\"map{count}\"), map)\n        \n    }\n  }  \n  \n  #arrange into two rows\n  facet_map &lt;- tmap_arrange(map1, map2, map3, map4, map5, nrow = 2)\n  \n  #edit variable name for better save path\n  i_lower &lt;- tolower(gsub(\" \", \"-\", i))\n \n  #save the map as a png\n  tmap_save(facet_map, filename = glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}.png\"))\n  \n  #save the legend separately\n  tmap_save(legend_map, glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}_legend.png\"))\n  \n}\n\n\nAnd this is how I create them now. The change is purely down to the introduction of the tm_facets_... group of functions, which are modelling after the ggplot2 facet_wrap functions if you are familiar. Without a doubt, much easier, much simpler, much quicker to understand.\n\n\nCode\n#plot\nfacet_map &lt;- tm_shape(all_year_region) +\n  tm_raster(col.scale = tm_scale_intervals(values = dhw_cols, \n                                           breaks = c(1:6),\n                                           labels = dhw_lab),\n            col.free = FALSE,\n            col.legend = tm_legend(title = \"Coral bleaching likelihood and number of DHW's\")) +\n  tm_shape(qld) +\n  tm_polygons(fill = \"grey80\",\n              col = \"black\") +\n  tm_shape(region_basins, is.main = T) +\n  tm_borders(col = \"black\") +\n  tm_shape(basins) +\n  tm_polygons(fill = \"grey90\", \n              col = \"black\") +\n  tm_layout(panel.labels = year(time(n3_dhw_5y))) +\n  tm_facets_hstack()\n\n#save the map as a png\ntmap_save(facet_map, filename = glue(\"{output_path}/plots/{i_lower}_dhw_fyear-{current_fyear}-to-{current_fyear-4}.png\"))\n\n\n\n\n6 Caveats\nThis blog has been written documenting some of the changes that have occurred with the R package “tmap”. I explain how some of the changes have impacted my work, and cover a few instances where the update from v3 to v4 might not be so obvious. However! In no way am I pretending to understand all of the changes that have occurred. I would highly recommend visiting the main tmap page here."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html",
    "href": "posts/making_beautiful_maps_in_r/index.html",
    "title": "Making Beautiful Maps in R",
    "section": "",
    "text": "The art of the map is an ancient skill, since the age of movement has the requirement of orientation been required. Just ask the first lobe-finned fishes from over 365 million years ago. Without maps, how would they have known to walk out of the ocean!\nSeriously though the human race has been making maps since forever, and there is just something so en-capturing about it. The ability to create a useful and visually map is incredible fun, however knowing where to start can be daunting. There are just so many options, most of which are incredibly complicated, expensive, or time consuming. Just to name a few you have ArcGIS, QGIS, Google Maps, Mapbox, Inkscape, and R. Where to start? What to choose?\nToday I’d like to share how I create my maps using R. We will cover a range of things including:\n\nWhy I use R over other programs,\nWhat are the requirements of creating maps in R,\nWhat other programs I use to help me along the way,\nThe type of code you can expect to write for maps.\n\nAnd at the end of all this, I will walk through the exact code used to produce this map:\n\n\n\nNetherlands Population Map\n\n\nLets get right into it."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#requirements-to-map-in-r",
    "href": "posts/making_beautiful_maps_in_r/index.html#requirements-to-map-in-r",
    "title": "Making Beautiful Maps in R",
    "section": "2.1 Requirements to Map in R",
    "text": "2.1 Requirements to Map in R\nTo map in R you only need a couple of things. You need your spatial data (duh), you need to decide on the R package you want to use for mapping (a bit harder), and you need patience (difficulty level: 100).\nPicking the right R package might seem hard, there are a lot of options including ggplot2, leaflet, mapview, tmap, and ggmap, however, I’ll make it easy. Pick tmap. The syntax makes sense, it has almost 100% coverage of things you would want to do, and it recently got a great update. Sorted."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#other-helpful-programs",
    "href": "posts/making_beautiful_maps_in_r/index.html#other-helpful-programs",
    "title": "Making Beautiful Maps in R",
    "section": "2.2 Other Helpful Programs",
    "text": "2.2 Other Helpful Programs\nAs I noted above, I don’t restrict myself to just R for maps, I also use ArcGIS and QGIS, because there are some things that R is just never going to be able to do well. For example, StoryMaps by ESRI (ArcGIS) - which are online interactive experiences that include excellent mapping tools. StoryMaps are great for education and outreach, and to take the reader on a journey. StoryMaps are not good for creating maps for you scientific reports. Additionally, I use QGIS for quick visualizations, you can simply drag and drop your data and instantly see what it looks like - no code necessary."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#find-the-datasets",
    "href": "posts/making_beautiful_maps_in_r/index.html#find-the-datasets",
    "title": "Making Beautiful Maps in R",
    "section": "3.1 Find the Datasets",
    "text": "3.1 Find the Datasets\nThe first thing you always want to do when making maps is to gather up all of the data you want to use. You might scoff at how obvious this seems, but you would be surprised how often you get halfway through designing the style of your core layer and you realise, wait… I don’t have a layer for the background. For the map we are going to make today, we have several datasets:\n\nThe core layer: “Netherlands Province”\nThe supporting layers;\n\n“Netherlands Municipality”\n“Netherlands District”\n\nThe background layers;\n\n“World”\n“Europe”\n\n\nA lot more than you might think for one map, don’t stress, not all maps have this many layers… some have more! Thankfully, most of our data comes wrapped up within the tmap package already, so we don’t have to do any work to find those. However, we will have to find our own source for the “Europe” background layer. There are plenty of options online, but if I’m honest I can’t even remember which one I used. Anyway, lets bring all these datasets into a global environment.\n\n\n\n\n\n\nNote\n\n\n\nFun side note, a lot of R packages are loaded in with their own testing datasets you can muck around with, such as tmap with its Netherlands dataset. Even base R has its own datasets you can access right away!\n\n\n\n\nCode\n#get a visual on the district dataset (it is pre-loaded in tmap, but we can't see it in our global environment yet)\nnld_district &lt;- NLD_dist\n\n#get the CRS of our main dataset, and use this to convert all others as needed\nproj_crs &lt;- st_crs(nld_district)\n\n#get a visual on the municipality dataset\nnld_municipality &lt;- NLD_muni |&gt; \n  st_transform(proj_crs)\n\n#get a visual on the province dataset\nnld_province &lt;- NLD_prov |&gt; \n  st_transform(proj_crs)\n\n#load in the Europe background data\neurope_background &lt;- read_sf(\"Europe_coastline_poly.shp\") |&gt; \n  st_transform(proj_crs)\n\n#get a visual on the world dataset\nworld_background &lt;- World |&gt; \n  st_transform(proj_crs)\n\n\nAt this point it is usually a great idea to map each layer individually to start to get an idea of what you are working with."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#edit-the-datasets",
    "href": "posts/making_beautiful_maps_in_r/index.html#edit-the-datasets",
    "title": "Making Beautiful Maps in R",
    "section": "3.2 Edit the Datasets",
    "text": "3.2 Edit the Datasets\nThe day that I don’t have to conduct edits on my data before mapping is the same day that I win the lottery. Editing the data is another step that is often overlooked in the process of making maps, usually because in demonstrations the edits and changes have been made before hand. That will not be the case here, I will be working step by step through each of the changes I made to the raw data to ensure that I got the best visualisation possible.\nFirst up is the Europe background layer. The first issue I have with this dataset is to do with its resolution compared to the Netherlands dataset. For example, here is each layer:\n\n\nCode\ntm_shape(europe_background) + \n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\") +\n  tm_add_legend(fill = c(\"#00252A\", \"#e6aa04\"),\n                labels = c(\"Europe\", \"Netherlands\"))\n\n\n\n\n\n\n\n\n\nWe are going to assume that the border for the Netherlands dataset is indeed more accurate and precise than the Europe dataset (given the scale). Therefore, looking at these two layers we can see that there should be a gap in the Europe dataset in the upper middle portion of the Netherlands that corresponds to a shallow bay. However the Europe dataset seems to be too low resolution to pick this up, so we will have to do it ourselves. This is achieved fairly easily:\n\nConvert the Netherlands dataset into one big polygon (remove any interior detailing)\nUse this to cut a hole out of the Europe dataset.\nThe Europe dataset will no longer appear in the shallow bay.\n\n\n\nCode\n#create a single polygon of the district dataset\nnld_dist_tmp &lt;- st_union(nld_district)\n\n#remove any holes within the polygon then make the shape valid\nnld_dist_tmp &lt;- st_remove_holes(nld_dist_tmp) |&gt; \n  st_make_valid()\n                      \n#take the Europe background and cut a hole out of it using the dataset above\neurope_sans_nld &lt;- st_difference(europe_background, nld_dist_tmp)\n\n\n\n\nCode\n#the first map is before any changes\nmap1 &lt;- tm_shape(europe_background) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n   tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#The netherlands dataset as a single polygon\nmap2 &lt;- tm_shape(nld_dist_tmp) +\n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#after cutting out a section of the data\nmap3 &lt;- tm_shape(europe_sans_nld) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_dist_tmp, is.main = T) +\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#how the finished product looks\nmap4 &lt;- tm_shape(europe_sans_nld) + \n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_district, is.main = T) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\")\n\n#arrange the maps into a row of 4\ntmap_arrange(map1, map2, map3, map4, nrow = 1)\n\n\n\n\n\n\n\n\n\nNot perfect, but definitely better as we can now clearly tell that area is supposed to be a shallow bay.\nThe second issue I have with the Europe dataset is that it is too big, and it takes quite a while to create each map - which is frustrating. To fix this we are going to crop the outer area of the dataset, because we aren’t interested in that part and won’t actually be showing it on our map. This is also achieved fairly easily:\n\nConvert the Netherlands dataset into one big bounding box (I.e., the most N,E,S,W points of the dataset)\nExpand the bounding box until it is just large than the map we intent to create\nUse this to crop the Europe dataset.\nThe Europe dataset will no longer contain all of Europe.\n\n\n\nCode\n#buffer (make bigger) the temporary dataset\nnld_dist_buf &lt;- st_buffer(nld_dist_tmp, 30000) #units is meters for this dataset (it can change)\n\n#convert the temporary dataset to a bounding box object, then back to an sf object (must be sf for mapping)\nnld_dist_bb &lt;- st_as_sfc(st_bbox(nld_dist_buf))\n\n#crop the europe background dataset\neurope_final &lt;- st_intersection(europe_sans_nld, nld_dist_bb)\n\n\n\n\nCode\n#the first map is the buffered Netherlands dataset with a bbox over the top\nmap1 &lt;- tm_shape(nld_district) + \n  tm_polygons(fill = \"#e6aa04\", \n              col = \"#e6aa04\") +\n  tm_shape(nld_dist_buf) +\n  tm_polygons(fill = NULL, \n              col = \"#8E3B46\") +\n  tm_shape(nld_dist_bb, is.main = T) +\n  tm_polygons(fill = NULL,\n              col = \"#8E3B46\")\n\n#then Europe before changes with the bbox shown\nmap2 &lt;- tm_shape(europe_sans_nld) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(nld_dist_bb) +\n  tm_polygons(fill = NULL,\n              col = \"#8E3B46\") +\n  tm_shape(frame_position, is.main = T) + #this is just to position the frame, dw about it\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#after cropping the data\nmap3 &lt;- tm_shape(europe_final) +\n  tm_polygons(fill = \"#00252A\",\n              col = \"#00252A\") +\n  tm_shape(frame_position, is.main = T) + #this is just to position the frame, dw about it\n  tm_polygons(fill = NULL,\n              col = NULL)\n\n#arrange the maps into a row of 4\ntmap_arrange(map1, map2, map3, nrow = 1)\n\n\n\n\n\n\n\n\n\nA subtle change, but one which leads to much faster processing and therefore greater efficiency.\nThe Europe dataset has now received all of its edits, but we are not done yet. Next up is the “world” dataset. The problem I have with this dataset is ironically that the data is stored too efficiently! What I mean by this is that the data is stored in multipolygons, not [single]polygons. For example, a single polygon dataset would have rows of data like this:\n\n\n\nCountry\nCode\ngeom\n\n\n\n\nFrance\nFRA\npolygon(…)\n\n\nFrance\nFRA\npolygon(…)\n\n\nFrance\nFRA\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nGermany\nDEU\npolygon(…)\n\n\nand so on…\n…\n…\n\n\n\n(There are only multiple rows if there are multiple seperate landmasses belonging to the country).\nWhere as a single multipolygon dataset would have rows of data like this:\n\n\n\n\n\n\n\n\nCountry\nCode\ngeom\n\n\n\n\nFrance\nFRA\nmultipolygon(polygon(…), polygon(…), polygon(…)\n\n\nGermany\nDEU\nmultipolygon(polygon(…), polygon(…), polygon(…)\n\n\nand so on…\n…\n…\n\n\n\nIf a row shares all the same metadata, then the geometry information is combined into a single multipolygon. Normally this is fine, but what I want to do with this dataset is put country labels on the surrounding countries so that the reader can be better orientated if they are not overly familiar with Europe. The issue is the labels are essentially applied “per row”, and are put at the exact center of that rows’ geometry. Which in some cases for multipolygons…:\n\n\nCode\n#extract france from the world dataset\nfrance_example &lt;- world_background |&gt; \n  filter(name == \"France\")\n\n#create the example map\ntm_shape(world_background) +\n  tm_polygons(fill = \"#99B5B1\",\n              col = \"#7bba9d\") +\n  tm_layout(bg.color = \"#C1DEEA\") +\n  tm_shape(france_example, is.main = T) +\n  tm_polygons(fill = \"name\",\n              fill.scale = tm_scale_categorical(values = c(\"#e6aa04\", \"#8E3B46\", \"#00252A\"))) +\n  tm_text(text = \"name\")\n\n\n\n\n\n\n\n\n\nis in the middle of the ocean!\nOnce again, this is a relatively easy fix. All we need to do is convert from a dataset that stores its geometry information as multipolygons, to a dataset that stores the information as just polygons:\n\n\nCode\n#extract rows into single polygons for our example\nfrance_example &lt;- world_background |&gt; \n  filter(name == \"France\") |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  mutate(Id = row_number())\n\n#then do it for real on the dataset we will actually be using \nworld_final &lt;- world_background |&gt; \n  st_cast(\"POLYGON\") |&gt; \n  mutate(Id = row_number())\n\n#create the example map\ntm_shape(world_background) +\n  tm_polygons(fill = \"#99B5B1\",\n              col = \"#7bba9d\") +\n  tm_layout(bg.color = \"#C1DEEA\") +\n  tm_shape(france_example, is.main = T) +\n  tm_polygons(fill = \"Id\",\n              fill.scale = tm_scale_categorical(values = c(\"#e6aa04\", \"#8E3B46\", \"#00252A\"),\n                                                labels = c(\"Polygon 1\", \"Polygon 2\", \"Polygon 3\")),\n              fill.legend = tm_legend(title = \"France Polygons\")) +\n  tm_text(text = \"name\")\n\n\n\n\n\n\n\n\n\nAwesome! And that now concludes the major data edits that we needed to conduct before starting the mapping (nearly there I promise).\n\n\n\n\n\n\nNote\n\n\n\nSide note, before I create each map I do a few more minor data edits. But these don’t really need explaining and are better suited sitting with the mapping code."
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-inset-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-inset-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.3 Create the Inset Map",
    "text": "3.3 Create the Inset Map\nThe final stage to address before we get to the real juicy part, is asking the question “would this map benefit from a secondary/inset map?” Sometimes this inset map is a more zoomed in version of the main map that gives a closer look at a particular part of the area, or sometimes it is a more zoomed out version that provided context about where the main map in located in a wider region. In either case, consider if your map would be better if it had it. For the purposes of this demonstration I will of course be saying that my map needs an inset map, whether you agree or not is up to you.\nIn the case of creating an inset map that is more zoomed in, I would recommend doing that after the main map. However, when creating an inset map that is more zoomed out - I usually try to do that first. So off we go, its pretty easy, there are often only 2 or 3 components to my inset maps:\n\nThe background. Sometimes I can get away with recycling one of the main datasets, but for this map I had to get a whole new dataset. This background is the “world” dataset that we were editing above.\nThe reference box. I create a bounding box that extends the perimeter of my main map, then I use this bounding box as a layer in the inset map. This bounding box tells the reader exactly where the main map is located.\nIn some cases it might be helpful to have another bounding box if there are a few key areas. For us, we don’t need that today.\n\n\n\nCode\n#create a bounding box for the extent of our main layer and/or focus area (remember we have to covert this back to an sf object for mapping purposes)\nnld_bbox &lt;- st_as_sfc(st_bbox(nld_district))\n\n#create a larger bounding box of the main layer to use to set the perspective (play around with the distance to get what suits you)\ninset_view_positioning &lt;- nld_municipality |&gt; \n  st_buffer(dist = 1000000) |&gt; #create a much larger buffer around our main layer\n  st_bbox() |&gt; #turn the data into a bounding box\n  st_as_sfc() #turn the bounding box into an sf object (required for mapping)\n\n#create the inset map\ninset_map &lt;- tm_shape(inset_view_positioning) + #this positions our perspective\n  tm_polygons(fill = NULL, #make both null so we don't actually see anything\n              col = NULL) + \n  tm_shape(world_final) + #this is the full background\n  tm_polygons(fill =  \"#99B5B1\") + #a muted green colour (land)\n  tm_text(text = \"iso_a3\", #each country gets its named printed \n          size = 0.3, #not too big\n          options = opt_tm_text(shadow = T, #with a shadow on the text to make it pop\n                                shadow.offset.x = 0.01,\n                                shadow.offset.y = 0.01)) + \n  tm_shape(nld_bbox) + #this is the bounding box of our main layer\n  tm_borders(lwd = 2, #we could also have used tm_polygons, but lets try something new\n             col = \"#8E3B46\") + #a deep red colour\n  tm_layout(asp = 0, #set aspect to zero to make the map full fill the frame when printed\n            bg.color = \"#C1DEEA\", # mute blue colour (ocean)\n            outer.bg.color = \"#F2F2F2\") # a very subtle eggshell (the background of my website)\n\n\nDon’t worry if this seems like a lot, once you start creating your own maps you will find it all makes sense :) Lets take a quick peak at the map now as well:\n\n\nCode\ninset_map"
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-main-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-main-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.4 Create the Main Map",
    "text": "3.4 Create the Main Map\nOkay nerds, the part you’ve all been waiting for, the main map. This map has a few layers to it:\n\nThe background, we are using the Europe background for this map (not the world background) as the Europe dataset has much greater detail which is necessary for viewing up close. Remember that we have also already edited the Europe dataset to have the better borders around the Netherlands as well.\nThe Netherlands district data, the focus of our map is population count per province. This is where a lot of the detail is going to come from so we have picked the dataset with the smallest cells.\nThe Netherlands municipality data, which we just use for the larger borders.\nThe Netherlands province data. We are also using this for its borders (larger again), but just to add a pop of red into the map as that is my colour scheme.\n\nAnd that’s it, the rest of the detail comes just from the code:\n\n\nCode\n#add a value of 1 to every row of population data (because you cant log transform zero)\nnld_district &lt;- nld_district |&gt; \n  mutate(population = population+1)\n\n#edit the province dataset to have a column named \"province' (a trick to streamline the legend)\nnld_province &lt;- nld_province |&gt; \n  mutate(UseMe = \"Province\")\n\n#edit the municipality dataset to have a column named \"municipality\" (same trick)\nnld_municipality &lt;- nld_municipality |&gt; \n  mutate(UseMe = \"Municipality\")\n\n#create the map\nmain_map &lt;- tm_shape(europe_final) + #the background data\n  tm_polygons(fill = \"#99B5B1\", #a muted green (land)\n              col = \"#7bba9d\") + #a slightly darker green for the borders\n  tm_shape(nld_district, is.main = T) + #the main dataset (use \"is.main\" to make sure the perspective is set by this layer)\n  tm_polygons(fill = \"population\", #what column dictates the styling\n              fill.scale = tm_scale_continuous_log(values = c(\"#FFFFFF\", \"#00252A\"), #white to a dark green\n                                                   ticks = c(1, 10000, 100000)), #what ticks are visible in the legend\n              fill.legend = tm_legend(title = \"Population\", #what is the title of the legend\n                                      position = tm_pos_out(\"right\", \"center\"), #where is the legend positioned\n                                      title.color = \"black\", #what is the colour of the legend title\n                                      reverse = TRUE), #reverse the numbers (1 at the bottom) of the legend\n              col = NULL) + #make sure the borders between districts are not visible\n  tm_shape(nld_municipality) + #municipality borders\n  tm_borders(col = \"UseMe\", #what column dictates the styling - this will also determine the name for this legend\n             col.scale = tm_scale_categorical(values = \"#e6aa04\"), #bright orange\n             col.legend = tm_legend(title = \"\", #no title (the name is in the key instead)\n                                    position = tm_pos_out(\"right\", \"center\"), #legend positioning\n                                    lwd = 2), #how thick is the line in the legend\n             lwd = 1) + #how thick is the line on the map\n  tm_shape(nld_province) + #province borders\n  tm_borders(col = \"UseMe\", #what column dictates the styling - this will also determine the name for this legend\n             col.scale = tm_scale_categorical(values = \"#8E3B46\"), #a dark red\n             col.legend = tm_legend(title = \"\",  #no title (the name is in the key instead)\n                                    position = tm_pos_out(\"right\", \"center\")), #legend positioning\n             lwd = 2) + #how thick is the line on the map\n  tm_layout(bg.color = \"#C1DEEA\", #a muted blue (ocean)\n            legend.bg.color = \"white\", #background colour of the legend\n            asp = 0, #set aspect to zero to make the map full fill the frame when printed\n            outer.bg.color = \"#F2F2F2\") + # a very subtle eggshell (the background of my website)\n  tm_credits(\"© Data: Statistics Netherlands, Software: R-tmap\\n Map Created: Adam Shand\", #some credit text to add to the map\n             position = c(\"left\", \"bottom\")) + #where does the text go\n  tm_compass(type = \"rose\", #what type of compass\n             position = c(\"LEFT\", \"TOP\"), #where is the compass positioned\n             color.dark = \"black\", #what is the \"dark\" colour of the compass\n             color.light = \"white\", #what is the \"light\" colour of the compass\n             size = 3.5) #how big is the compass\n\n\nOnce again this might seem like a lot, but I promise it really isn’t that bad once you get the hang of it. Plus, the control you will have over the styling of your maps is amazing. Anyway, here’s how the main map looks:\n\n\nCode\nmain_map"
  },
  {
    "objectID": "posts/making_beautiful_maps_in_r/index.html#create-the-final-map",
    "href": "posts/making_beautiful_maps_in_r/index.html#create-the-final-map",
    "title": "Making Beautiful Maps in R",
    "section": "3.5 Create The Final Map",
    "text": "3.5 Create The Final Map\nThe final stage of making our map is to combine the inset map and the main map. To achieve this we need to create a “viewport” using the grid package, which is essentially a little box somewhere on the main map that we can put the inset map inside. Getting the correct positioning of the viewport can be a bit tricky as the controls are not completely intuitive, so I will give a little demonstration of how things work.\nThe viewport is created with the function viewport, which has a few main arguments; viewport(x, y, width, height, just). Each of these control the follow aspects:\n\nx = the position of the viewport on the x-axis as a proportion (scaled from 0 to 1)\ny = the position of the viewport on the y-axis as a proportion (scaled from 0 to 1)\nwidth = the width of the viewport window as a proportion of the whole image.\nheight = the height of the viewport window as a proportion of the whole image.\njust = the “starting” point of the positioning. This is where things become less intuitive.\n\nWidth and height are pretty obvious so we will skip over those. However, x, y, and just all interact with one other. For example, if we set x and y to 0.3 , and just to “right”/“bottom” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"right\", \"bottom\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport1.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 1\n\n\nBut if we, change just to “left”/“bottom” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"left\", \"bottom\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport2.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 2\n\n\nFurther again, lets change just to “left”/“top” we get the following:\n\n\nCode\n#create a viewport using the viewport function\nexample_viewport &lt;- viewport(y = 0.3,\n                             x = 0.3, \n                             width = 0.2,\n                             height = 0.2, \n                             just = c(\"left\", \"top\")) \n\n#place the viewport\ntmap_save(main_map, \n          \"example_viewport3.png\", \n          insets_tm = inset_map, \n          insets_vp = example_viewport, \n          height = 6, \n          width = 8)\n\n\n\n\n\nExample Viewport 3\n\n\nOkay, so what the hell is happening here? I have made this graphic to help explain:\n\n\n\nExample Viewport 4\n\n\nSo essentially the “just” argument is telling the x and y arguments where to measure to on the viewport. If you try to make a viewport and it doesn’t appear on your final map, the chances are that the interaction between x/y and just is putting the viewport window off the edge of the image. My positioning for the viewport is as follows:\n\n\nCode\n#place the viewport\ninset_viewport &lt;- viewport(y = 0.015, #just a tiny bit off the bottom\n                           x = 0.98, #almost all the way over to the right\n                           width = 0.33, #make window 1/3 the size of the main image\n                           height = 0.33, #make window 1/3 the size of the main image\n                           just = c(\"right\", \"bottom\")) #x and y are to the right bottom of the window\n\n#save map\ntmap_save(main_map, \"final_map.png\", \n          insets_tm = inset_map, \n          insets_vp = inset_viewport,\n          height = 6, \n          width = 8)\n\n\nHere’s how that looks:\n\n\n\nFinal Image"
  },
  {
    "objectID": "posts/learning_to_create_custom_functions/index.html",
    "href": "posts/learning_to_create_custom_functions/index.html",
    "title": "Learning To Create Your Own Custom Functions",
    "section": "",
    "text": "If you’re anything like me, when you first thought of creating your own custom functions in R it felt so wildly out of your comfort zone that you decided against it. They seemed big and scary, and only something the “really good” R coders did. The reality is far from that, and I hope this post does something to dissuade that fear and push you to start creating your own functions. Below, I’d like to discuss the essentials:\n\nWhat are functions really?\nWhy you should consider making custom functions.\nHow you can make your own functions.\nSome compelling reasons for making your own functions.\n\n\n\n\n\n\n\nNote\n\n\n\nWant to see one of my custom functions in action? I’ve already written an entire blog post about a function I use almost every day! Check it out here, it is all about cleaning and organizing dataframes (yes, I am aware that sounds boring, I promise its not)."
  },
  {
    "objectID": "posts/learning_to_create_custom_functions/index.html#an-example-of-a-useful-custom-function",
    "href": "posts/learning_to_create_custom_functions/index.html#an-example-of-a-useful-custom-function",
    "title": "Learning To Create Your Own Custom Functions",
    "section": "3.1 An Example of A Useful Custom Function",
    "text": "3.1 An Example of A Useful Custom Function\nAnyway, lets actually create our own (useful) function. When I first understood how to create a function I was super excited to get started, but I quickly realized that I didn’t actually have a good reason to write a function. I find this is the case with a lot of intermediate coders, you might know the theory, but then finding places to implement it presents a whole new challenge. So lets refresh and hopefully come up with some good ideas:\n\nFunctions are bits of code that are used lots - is there anything code you have written that you have used more than once?\nFunctions usually do one thing really well - your first function doesn’t have to change the world!\nThere are thousands of functions already out there - the “best” problems probably already have functions written for them, focus on problems specific to your niche of work to find gaps.\n\nUsing these points, here are some ideas relevant to me (I encourage you to think of your own):\n\nA function that cleans tables how I specifically like them to look (see here).\nA function that run specific statistical calculates I use for my scientific reports.\nA function that calculates landuse change by class (check out my long-form projects to read about this one).\nA function that calculates important summary statistics about fish observations.\n\nFor demonstration purposes, lets learn together how to create that fourth function; calculating summary stats for fish observation data. First, here is some example data that I made up. It has observation counts for three different fish species across four different locations:\n\n\nCode\n#read in the example dataset\nfish_obs_df &lt;- read.csv(\"fish_obs_df.csv\")\n\n#view the dataframe\ncond_form_tables(head(fish_obs_df, 10))\n\n\n\n\nLocationSpeciesObservations\n\nLocation DSpecies 20\n\nLocation BSpecies 217\n\nLocation ASpecies 39\n\nLocation ASpecies 213\n\nLocation DSpecies 19\n\nLocation CSpecies 24\n\nLocation DSpecies 215\n\nLocation CSpecies 38\n\nLocation BSpecies 310\n\nLocation ASpecies 29\n\n\n\n\n\nCode\n#plot the data\nggplot(fish_obs_df) +\n  geom_density(aes(x=Observations, color = Species, fill = Species), bw = 0.4, alpha = 0.5) +\n  scale_fill_manual(values = c(\"#e6aa04\", \"#00252A\", \"#8E3B46\")) +\n  scale_colour_manual(values = c(\"#e6aa04\", \"#00252A\", \"#8E3B46\")) +\n  theme_bw() +\n  facet_wrap(~Location)\n\n\n\n\n\n\n\n\n\nThe data looks fairly standard. Normally, we probably then proceed to calculate some basic stats like the mean, median, min, max, etc. So lets do that:\n\n\nCode\n#generic summary stats\nsummary_table &lt;- fish_obs_df |&gt; \n  group_by(Location, Species) |&gt; \n  summarise(Mean = round(mean(Observations),2),\n            Median = median(Observations),\n            Min = min(Observations),\n            Max = max(Observations),\n            Range = Max - Min) |&gt; \n  ungroup()\n\n#print the table\ncond_form_tables(summary_table)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRange\n\nLocation ASpecies 112.8 139178\n\nLocation ASpecies 211   1102222\n\nLocation ASpecies 37.82821210\n\nLocation BSpecies 115.1 1542622\n\nLocation BSpecies 217   1713207\n\nLocation BSpecies 311.9 1261711\n\nLocation CSpecies 16.11601818\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.0885116\n\nLocation DSpecies 19.84107136\n\nLocation DSpecies 28.09801919\n\nLocation DSpecies 34.94501212\n\n\n\nCool, and for fun, lets also say that we are interested to know how many times the observation count of the species was above 10 at each site:\n\n\nCode\n#number of observations above n\nsummary_table_2 &lt;- fish_obs_df |&gt; \n  filter(Observations &gt; 10) |&gt; \n  group_by(Location, Species) |&gt; \n  summarise(CountAbove10 = n()) |&gt; \n  ungroup()\n\n#add the count to the main table\nsummary_table &lt;- left_join(summary_table, summary_table_2)\n\n#print the table\ncond_form_tables(summary_table)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\nNow, for the purposes of this learning experience, lets say that this initial analysis above is something that I will need to do every time I load a dataset, and is therefore a perfect time to write a function to do the analysis for me. So do I go from the code I have written to a function? Like this:\n\nIdentify the code to go in the function (we’ve done this).\nPut the code inside the function wrapper my_custom_function &lt;- function(input){right here!}:\n\n\n\nCode\nmy_custom_function &lt;- function(inputs){\n  \n  #generic summary stats\n  summary_table &lt;- fish_obs_df |&gt; \n    group_by(Location, Species) |&gt; \n    summarise(Mean = round(mean(Observations),2),\n              Median = median(Observations),\n              Min = min(Observations),\n              Max = max(Observations),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- fish_obs_df |&gt; \n    filter(Observations &gt; 10) |&gt; \n    group_by(Location, Species) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nIdentify the inputs required for the code to run:\n\n\n\nCode\nmy_custom_function &lt;- function(inputs){\n  \n  #generic summary stats\n  summary_table &lt;- fish_obs_df |&gt; #the fish_obs_df dataset is an input, we need to tell the function what dataset to use\n    group_by(Location, Species) |&gt; #the Location and Species columns are inputs, we need to tell the function what columns to group by\n    summarise(Mean = round(mean(Observations),2), #the Observation column is an input, we need to tell the function what columns calculate on\n              Median = median(Observations),\n              Min = min(Observations),\n              Max = max(Observations),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- fish_obs_df |&gt; \n    filter(Observations &gt; 10) |&gt; #the value 10 is an input, we need to tell the function what cut off value to use\n    group_by(Location, Species) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nLooks like we have five different inputs, next we give each of those inputs their own placeholder:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; #the fish_obs_df dataset is now \"x\"\n    group_by({{y}}, {{z}}) |&gt; #the Location and Species columns are now \"y\" and \"z\" we need to use curly-curly brackets for column names provided externally\n    summarise(Mean = round(mean({{a}}),2), #the Observation column is now \"a\"\n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; #the value is now \"b\"\n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n}\n\n\n\nSpecify the output of the function:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(Mean = round(mean({{a}}),2), \n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n\n  #what should be returned?\n  return(summary_table)\n\n}\n\n\n\nRun the function:\n\n\n\nCode\nmy_custom_function &lt;- function(x,y,z,a,b){\n  \n  #generic summary stats\n  summary_table &lt;- x |&gt; \n    group_by({{y}}, {{z}}) |&gt; \n    summarise(Mean = round(mean({{a}}),2), \n              Median = median({{a}}),\n              Min = min({{a}}),\n              Max = max({{a}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- x |&gt; \n    filter({{a}} &gt; {{b}}) |&gt; #the value is now \"b\"\n    group_by({{y}}, {{z}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nnow, if we run the code, the function will appear in your global environment on the right. Congratulations, you just made a function. Lets see if it works:\n\n\nCode\n#run the function\nmy_custom_function(x = fish_obs_df,\n                   y = Location,\n                   z = Species,\n                   a = Observations,\n                   b = 10) \n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\nToo Easy!\nOk, spoiler, we are not actually done yet. First of all, lets make those place holders more helpful:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt;\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(CountAbove10 = n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nSecondly, lets make the column name for the cut off value adaptive to the actual cut off value:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt; #we have to use !!sym() when the name is not named col. We also use \":=\" in place of the normal equals\n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nThird, lets identify what kind of dependencies this function has, i.e., what kind of functions does it rely on and what packages would we have to load for it to work:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #load the required packages\n  library(dplyr)\n  library(glue)\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; #the value is now \"b\"\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt; \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nForth, what if the packages haven’t been installed? Lets add a check and warning for this:\n\n\nCode\nmy_custom_function &lt;- function(df, group_col_1, group_col_2, value, cut_off_value){\n  \n  #set a vector of names of packages we need\n  pkg &lt;- c(\"dplyr\", \"glue\")\n  \n  # Loop through each package\n  for (p in pkg) {\n    if (!requireNamespace(p, quietly = TRUE)) {\n      warning(sprintf(\"The package '%s' is not installed. Please install it with install.packages('%s')\", p, p))\n    } else {\n      library(p, character.only = TRUE)\n    }\n  }\n  \n  #generic summary stats\n  summary_table &lt;- df |&gt; \n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(Mean = round(mean({{value}}),2), \n              Median = median({{value}}),\n              Min = min({{value}}),\n              Max = max({{value}}),\n              Range = Max - Min) |&gt; \n    ungroup()\n\n  #number of observations above n\n  summary_table_2 &lt;- df |&gt; \n    filter({{value}} &gt; {{cut_off_value}}) |&gt; #the value is now \"b\"\n    group_by({{group_col_1}}, {{group_col_2}}) |&gt; \n    summarise(!!sym(glue(\"CountAbove{cut_off_value}\")) := n()) |&gt;  \n    ungroup()\n  \n  #add the count to the main table\n  summary_table &lt;- left_join(summary_table, summary_table_2)\n  \n  #what should be returned?\n  return(summary_table)\n\n}\n\n\nNow this is starting to look like a real function! Lets do some testing to make sure those adjustments worked fine:\n\n\nCode\ncut_off_is_10 &lt;- my_custom_function(df = fish_obs_df,\n                                    group_col_1 = Location,\n                                    group_col_2 = Species,\n                                    value = Observations,\n                                    cut_off_value = 10)\n\ncond_form_tables(cut_off_is_10)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove10\n\nLocation ASpecies 112.8 139178238\n\nLocation ASpecies 211   1102222136\n\nLocation ASpecies 37.8282121020\n\nLocation BSpecies 115.1 1542622225\n\nLocation BSpecies 217   1713207256\n\nLocation BSpecies 311.9 1261711192\n\nLocation CSpecies 16.1160181834\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.08851163\n\nLocation DSpecies 19.8410713669\n\nLocation DSpecies 28.0980191973\n\nLocation DSpecies 34.945012121\n\n\n\n\n\nCode\ncut_off_is_20 &lt;- my_custom_function(df = fish_obs_df,\n                                    group_col_1 = Location,\n                                    group_col_2 = Species,\n                                    value = Observations,\n                                    cut_off_value = 20) \n\ncond_form_tables(cut_off_is_20)\n\n\n\n\nLocationSpeciesMeanMedianMinMaxRangeCountAbove20\n\nLocation ASpecies 112.8 139178\n\nLocation ASpecies 211   11022221\n\nLocation ASpecies 37.82821210\n\nLocation BSpecies 115.1 154262219\n\nLocation BSpecies 217   1713207\n\nLocation BSpecies 311.9 1261711\n\nLocation CSpecies 16.11601818\n\nLocation CSpecies 23.033077\n\nLocation CSpecies 38.0885116\n\nLocation DSpecies 19.84107136\n\nLocation DSpecies 28.09801919\n\nLocation DSpecies 34.94501212\n\n\n\nLooking good to me. What we have now is our very own custom function that:\n\ntakes a dataframe, two grouping columns, a value column, and a cut-off/objective value\nand returns a summary dataframe as well as the number of observations that were above the cut-off\n\nHowever, there is still one glaring gap that I find alot of tutorials skip over… this code is still in the same script! All we have really done is make it longer and slightly abstracted so far!\nThe final stage of creating our custom function is saving and tucking away the function somewhere else so we can then refer to it later as we need. Doing this is not to hard:\n\nOpen a new R script. Not a .qmd file, or a markdown file, a pure R script.\nCopy and paste the custom function into the new R script.\nSave this script somewhere relevant, I like to create a folder in my work space called “functions”.\n\nDone. To access the function that we just put inside the script we then write the following code:\n\n\nCode\nsource(\"path_to_script/script_name.R\")\n\n\nThis will load the function into your global environment ready for use."
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html",
    "href": "posts/going_loopy_for_for_loops/index.html",
    "title": "Going Loopy for For Loops",
    "section": "",
    "text": "Here’s the scene, you’ve started on your R coding journey, know how to create an object, how to discern between vectors and lists, maybe even written a few short scripts. Then all of a sudden your professor/teacher/boss pulls a fast one on you and introduces for loops. For loops? What are they? How’s that work? Whats going on? You struggle through and complete the task, but didn’t quite understand what was going on when they explained it to you… Well at least that’s how it went for me.\nIn this post I wanted to quickly talk about for loops in R, specifically, I’m looking to cover:\n\nWhat is really happening in a for loop\nWhere you can go to read about for loops in much (much) more detail\nWhy you might want to write a for loop\nHow you can start to write your own loops\nAnd ironically, why I actually try to avoid using for loops"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#i-the-iteration",
    "href": "posts/going_loopy_for_for_loops/index.html#i-the-iteration",
    "title": "Going Loopy for For Loops",
    "section": "2.1 i The Iteration",
    "text": "2.1 i The Iteration\n“i” can be anything, which is not super helpful (sorry). What might be helpful is just seeing an example. This code:\n\n\nCode\n#write a for loop to print the numbers 1 to 10\nfor (PotatoSalad in 1:10){\n  print(PotatoSalad)\n}\n\n\nwill produce the exact same result as this code:\n\n\nCode\n#write a for loop to print the numbers 1 to 10\nfor (i in 1:10){\n  print(i)\n}\n\n\nWe use “i” as an iteration counter (hence usually getting called “i”) that keeps track of what loop we are on with respect to n. For our example above, n is 10. So on the first loop “i” is 1, on the second loop “i” is 2, on the third loop “i” is three…, all the way until “i” is 10. At that point, the for loop finishes. Hopefully it is now intuitive to see how print(i) produces the numbers it does."
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#n-the-range",
    "href": "posts/going_loopy_for_for_loops/index.html#n-the-range",
    "title": "Going Loopy for For Loops",
    "section": "2.2 n The Range",
    "text": "2.2 n The Range\n“n” is the range of the loop. “n” tells the for loop two things;\n\nHow many times to continue looping\nWhat elements to loop over\n\nIn our example above, n is 10. But wait, that’s not quite right, n is actually 1 to 10. This is an important distinction to make because it is usually one of the first places we make errors. Lets take a look:\n\n\nCode\n#write a for loop to print the number 10 (spot the issue)\nfor (i in 10){\n  print(i)\n}\n\n\n[1] 10\n\n\nAs you can see, when “n” is just 10, then the output is only the value “10”. This is because the range (AKA length) of “n” was only 1. An easy way to check this is to use the length() function:\n\n\nCode\nlength(10)\n\n\n[1] 1\n\n\n\n\nCode\nlength(1:10)\n\n\n[1] 10\n\n\nSo the first super important thing to remember is that “n” is a range, it has a point you want to start at, and a point you want to end at. The second super important thing to remember about “n” is that it directly tells the specific value to start and end at, and therefore determine the value that “i” is going to be. Here is a simple demonstration:\n\n\nCode\n#write a for loop to print the numbers 15 to 22\nfor (i in 15:22){\n  print(i)\n}\n\n\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n[1] 21\n[1] 22\n\n\nIn this case, we started at 15 and ended at 22. So on the first loop, “i” is 15, second loop “i” is 16, etc.\nThe last super important thing to remember about “n” is that it does not have to be numeric! It took me a while to realise this, but it can allow you to do some cool things. Here is another quick example:\n\n\nCode\n#create a vector of character elements\nn_range &lt;- c(\"PotatoSalad\", \"FishFingers\", \"Im... Kinda Hungry\")\n\n#write a for loop to print this vector one element at a time\nfor (i in n_range){\n  print(i)\n}\n\n\n[1] \"PotatoSalad\"\n[1] \"FishFingers\"\n[1] \"Im... Kinda Hungry\"\n\n\nThis does throw a minor curve ball though. Did you notice that the code is now for (i in n_range), there is no “1:” in front of “n_range”, this is just because the object “n_range” already has a range of elements that we can loop over. Lets use length() again to show this:\n\n\nCode\nlength(n_range)\n\n\n[1] 3\n\n\nIf you can remember these core things about for loops you will get very, very far with them. So to summarise. “n”:\n\nMust be a range, it needs a start and end point\nTells “i” what value it is going to be\nCan be a numeric, or character!"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#f-the-function",
    "href": "posts/going_loopy_for_for_loops/index.html#f-the-function",
    "title": "Going Loopy for For Loops",
    "section": "2.3 f The Function",
    "text": "2.3 f The Function\nThe final part of a for loop is by far the biggest part of the code, but it is ironically very straight forward to understand if you have a little bit of an R background. The function or functions inside a for loop are the exact same functions that you would be using outside a for loop! The hard part to figure out is where to place that stupid “i” value. Personally, I haven’t been able to find a method of explaining where “i” goes other than by being very conscious of the purpose of your for loop. Start with simple and short loops and work your way to more complicated tasks, it will come naturally. Generally you will find that “i” only needs to be placed in a few key locations, however if you miss a spot, happy debugging!\n\n\n\n\n\n\nNote\n\n\n\nWant to learn more about For Loops from the professionals? Check out the Iteration chapter in R for Data Science!"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#the-scenario",
    "href": "posts/going_loopy_for_for_loops/index.html#the-scenario",
    "title": "Going Loopy for For Loops",
    "section": "3.1 The Scenario",
    "text": "3.1 The Scenario\nSomething I do almost every day is make maps. Usually fairly simple maps, often they show sample site locations, or coral monitoring locations, or the size of a seagrass meadow, things like that. These maps are included in static word documents and are often needed over large chunks of areas. However, the combination of a large study location, a high quantity of sample site locations, and a static output (can’t put an interactive map into word), means that instead of one large map, I need to create lots of small maps for each little area.\nThis here, is an absolutely prime example of a compelling reason to use a for loop. To give you some numbers, in one of my projects I need to create 67 maps. If I was to manually write out the code for each of those 67 maps, my script would have 3886 lines of code just dedicated to creating the maps. Instead, I use a for loop and pull 67 maps out of less than 100 lines of code. Not only that, but I also reduce the chance of an error sneaking into my code by 67x.\nBelow is a simplified mock up of the code I would use for this, noting that I have used made up sampling locations for data privacy, and created interactive maps for your enjoyment. We will see the full code in action first, then break it down step by step.\n\n\nCode\n#read in some example data that I made up\nexample_sites &lt;- st_read(\"example_data.gpkg\")\n\n#extract an object that contains the three unique locations we are looking at\nlocations &lt;- unique(example_sites$Location)\n\n#create a list that will store my maps\nlist_of_maps &lt;- setNames(vector(\"list\", length(locations)), locations)\n\n#initialize the for loop\nfor (i in locations){\n  \n  #filter our dataset\n  sub_set_of_sites &lt;- example_sites |&gt; \n    filter(Location == i)\n  \n  #create a simple map\n  single_map &lt;- tm_shape(sub_set_of_sites) +\n    tm_dots(shape = \"Site\", size = 1, col = \"Site\", fill = \"Site\") +\n    tm_text(\"Site\", size = 2, ymod = 1) +\n    tm_layout(legend.show = F)\n  \n  #add each map to the list\n  list_of_maps[[i]] &lt;- single_map\n  \n}\n\n\nHere is how one of the maps looks:\n\n\nCode\n#view the map\nlist_of_maps[[\"Alligator Creek\"]]"
  },
  {
    "objectID": "posts/going_loopy_for_for_loops/index.html#the-breakdown",
    "href": "posts/going_loopy_for_for_loops/index.html#the-breakdown",
    "title": "Going Loopy for For Loops",
    "section": "3.2 The Breakdown",
    "text": "3.2 The Breakdown\nTime to take a closer look at whats happening here.\n\nFirst of all, i use a function called st_read() from the sf package to load in my dataset. For the purposes of this post, we don’t need to worry about this package and its functions. Check out my other posts for more details on this area. What I will do here though, is show a sneak peak of the data.\n\n\n\nCode\n#read in some example data that I made up\nexample_sites &lt;- st_read(\"example_data.gpkg\")\n\n\n\n\n\n\nLocationSiteXY\n\nAlligator CreekSite 1147-19.3\n\nAlligator CreekSite 2147-19.3\n\nAlligator CreekSite 3147-19.3\n\nThe StrandSite 1147-19.2\n\nTown CommonSite 1147-19.2\n\nTown CommonSite 2147-19.2\n\nTown CommonSite 3147-19.2\n\nMagnetic IslandSite 1147-19.2\n\nMagnetic IslandSite 2147-19.2\n\n\n\n\nFrom this dataset I then extract a vector of unique locations, which in this case we can easily see is just the four (Alligator Creek, The Strand, Town Common, Magnetic Island).\n\n\n\nCode\n#extract an object that contains the three unique locations we are looking at\nlocations &lt;- unique(example_sites$Location)\n\n\n\nI then create a list to store the outputs of my for loop. This step can be done in a wide range of ways, for example you could store each output as a separate object, if you know the number of outputs you could pre-define a list of that length to store the outputs (like I did), or if the number of outputs is a mystery you can grow the list as you go. There is no “best” way to do this, however it is generally frowned upon to grow the list as you go, as this can be computationally quite expensive. My recommendation would be to use the first two options, favoring a list with a pre-defined length if you can.\n\n\n\nCode\n#create a list that will store my maps\nlist_of_maps &lt;- setNames(vector(\"list\", length(locations)), locations)\n\n\n\nThe set up is done and it is now time to begin the for loop. This section of the code is a good time to review what we discussed above. We can see that I am going to loop over “locations”, and for each loop “i” will become of the elements in “locations”.\n\n\n\nCode\n#initialize the for loop\nfor (i in locations){\n\n\n\nWe are now working within the for loop. Remember, this section of the code will be run again and again and again. The first thing we do inside the for loop is take a subset of our data. We can filter the data by “i” because “i” has taken on the first element of “location”.\n\n\n\nCode\n  #filter our dataset\n  sub_set_of_sites &lt;- example_sites |&gt; \n    filter(Location == i)\n\n\n\nUsing the subset of the data, which will now only contains rows from one location thanks to our filter. We then create the map. I like to use the tmap package, however there is a wide range of options available. Maybe I will write a post on mapping with tmap one day… we will see.\n\n\n\nCode\n  #create a simple map\n  single_map &lt;- tm_shape(sub_set_of_sites) +\n    tm_dots(shape = \"Site\", size = 1, col = \"Site\", fill = \"Site\") +\n    tm_text(\"Site\", size = 2, ymod = 1) +\n    tm_layout(legend.show = F)\n\n\n\nThe final step of our for loop is to save the output of the loop somewhere. This step can catch alot of people off guard, they write the perfect loop, they check everything runs properly, and then they forgot to save the output each loop. Shame.\n\nIn my case, I have put the map into the list that we defined earlier. Notice that because I named each item in the list, I can then place the map under the correct item using “i”.\n\n\nCode\n  #add each map to the list\n  list_of_maps[[i]] &lt;- single_map\n  \n}"
  },
  {
    "objectID": "posts/an_opinionated_dataframe_cleaner/index.html",
    "href": "posts/an_opinionated_dataframe_cleaner/index.html",
    "title": "An Opinionated Dataframe Cleaner",
    "section": "",
    "text": "1 Introduction\nInconsistent and illogical naming conventions can ruin even the best analysts flow, cause sneaky errors, and potentially lead to misleading or completely incorrect results. Throughout my time as an environmental data analyst I have come across countless instances where the names used in a dataframe mess up my analysis, and I can guarantee I’m not the only one. Just Google “the importance of file naming” to find countless monologues (just like this one), or “bad naming conventions” to realize, actually it could be worse!\nSo if this is such a widely acknowledged issue, why is it still an issue? How has it not been fixed? Simply put, because a) its boring, and b) everyone is unique and has their own idea of what a “good” system looks like. This leads to people not bothering, or instances where you might pull together several datasets from a range of sources, each using their own (different) naming conventions. Thankfully, if each dataset is at least internally consistent, we can address these differences.\nBelow, I introduce my method of addressing this issue. It is a highly opinionated dataframe cleaner that focuses exclusively on ensuring every dataframe I touch receives exactly the same column naming convention. Before we dive into it, I believe it is critical to recognise that this method is customized to my needs, it may work for you as well, but I recommend instead that you use this as inspiration to develop your own method.\n\n\n2 The Naming Convention\nSo what naming convention am I using exactly? “Upper Camel Case” is my choice, however some people may also refer to it as “Pascal Case”. If your are unfamiliar, here are some examples of naming conventions:\n\nUpperCamelCase\nsnake_case\nkebab-case\nUPPERFLATCASE\netc.\n\nWhy UpperCamelCase? As noted above, everyone has their own idea of what is good. I find that upper camel case suite my purposes well, it is fairly easy to read, it only contains A-Z, 0-9 (no underscores or dashes), and most importantly it does not clash with my object names when coding it in R. What I mean by this is that I use snake_case to name my objects, and UpperCamelCase to name columns within my objects. Lets consider the following example.\nLets say I have a dataframe that counts fish (called “fish”):\n\n\nCode\n#load the dplyr package\nlibrary(dplyr)\n\n#create an example dataframe\nfish &lt;- data.frame(species = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                   fish_count_location_1 = c(6,9,3,5,10),\n                   fish_count_location_2 = c(1,16,3,2,7))\n\n#print the dataframe. If you want to learn about this function, check out my pretty tables post!\ncond_form_tables(fish)\n\n\n\n\nspeciesfish_count_location_1fish_count_location_2\n\nA61\n\nB916\n\nC33\n\nD52\n\nE107\n\n\n\n(Note that both the object and column names are in snake_case).\nThen I decide to figure out the mean number of each species of fish, across all locations (called “mean_fish”):\n\n\nCode\n#get the rowwise mean of the fish counts per species\nmean_fish &lt;- fish |&gt; \n  rowwise() |&gt; \n  mutate(mean_fish = mean(c(fish_count_location_1, fish_count_location_2))) |&gt; \n  ungroup()\n\n#print the dataframe\ncond_form_tables(mean_fish)\n\n\n\n\nspeciesfish_count_location_1fish_count_location_2mean_fish\n\nA613.5\n\nB91612.5\n\nC333  \n\nD523.5\n\nE1078.5\n\n\n\nWhoops, just by using some logical naming I now accidentally have a dataframe object named “mean_fish”, and a column within that dataframe named “mean_fish”. Now obviously this is a silly example, but imaging we have 1000+ lines of code, and we need to know something about the mean number of fish. Suddenly we can’t remember whats an object and whats a column and we can run into subtle errors, or have very confusing lines of codes.\nThus; my final reason for choosing UpperCamelCase:\n\n\nCode\n#create a new example dataframe\nfish &lt;- data.frame(Species = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                   FishCountLocation1 = c(6,9,3,5,10),\n                   FishCountLocation2 = c(1,16,3,2,7))\n\n#get the rowwise mean of the fish counts per species\nmean_fish &lt;- fish |&gt; \n  rowwise() |&gt; \n  mutate(mean_fish = mean(c(FishCountLocation1, FishCountLocation2))) |&gt; \n  ungroup()\n\n#print the dataframe\ncond_form_tables(mean_fish)\n\n\n\n\nSpeciesFishCountLocation1FishCountLocation2mean_fish\n\nA613.5\n\nB91612.5\n\nC333  \n\nD523.5\n\nE1078.5\n\n\n\n\n\n3 The Function\nMy custom function takes advantage of the janitor R package, which includes a wide range of functions to perform standard cleaning and organisation steps (check out the janitor documentation to see what it can do). Specifically, we are going to use the clean_names() function, along with some bells and whistles to catch our edge cases. Lets take a look:\n\n\nCode\n#create the custom function\nname_cleaning &lt;- function(df){\n\n  #load and install (if required) the pacman package handler package, which we will use for all future package downloads\n  if(!require(\"pacman\")){install.packages(\"pacman\")}\n\n  #use the pacman function to load and install (if required) all other packages\n  pacman::p_load(janitor, dplyr, sf, stringr)\n\n  #check if the df is an sf object and if so, apply clean names to every column but the last column\n  if(inherits(df, \"sf\")){\n    \n    #convert all but the geometry column to upper camel type\n    df_new &lt;- df |&gt; \n      st_drop_geometry() |&gt;\n      clean_names(case = \"upper_camel\")\n    \n    #bind the geometry column back on with its new name. Note that it should also be named \"geom\"\n    df_new &lt;- df_new |&gt;\n      dplyr::mutate(geom = st_geometry(df)) |&gt; \n      st_as_sf()\n  \n  } else {\n    \n    #convert ALL columns to upper camel type, don't have to worry about geometry\n    df_new &lt;- df |&gt; \n      clean_names(case = \"upper_camel\")\n    \n  }\n  \n  #for every character type column, run a encoding check and fix, then remove weird new line characters\n  df_new &lt;- df_new  |&gt; \n    mutate(across(where(is.character), ~ iconv(., from = 'UTF-8', to = 'ASCII//TRANSLIT'))) |&gt; \n    mutate(across(where(is.character), ~str_replace_all(., \"\\r\\n\", \" \")))\n  \n  return(df_new)\n  \n}\n\n\nOk, so even though that is a relatively short function, there is still a few things going on. Lets break it down a bit.\n\nFirst we will initialize the function (if you are unfamiliar with creating your own functions check out my functions post).\n\n\n\nCode\n#initialize the function\nname_cleaning &lt;- function(df){\n\n\n\nThen we load each of our required packages. Noting that generally we would expect these packages to already have been loaded in by the script calling this function, but we can’t be sure. Here we use the pacman package to make the install/load steps a bit more streamline, documentation for pacman can be found here.\n\n\n\nCode\n  #load and install (if required) the pacman package handler package, which we will use for all future package downloads\n  if(!require(\"pacman\")){install.packages(\"pacman\")}\n\n  #use the pacman function to load and install (if required) all other packages\n  pacman::p_load(janitor, dplyr, sf, stringr)\n\n\n\nWe then check if the dataframe we are cleaning is actually an “sf” (simple feature) object. Sf objects are special types of dataframes used in geospatial analytics that have an extra column containing coordinate information. This special column has its own rules for column naming and therefore sf objects should be handled differently. In my work I encounter sf objects very often.\n\n\n\nCode\n  #check if the df is an sf object and if so, apply clean names to every column but the last column\n  if(inherits(df, \"sf\")){\n\n\n\nIf we are looking at an sf object, we copy the sf object and remove the geometry column from this copy. Following this, we can then run janitor’s clean_names() function on the copy with no geometry column. The reason we do this is that the janitor package has no precedent for sf objects. In the clean_names() function, we specify that we want the column names to follow the “upper_camel” format. This will convert all our column names to the desired format.\n\n\n\nCode\n    #convert all but the geometry column to upper camel type\n    df_new &lt;- df |&gt; \n      st_drop_geometry() |&gt;\n      clean_names(case = \"upper_camel\")\n\n\n\nOnce we have cleaned the names of every column in the sf object, we can then add the special geometry column back on to the dataset. At this point we also need to convert the object back to the “sf” type.\n\n\n\n\n\n\n\nNote\n\n\n\nYou may notice that this special geometry column is called “geom” rather than “Geom”… which doesn’t adhere to our naming convention. Unfortunately, this is an annoying quirk of spatial datasets. When they are loaded, the geometry column can take on 1 of 3 different names depending on the source of the data; “geom”, “geometry”, or “shape”. In all cases the name is lowercase, even when the data is saved in uppercase, it will be reloaded in lowercase. Thus, for this issue, we simply ensure that the 3 different possibilities are all just converted to the “geom” option.\n\n\n\n\nCode\n    #bind the geometry column back on with its new name. Note that it should also be named \"geom\"\n    df_new &lt;- df_new |&gt;\n      dplyr::mutate(geom = st_geometry(df)) |&gt; \n      st_as_sf()\n\n\n\nIf the object is a simple dataframe (not an sf object), we can just move straight to the clean_names() step that we explained above.\n\n\n\nCode\n  } else {\n    \n    #convert ALL columns to upper camel type, don't have to worry about geometry\n    df_new &lt;- df |&gt; \n      clean_names(case = \"upper_camel\")\n    \n  }\n\n\n\nNext we look to catch strange edge cases related to the encoding column of columns. You are likely familiar with the concept of a column being of type “character” or “numeric” or “boolean”, etc. Our strange edge case is similar to this. What we have found is that in some instances the character column type is encoded as “UTF-8”, while other times it is encoded as “ASCII”. Much like how you can’t combine character and numeric columns, you also can’t combine columns encoded as UTF-8 and ASCII. Below we convert all columns encoded as UTF-8 to ASCII to avoid this issue.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease note that these encodings are hidden from the user and you will never normally need to interact with them, the reason this happens doesn’t matter, and is frankly some mysterious property of excel. Broadly, you probably don’t need to ever understand why/how this step works.\n\n\n\n\nCode\n  #for every character type column, run a encoding check and fix, then remove weird new line characters\n  df_new &lt;- df_new  |&gt; \n    mutate(across(where(is.character), ~ iconv(., from = 'UTF-8', to = 'ASCII//TRANSLIT'))) |&gt; \n    mutate(across(where(is.character), ~str_replace_all(., \"\\r\\n\", \" \")))\n\n\n\nThe object is then returned and the function is complete.\n\n\n\nCode\n  return(df_new)\n  \n}\n\n\n\n\n4 In Practice\nNow that we understand how the function works, lets demonstrate its use with another example dataset that has a wide range of column names. Here is before:\n\n\nCode\n#create an example table with example names\nexample_df &lt;- data.frame(\"column 1\" = c(1,2,3,4,5),\n                         \"column-2\" = c(\"A\", \"B\", \"C\", \"D\", \"E\"),\n                         \"column_3\" = c(NA, NA, NA, NA, NA),\n                         \"column-four\" = c(\"1A\", \"2B\", \"3C\", \"4D\", \"5E\"),\n                         \"Column Five\" = c(TRUE, FALSE, TRUE, FALSE, TRUE))\n\n#print the table\nprint(example_df)\n\n\n  column.1 column.2 column_3 column.four Column.Five\n1        1        A       NA          1A        TRUE\n2        2        B       NA          2B       FALSE\n3        3        C       NA          3C        TRUE\n4        4        D       NA          4D       FALSE\n5        5        E       NA          5E        TRUE\n\n\nAnd after:\n\n\nCode\n#run the clean name functions\nexample_df_cleaned &lt;- name_cleaning(example_df)\n\n#print the cleaned dataset\ncond_form_tables(example_df_cleaned)\n\n\n\n\nColumn1Column2Column3ColumnFourColumnFive\n\n1A1ATRUE\n\n2B2BFALSE\n\n3C3CTRUE\n\n4D4DFALSE\n\n5E5ETRUE\n\n\n\n\n\n5 Caveats\nIt is also important to acknowledge the caveats of your own work. To my knowledge the only caveat of this function is that it relies on a sensible preexisting column name, even if the format is horrible. What I mean by this is that a column named “Mean-fish_in Townsville” can be cleaned, but a column with no name… well how can you rename that to something appropriate? As a side note R does generally replace empty column names with “X1”, “X2”, etc. however this still does not provide any information about the column."
  },
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "ADAM SHAND",
    "section": "",
    "text": "Document\n\n\n\n  \n    \n      \n    \n      Download Current CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, Im Adam.",
    "section": "",
    "text": "Document\n\n\n\n  \n    \n      \n      An environmental data analyst decoding nature's secrets.\n      \n      \n       With experience in R, Tableau, SQL and GIS, I transform raw environmental data into meaningful narratives with stunning visuals. From crafting data-driven solutions to fostering sustainability, I am dedicated to bridging the gap between technology and the environment."
  },
  {
    "objectID": "posts/automated_output_organisation/index.html",
    "href": "posts/automated_output_organisation/index.html",
    "title": "Automate Your Output Folders",
    "section": "",
    "text": "1 Introduction\nBeing organised is hard work, no doubt about it. Being consistent is hard work, no doubt about it. But automating your output folders to be both organised and consistent? Well that’s easy, and today I’ll be showing you how.\nLets set a scene that I’m sure many of you have encountered. You create a new repository for your next work task or personal project, initially you take your time, you’re careful, you organise everything neatly. But time goes on and competing priorities appear, suddenly you are in a rush, naming falls to the wayside and you just need to get things done. One day you look back at your work and you realise you have created a monster, your results folder probably looks something like this…\n\nand your scripts folder a bit like this:\n\n… its a sight to behold…\n\n\n\n\n\n\nNote\n\n\n\nI will give you $100 if you successfully match the output(s) to the script that produce it.\n\n\nconceptually, you could represent the above arrangement like this:\n\nWith every script and every output all dumped into the project directory folder.\n\n\n2 The Manual Fix\nNot to worry, to keep things organised you put your scripts into a scripts folder. Maybe you have so many scripts that you also create sub folders!:\n\n\n\nVariation 1\n\n\n\n\n\nVariation 2\n\n\nhowever, in both these cases, the outputs are still getting dumped directly into the project directory. The natural next step here is to create a folder that stores all of the outputs:\n\nOr, if you have sub folders it could look like this:\n\nBut that’s hard work! And as we covered above, overtime the maintenance on these structures fall away and get forgotten.\n\n\n3 The Automated Solution\nThankfully there is an easy fix, a little something called getting the active document context. Effectively what we are going to do is create a function that we can run at the start of every single script. This function will get information about the active document (i.e. the script being run), and use this to create a series of folders and paths that perfectly mirror the path to the script.\nFor example by running rstudioapi::getActiveDocumentContext() we return a bit of information about the path of the script, the id, and where the mouse is. We can use this to our advantage by changing the function to rstudioapi::getActiveDocumentContext()$path and just selecting the path. This will return something like this:\n“~/GitHub/scripts/posts/automated_output_organisation/climate_final_report.qmd”\nFollowing this, we can isolate the specific name of the script using a little bit of stringr() magic, which leaves us with the string “climate_final_report”. Next, we can inject this string into the dir.create() function and hey presto the function automatically creates a folder with the exact same name as the script, right next to our script. Then we can just save the path to this folder as an object that we can reference whenever we want to save one of our outputs.\nThe bones of this function are presented below:\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load in the required libraries\n  library(rstudioapi)\n  library(stringr)\n  \n  #get the file path and name of the active document\n  script_path &lt;- getActiveDocumentContext()$path\n    \n  #remove the unnecessary components of the file path\n  output_path &lt;- str_remove_all(script_path, \".*/|.qmd|.R\")\n  \n  #create a folder at the location\n  dir.create(output_path)\n  \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nHowever, there are a two key issues with this set up so far:\n\nThis function only works when you manually run an R script or quarto document, it fails when you try to render a quarto document (bit too complicated to cover why here).\nThis function is currently assuming the output folder should be created in the same folder that the script is located - in our examples above this would only work in the very first conceptual diagram.\n\nThe first issue is a relatively simple fix, we just need to include the “quarto render” version of the rstudioapi::getActiveDocumentContext() function, which is knitr::current_input(). Below we update the function to first try rstudioapi::getActiveDocumentContext() , and if that fails, switch to knitr::current_input():\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load in the required libraries\n  library(rstudioapi)\n  library(stringr)\n  \n  #try the first method, if it fails \"result\" becomes an object of class \"try-error\"\n  result &lt;- try({\n  \n    #get the file path and name of the active document\n    script_path &lt;- getActiveDocumentContext()$path\n      \n    #remove the unnecessary components of the file path which is everything left of the last /, and the \".qmd\" or \".R\"\n    output_path &lt;- str_remove_all(script_path, \".*/|.qmd|.R\")\n    \n  }, silent = TRUE)\n  \n  #if the result object became a class \"try-error\" then we attempt the second method\n  if (inherits(result, \"try-error\")){\n    \n    #get the file path and name of the active document\n    script_path &lt;- knitr::current_input()\n    \n    #remove unnecessary components of the file path\n    output_path &lt;- str_remove_all(script_path, \".rmarkdown\")\n  }\n   \n  #create a folder at the location\n  dir.create(output_path)\n    \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nHowever the second issue is a bit more abstract as it depends on your exact folder structure. As we covered above, your scripts might be stored in a “scripts/” folder, or maybe there are layers of sub folders.\nThe best solution I have found for this is to mirror the script folder organisation into a newly created “outputs” folder. This is achieved by modifying the function to use here(). What here() does is return a path to the R project directory that the script is being run within (i.e. the parent folder in our conceptual diagram). By combining the here() function with our custom function above we can isolate the exact path from the parent folder of the project to the script being run, and then mirro this to create the folder and path structure for our outputs folder. For example, if we use rstudioapi::getActiveDocumentContext() and the path to the script is:\n“~/GitHub/website/scripts/automated_output_organisation/climate_final_report.qmd”\nand the here() function returns:\n“C:/Users/adams/XYZ/Documents/GitHub/website”\nwe can first identify that the parent folder is the final phrase from the here() function, which in our case is “website”. Following this, we can then see that everything between “website” and the file type from the rstudioapi::getActiveDocumentContext() function must be the folder(s) in which the R script is stored and the name of the script. In our case this would be:\n“scripts/automated_output_organisation/climate_final_report/”\nfinally, we just take this string and substitute any text before the first slash (/) with “output” to create:\n“output/automated_output_organisation/climate_final_report/”\nConceptually this would look almost exactly the same as our previous diagram, except that we have created an additional folder with the same name as the script to provide clarity on the source of the output, and that all of highlighted folders were created automatically:\n\nThe code to achieve this is as follows:\n\n\nCode\n#create a function that makes an output folder and path that matches the script calling it\nauto_output &lt;- function(){\n  \n  #load the required packages\n  library(rstudioapi)\n  library(stringr)\n  library(here)\n  library(glue)\n  \n  #try the first method, if it fails \"result\" becomes an object of class \"try-error\"\n  result &lt;- try({\n  \n    #get the file path and name of the active document\n    script_path &lt;- getActiveDocumentContext()$path\n\n    #get the name of the folder that the R project sits in. Regex is to grab everything after the last slash\n    parent_folder_name &lt;- str_extract(here(), \"[^/]+$\")\n    \n    #get everything after the parent folder, note this may be several sub folders.\n    sub_folder_names &lt;- str_extract(script_path, glue(\"(?&lt;=/{parent_folder_name}/).*\"))\n    \n    #replace the very first subfolder with \"output\" this will split any additional subfolders off down the output folder chain\n    output_path &lt;- str_replace(sub_folder_names, \"^[^/]+\", \"output\")\n    \n    #drop the \".qmd\" or \".R\" off the end\n    output_path &lt;- str_remove_all(output_path, \".qmd|.R\")\n\n  }, silent = TRUE)\n  \n  #if the result object became a class \"try-error\" then we attempt the second method\n  if (inherits(result, \"try-error\")){\n    \n    #get the file path and name of the active document\n    script_path &lt;- paste0(getwd(), \"/\", knitr::current_input())\n    \n    #get the name of the folder that the R project sits in. Regex is to grab everything after the last slash\n    parent_folder_name &lt;- str_extract(here(), \"[^/]+$\")\n    \n    #get everything after the parent folder, not this may be several sub folders.\n    sub_folder_names &lt;- str_extract(script_path, glue(\"(?&lt;=/{parent_folder_name}/).*\"))\n    \n    #replace the very first subfolder with \"output\" this will split any additional subfolders off down the output folder pathway\n    output_path &lt;- str_replace(sub_folder_names, \"^[^/]+\", \"output\")\n    \n    #remove unnecessary components of the file path\n    output_path &lt;- str_remove_all(sub_folder_names, \".rmarkdown\")\n    \n  }\n   \n  #create a folder at the location\n  dir.create(here(output_path), recursive = TRUE)\n    \n  #save the location as an object to the global environment\n  assign(\"output_path\", paste0(output_path, \"/\"), envir = .GlobalEnv)\n  \n}\n\n\nAnd thats it! You can save this function, call it at the start of any script you write, and it will automatically create the required output folders for you to store all of the awesome work you create. To access the path to the created folders all you need to do is call the “output_path” object.\n\n\n4 When Is This Useful When Is it Not?\nBroadly speaking, you will find this function useful when the repository you are working in is high traffic, and/or has a lot of scripts and outputs. If you have several scripts that produce outputs with similar names this is also a life save.\nConversely, this function is certainly overkill for a simple repository that only contains one or two scripts.\n\n\n5 Extensions\nThere are a few natural extensions to this function that I have also implemented in my day to day. The most obvious of which is creating another mirrored folder structure for my input data to be stored. This is particularly helpful when the data is automatically downloaded and then stored for later retrieval.\nTo the contrary, if you manually access and store your data this extension might not work so well for you - since you would have to be manually creating the folders to store the data to begin with."
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Environmental Bytes for a Better Earth",
    "section": "",
    "text": "Document\n\n\n  \n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nAutomate Your Output Folders\n\n\nGot a bunch of scripts and are confused which output belongs to which script? Maybe you promised yourself your file organisation would be good this time? Never fear, this post is for you!\n\n\n\n\n\n19 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nOoh Shiny! A Web App Built in R\n\n\nDid you know you can build a web app in R? Did you know this web app could be a custom dashboard showing off your latest data analysis and results? In this blog I am learning how to create my very own Shiny web app, and I am taking you along for the ride.\n\n\n\n\n\n15 May, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn Opinionated Dataframe Cleaner\n\n\nNaming your dataframe columns doesn’t have to be hard, does it? Here I demonstrate the benefits of implimenting an opionated dataframe cleaner to help keep your columns organised.\n\n\n\n\n\n13 Apr, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMaking Beautiful Maps in R\n\n\nCreating maps using a programming language can be a painful process. In this blog I explore the R package ‘tmap’ and how it can be used to programmatically make beautiful, report ready maps, with as little stress as possible.\n\n\n\n\n\n23 Mar, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMigrating to Version 4 of the tmap R Package\n\n\nMy favourite mapping package just released a major update! In this post I discuss the changes made as they relate to my work, and provide some tips and tricks I have learnt so far when migration from Version 3.0 to Version 4.0.\n\n\n\n\n\n04 Feb, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAn Epic Battle Between Vectorised Functions and For Loops\n\n\nThe choice between using a vectorised function or a for loop can sometimes be a hard one. Don’t know what they are? Or which one is right for which scenario? This blog is for you.\n\n\n\n\n\n29 Jan, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nLearning To Create Your Own Custom Functions\n\n\nHaving the ability to create functions is both a blessing and a curse. You are gifted with limitless potential, but absolutely limited power supply (AKA your brain). In this post I discuss how I learnt to make my own functions and the trials I faced along the way.\n\n\n\n\n\n10 Jan, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nGoing Loopy for For Loops\n\n\nWhats the point of for loops? Well for looping of course. In this post I break down how to write for loops in R, how you can easily understand them, and compelling reasons that you might want to learn them yourself!\n\n\n\n\n\n18 Dec, 2024\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html",
    "href": "posts/loops_and_vectorised_functions/index.html",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "",
    "text": "Functions and for loops… If you haven’t read my blog posts on these two topics I will shamelessly self promote and say that you should. But honestly, I would like to think that the content written here can be understood without any prior knowledge. So please, read on if you want!\nIf you are not aware, the tension between vectorise functions and for loops is weirdly high. Some people say that vectorised functions are easier to read and understand, others swear that for loops are a more obvious and legible option. Some people prioritise processing time above all else and will create vectorised functions for everything, others say that in todays’ coding languages it really doesn’t make much of a different.\nSo which is it? What do I use? Why do I use it? The idea of my blog today to try and pull apart the differences between vectorised functions and for loops as they relate to my work. I will then dive deeper into how you can use vectorised functions, and code up some examples."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#vectorised-functions",
    "href": "posts/loops_and_vectorised_functions/index.html#vectorised-functions",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.1 Vectorised Functions",
    "text": "2.1 Vectorised Functions\nThis is a bit of a weird analogy but you can think of a vectorised function almost like a shotgun. The idea is that all the actions are completed at the same time in one big boom (like how all the pellets in a shotgun fire at the same time). To do this, you take any “normal” function with the following structure your_function_name &lt;- function(inputs){code} and place this inside a “special” vectoring function such as map() or lapply(). The special vectoring function does all the repetition of the normal function.\n\n\n\n\n\n\nNote\n\n\n\nI will once again promote my functions blog, which will help you understand the “normal” function within the “special” vectoring function."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#for-loops",
    "href": "posts/loops_and_vectorised_functions/index.html#for-loops",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.2 For Loops",
    "text": "2.2 For Loops\nIn contrast, a for loop is more like a chain-gun, with each action completed one after the other in rapid succession. In this analogy you take any “normal” function with the following structure your_function_name &lt;- function(inputs){code} and place this inside the repeating for loop. The normal function is then repeated for each iteration of the loop.\n\n\n\n\n\n\nNote\n\n\n\nYou know where to go if you want to know more… for loops."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#similarities-and-differences",
    "href": "posts/loops_and_vectorised_functions/index.html#similarities-and-differences",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.3 Similarities and Differences",
    "text": "2.3 Similarities and Differences\nWith a better understanding of the idea behind each method, we can now summarise some of the main similarities and differences that exist in practice;\nSimilarities:\n\nUsed for repeated action, to do something several times over\nAre often lauded as “efficient”, “effective”, and “scalable”\nCan be daunting for newcomers\n\nDifferences:\n\nDifferent structure/syntax/method\nVectorised functions are (in my opinion) more vague\nLoops are great to work on products of previous loops (vectorised functions can’t do that)\nVectorised functions are an equal speed or faster (depending the scenario)"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#examples-of-each",
    "href": "posts/loops_and_vectorised_functions/index.html#examples-of-each",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "2.4 Examples of Each",
    "text": "2.4 Examples of Each\nSometimes its just easier with some practical examples: lets pretend that we have a vector of numbers from 1 to 10 that each need to have 1 added to them. (Yes I know we don’t actually need a vectorised function or for loop to do this, but lets pretend we do).\nSo how would this look in a for loop?\nFirst we have our numbers:\n\n\nCode\nour_numbers &lt;- c(1:10)\n\nour_numbers\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThen we have our loop:\n\n\nCode\n#create an empty vector to store the output\nour_num_plus_1 &lt;- vector(mode = \"numeric\", length = length(our_numbers))\n\n#run the loop\nfor (i in 1:length(our_numbers)){\n  \n  #replace the number in the empty vector with tbe new number\n  our_num_plus_1[i] &lt;- our_numbers[i] + 1\n  \n}\n\n\nAnd then our output:\n\n\nCode\nour_num_plus_1\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nTo contrast, here is the vectorised function. First we would create our numbers (we did that above), then we have the vectorised function:\n\n\nCode\n#use map_int to return a integer vector\nour_num_plus_2 &lt;- map_int(our_numbers, \\(x) x + 1)\n\n\nAnd our output:\n\n\nCode\nour_num_plus_2\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nIdentical outputs, as expected.\nComparing the two side by side in this way confirms our early observations, and tell us more. For example, I stated above that I find vectorised functions to be more vague, and in this example it seems to be the case. Looking at the code it is not at all clear what the function map_int() does, nor how our_numbers and + 1 interact with each other, and what the hell is that \\(x) doing there? In contrast, the for loop might take a few more lines of code, but it is probably a bit easier to understand that the code is meant to do something a few times in a row, even if you can’t quite pick what exact it is."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#purrr",
    "href": "posts/loops_and_vectorised_functions/index.html#purrr",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "3.1 Purrr",
    "text": "3.1 Purrr\n\n\n\n\n\n\nNote\n\n\n\nAs with most topics I have written about, I would highly recommend that you check out the webpage written for the Purrr package. It has excellent documentation and some basic examples that cover the major use cases you would expect to encounter.\n\n\nThe basic syntax of a purrr mapping function is as follows:\n\noption 1: map(vector, function)\noption 2: map(vector, \\(x) x + 1)\noption 3: map(vector, \\(x) function(x))\n\nAs well as some older methods that are no longer actively recommended, but that you might find in old code:\n\nout-dated option 1: map(vector, ~ .x + 1)\nout-dated option 2: map(vect, ~ function(.x))\n\nYou will note that I have written more than one example of the syntax, this is not a mistake. Annoyingly there are multiple “correct” ways to write the code depending on the situtation… Yea I know. I’ll do my best to explain each one.\n\n3.1.1 Syntax Option 1\nSyntax: map(vector, function)\nSummary: no explicit designation of x and y, but does have a named function.\n\n3.1.1.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. Finally we write the function that we want to repeat a bunch of times. IMPORTANT! In this syntax the function we are repeating does not have any brackets after it.\n\n\n3.1.1.2 Why Choose This Way?\nThis option is the most streamlined choice. It has very little moving parts and thus very little that you can write incorrectly. However, the downside of using this method is that you need to be familiar with the map() function otherwise it is really not clear what the code is doing. There are two addition downsides to this method that mean it is not always the right choice:\n\nThe function you want to repeat has to be named, i.e. it needs to be something like mean(), sum(), etc. Or a custom function that you have written and given a name.\nmap() assumes that the first argument in the function you want to repeat is where the vector should go, if the vector needs to go somewhere else you will have to use a different syntax.\n\nAs an example\n\n\nCode\n#This won't do anything to the numbers because it is taking the mean of 1 number each time\nmap_int(our_numbers, mean)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\n\nNote\n\n\n\nI’m using the map_int() function simply because I want the output to be an integer, the syntax for map_int() is the same as for map().\n\n\n\n\n\n3.1.2 Syntax Option 2\nSyntax: map(vector, \\(x) x + 1)\nSummary: for anonymous (not named) functions that are simple.\n\n3.1.2.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. In this option we then write \\(x), this is essentially saying that from this point on the vector is now “x”. We follow this up by writing the code we want to apply to the vector such as adding 1 to the each element in the vector, which looks like x + 1. IMPORTANT! In this syntax this is no comma between \\(x) and the code you want to apply.\n\n\n3.1.2.2 Why Choose This Way?\nThis option is great when the thing you want to do to the vector does not have a pre-existing function. An additional benefit of this option is that it is one of the clearest ways to demonstrate what is happening to the vector. However, this option is not always the best choice if the thing you want to do to the vector is super complicated and take several lines of code to write. If that is the case, I would recommend writing your own custom function that encapsulates the things you want to do, and then providing this custom function to map using option 1 or 3.\n\n\n\n\n\n\nNote\n\n\n\nThe “x” in \\(x) x + 1 is just a placeholder. If you really wanted to, you could write \\(cashew) cashew + 1 and it would work fine. However, you can’t change the backslash, brackets, or lack of comma.\n\n\nExample 1:\n\n\nCode\nmap_int(our_numbers, \\(x) x + 1)\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nExample 2:\n\n\nCode\nmap_int(our_numbers, \\(cashew) cashew + 1)\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\n\n\n\n3.1.3 Syntax Option 3\nSyntax: map(vector, \\(x) function(x))\nSummary: for custom functions and/or functions that need strong control over where x goes\n\n3.1.3.1 Breakdown\nThe first argument in the map function is always our vector. This is then always followed by a comma no matter what. Once again we then write \\(x), which is saying that from this point on the vector is now “x”. This is then followed by a named function, that includes brackets, and within the brackets you place your “x” which looks like: function(x). IMPORTANT! In this syntax this is no comma between \\(x) and the code you want to apply.\n\n\n3.1.3.2 Why Choose This Way?\nThis final option is the most verbose, but most robust method of writing a map function. This option is useful when you need to be very specific about where x goes in your repeated function. For example, if x is meant to be the second argument for some reason. There are no real downsides to this method, and it can technically be used in any scenario, but in a lot of cases you will find that it is overkill.\nHere are a few demonstrations of the benefits of this option, first we need to create a custom function where we deliberately want x to be the second argument:\n\n\nCode\n#create a custom function to demonstrate if for some reason x needs to go somewhere else\ncustom_func &lt;- function(a = 1, b) {b = b + a}\n\n\nThen, if we don’t explicitly put x as the second argument (i.e. if we were to try and use syntax option 1) we would get this error:\n\n\nCode\n#will not work without direct placement\nmap_int(our_numbers, custom_func)\n\n\nError in `map_int()`:\nℹ In index: 1.\nCaused by error in `.f()`:\n! argument \"b\" is missing, with no default\n\n\nSo instead, we use syntax option 3 and directly tell “x” where it needs to go and that it is the second argument:\n\n\nCode\n#working version\nmap_int(our_numbers, \\(x) custom_func(b = x))\n\n\n [1]  2  3  4  5  6  7  8  9 10 11\n\n\nAnd as a bonus, our custom function also allows us to change how much is added to each element by changing the first argument:\n\n\nCode\n#working version\nmap_int(our_numbers, \\(x) custom_func(5, x))\n\n\n [1]  6  7  8  9 10 11 12 13 14 15"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#purrr-x2",
    "href": "posts/loops_and_vectorised_functions/index.html#purrr-x2",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "3.2 Purrr x2!",
    "text": "3.2 Purrr x2!\nHopefully those above examples made sense, if not we will be working on another more realistic example further below. But before we do I want to take a slight detour to touch on an important additional feature of Purrr called map2()!\nLets set the scene, imagine you have a vector of number just like we did above. And for this vector of numbers you want to add something to each one, just like we did above. Except this time, the thing you want to add to each number in the vector is different every time! How are we going to do that? I’ve spoiled it already, but of course we would use map2() (the “2” means it takes two vectors or lists). Thankfully, the syntax is not very different from the normal map(), here is a quick example:\n\n\nCode\n#create our second vector of numbers in the reverse order of the orginal numbers\nour_numbers2 &lt;- c(10:1)\n\n#run the map2 function\nmap2_int(our_numbers, our_numbers2, \\(x,y) x + y)\n\n\n [1] 11 11 11 11 11 11 11 11 11 11\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhat if you have more than two vectors/lists? You can use the pmap() functions."
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#get-the-data",
    "href": "posts/loops_and_vectorised_functions/index.html#get-the-data",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.1 Get The Data",
    "text": "4.1 Get The Data\nI have pre-prepared some data for us to use, lets make a object that lists the names of each of the datasets we want to open.\n\n\nCode\n#list the files found in the directory with the extension \".RData\"\nrasters_to_open &lt;- list.files()[str_detect(list.files(), \".RData\")]\n\n#list the files found in the directory with the extension \".gpkg\"\ncrops_to_open &lt;- list.files()[str_detect(list.files(), \".gpkg\")]"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#create-a-list-of-data",
    "href": "posts/loops_and_vectorised_functions/index.html#create-a-list-of-data",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.2 Create A List of Data",
    "text": "4.2 Create A List of Data\nAs you know (because you are a talented spatial data analyst), you can you put almost anything you want into a list in R. Using this knowledge we are going to load each of the raster datasets into a single list.\n\n\nCode\n#ironically we need to use a loop to load Rdata objects (quirk of the fucnction)\nfor (i in rasters_to_open){load(i)}\n\n#before putting the datasets into a list\nlist_of_rasters &lt;- map(str_remove_all(rasters_to_open, \".RData\"), get)\n\n#for gpkg objects we can directly open and put into a list\nlist_of_crops &lt;- map(crops_to_open, st_read)\n\n\nReading layer `dt' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_1.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 146.2958 ymin: -19.30559 xmax: 148.2985 ymax: -17.62597\nGeodetic CRS:  GDA2020\nReading layer `wt' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_2.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 145.346 ymin: -18.898 xmax: 147.2067 ymax: -15.19529\nGeodetic CRS:  GDA2020\nReading layer `mwi' from data source \n  `C:\\Users\\adams\\OneDrive - drytropicshealthywaters.org\\Documents\\GitHub\\website\\posts\\loops_and_vectorised_functions\\example_crop_3.gpkg' \n  using driver `GPKG'\nSimple feature collection with 1 feature and 0 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 147.7533 ymin: -22.17852 xmax: 151.3289 ymax: -18.58484\nGeodetic CRS:  GDA2020\n\n\nHa! Look at that, we already had to use a vectorised function ;)"
  },
  {
    "objectID": "posts/loops_and_vectorised_functions/index.html#vectorise-our-function",
    "href": "posts/loops_and_vectorised_functions/index.html#vectorise-our-function",
    "title": "An Epic Battle Between Vectorised Functions and For Loops",
    "section": "4.3 Vectorise Our Function",
    "text": "4.3 Vectorise Our Function\nWith the data ready in a list, all we have left to do is decide what function we want to apply. In this example there are a few things we want to do:\n\nReplace any value in the raster that is greater than 200 with “NA”. The reason we want to do this is the data provider designated NA values as very large numbers (because they couldn’t store them directly as NA),\nCrop each dataset to a specific shape in the area, and\nConvert the raster from an nc object to a sf object (don’t worry if you don’t know what that means, its not important for this blog).\n…\n\n\n4.3.1 Custom Function\nLets quickly write that custom function:\n\n\nCode\nreplace_with_na &lt;- function(raster_dataset, crop_dataset){\n  \n  #replace values greater than 200 with NA\n  raster_dataset[raster_dataset &gt; 200] &lt;- NA\n  \n  #change the crs of the dataset and crop the dataset to the area of interest\n  raster_dataset &lt;- raster_dataset |&gt; \n    st_transform(\"EPSG:7844\") |&gt; \n    st_crop(crop_dataset)\n  \n  #extract just one day from the multiyear dataset (for demonstration purposes)\n  raster_dataset &lt;- raster_dataset[,,,,1]\n  \n  #return the object\n  return(raster_dataset)\n}\n\n\n\n\n4.3.2 Vectorising\nAnd now we can put that custom function to work!\n\n\nCode\n#vectorise our custom function\noutput &lt;- map2(list_of_rasters, list_of_crops, \\(x, y) replace_with_na(x, y))\n\n\nDone!\nLets take a look at what we just did by putting each of the datasets into one map, I’ve coloured each location differently:\n\n\nCode\ntm_shape(output[[1]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.greens\")) +\n  tm_shape(output[[2]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.blues\")) +\n  tm_shape(output[[3]]) +\n  tm_raster(col.scale = tm_scale_continuous(values = \"brewer.oranges\")) +\n  tm_layout(legend.show = F)\n\n\n\n\n\n\n\n\n\nPretty cool.\nAs you can see, by using vectorised functions we made the entire workflow of getting the data, manipulating the data, and mapping the data take less than 20 lines of code. This is the power of vectorised functions - clean, efficient, effective code.\nThats all for now!"
  },
  {
    "objectID": "posts/shiny_a_web_app_built_in_r/index.html",
    "href": "posts/shiny_a_web_app_built_in_r/index.html",
    "title": "Ooh Shiny! A Web App Built in R",
    "section": "",
    "text": "Notes\n\ndraw parallels to Tableau and eventually link to a Tableau blog post\nin my opinion shiny is best for light weight small scale projects that allow for quick pivots and fast turn around\nit is not so handy in super large or complicated projects (maybe), depending on where the complexity comes from (UI or analysis). In the case of UI it is probably better to use things like tableau\nI will initially be following this: https://shiny.posit.co/r/getstarted/shiny-basics/lesson1/ tutorial. However I plan to also develop my own web app\nnote the error i found with standalone = T which stopped shiny from working\n\nIdeas for Shiny Web Apps\nI guess the core purpose of the app is that the user interacts with it. Thus, there needs to be something that changes. I want to the user to instinctively ask questions about the data and then be able to do something about it to find the answer. For example:\n\nWhere is something - map?\nHow does this data look in a different plot type (alternative visuals)\n… ?\n\nAlternatively, shiny apps can take external inputs. I could make an app where I want users to input data. For my purposes I want this data to be as simple and generic as possible. Examples include:\n\nthe PRM dashboard\nthe land comparison thing? - Can i make a simplified version of this? (i.e. take a more basic dataset)\n\n\n\n\n#| '!! shinylive warning !!': |\n#|   shinylive does not work in self-contained HTML documents.\n#|   Please set `embed-resources: false` in your metadata.\n#| standalone: true\n\nlibrary(shiny)\nlibrary(bslib)\n\n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  # App title ----\n  title = \"Hello Shiny!\",\n  # Sidebar panel for inputs ----\n  sidebar = sidebar(\n    # Input: Slider for the number of bins ----\n    sliderInput(\n      inputId = \"bins\",\n      label = \"Number of bins:\",\n      min = 1,\n      max = 50,\n      value = 30\n    )\n  ),\n  # Output: Histogram ----\n  plotOutput(outputId = \"distPlot\", height = \"600px\")\n)\n\n# Define server logic required to draw a histogram ----\nserver &lt;- function(input, output) {\n\n  # Histogram of the Old Faithful Geyser Data ----\n  # with requested number of bins\n  # This expression that generates a histogram is wrapped in a call\n  # to renderPlot to indicate that:\n  #\n  # 1. It is \"reactive\" and therefore should be automatically\n  #    re-executed when inputs (input$bins) change\n  # 2. Its output type is a plot\n  output$distPlot &lt;- renderPlot({\n\n    x    &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n    hist(x, breaks = bins, col = \"#007bc2\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\")\n\n    })\n\n}\n\nshinyApp(ui = ui, server = server)"
  }
]